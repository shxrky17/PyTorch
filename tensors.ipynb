{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "370VjmWZ3j-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "puRr68tq3mSS",
        "outputId": "58908fe6-01b4-44b1-90df-7788b4e0e63d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(f\"{torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "  print(\"NO\")"
      ],
      "metadata": {
        "id": "1WlvPm1z3tVZ",
        "outputId": "bcc9d6ce-5b8b-4bd2-8244-d02d631399c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.empty(2,3)"
      ],
      "metadata": {
        "id": "Z5XImNw13_6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)\n",
        "print(a.type())"
      ],
      "metadata": {
        "id": "JRhjWhxB4Tw-",
        "outputId": "03b912e2-09d3-45f3-c26c-15eb46fda753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.2081e-32, 0.0000e+00, 1.1210e-44],\n",
            "        [0.0000e+00, 8.9683e-44, 0.0000e+00]])\n",
            "torch.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(a)"
      ],
      "metadata": {
        "id": "wqUbrLI54W6_",
        "outputId": "14390e10-c659-42e5-c9f6-48ef116c16c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=torch.zeros(47,1)"
      ],
      "metadata": {
        "id": "kvCJ2Bvu4eXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(b)"
      ],
      "metadata": {
        "id": "SfA0KEgY4hp7",
        "outputId": "eac1c033-e969-49de-b855-e457a0107276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c=torch.rand(2,3)"
      ],
      "metadata": {
        "id": "6Wp7_CpL4oW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c)"
      ],
      "metadata": {
        "id": "y5hKZL2H4rha",
        "outputId": "b32731e0-419c-48c6-e61b-2e93876d7330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8823, 0.9150, 0.3829],\n",
            "        [0.9593, 0.3904, 0.6009]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "torch.rand(2,3)"
      ],
      "metadata": {
        "id": "wPv4C5yM4xpY",
        "outputId": "e6b43648-a5eb-43cb-ff08-e6671d900a12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1117, 0.8158, 0.2626],\n",
              "        [0.4839, 0.6765, 0.7539]])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([1,2,3])"
      ],
      "metadata": {
        "id": "e39aCjm049av",
        "outputId": "f6525824-18a2-4702-bb62-81112d5107ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"using arange\",torch.arange(0,10,2))\n",
        "print(torch.linspace(0,10,10))\n",
        "print(torch.eye(5))\n",
        "print(torch.full((3,3),5))"
      ],
      "metadata": {
        "id": "uByf-Hke5B4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f62f4c1-cf31-4339-b760-cb5033e1bd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using arange tensor([0, 2, 4, 6, 8])\n",
            "tensor([ 0.0000,  1.1111,  2.2222,  3.3333,  4.4444,  5.5556,  6.6667,  7.7778,\n",
            "         8.8889, 10.0000])\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.]])\n",
            "tensor([[5, 5, 5],\n",
            "        [5, 5, 5],\n",
            "        [5, 5, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zreSyvFL5czD",
        "outputId": "69a0add6-623f-468e-fa20-0b58bc35a551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros_like(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSSauxJt6NNq",
        "outputId": "a44da60d-5fad-4da5-8636-5a25a711a735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand_like(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrDBX-DS6jbQ",
        "outputId": "fa87de8b-aa26-4d35-a146-2cc9c3cdfb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2627, 0.0428, 0.2080],\n",
              "        [0.1180, 0.1217, 0.7356]])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2dFG7zl6wck",
        "outputId": "2308bbdd-3053-4328-91ea-04c99cb7bd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8823, 0.9150, 0.3829],\n",
              "        [0.9593, 0.3904, 0.6009]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2KtzLvZ67_s",
        "outputId": "36779fa2-c655-40d6-b0c1-fcfc1ef51330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d=torch.tensor([1.0,2.0],dtype=torch.uint8)"
      ],
      "metadata": {
        "id": "ExpoSozd69-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.ones(1,2)\n",
        "b=torch.zeros(1,2)"
      ],
      "metadata": {
        "id": "BG8wjohZ7MVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a+b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBhjlagq7n2D",
        "outputId": "f6f266f9-59aa-43af-c177-b7988b5bfd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a*b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP_3cuuA72_9",
        "outputId": "af27ee7a-ce1e-43bb-f528-92c596b954f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a-b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BXwxeXU74lO",
        "outputId": "ad89906a-a2d4-47c3-cae3-9f8e5435aafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a*a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvkPZzIU7-Vg",
        "outputId": "e58e96f2-f3f7-4e5b-d2b7-1a745b813bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiHfsfNN7_pN",
        "outputId": "fd1e6cd2-e427-47e5-ca6c-b689fbad29ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.neg(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10PvtOT98Y3p",
        "outputId": "b491d0da-388c-41c6-83a7-8687561014d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1., -1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.clamp(a,min=-1,max=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ_UnGhJ8am8",
        "outputId": "219f7853-ef93-452a-881b-80022d762263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e=torch.randint(size=(2,3),low=0,high=10,dtype=torch.int64)\n",
        "print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBe-nBZs8r99",
        "outputId": "556267f4-cd1a-438a-da9e-27124c096a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8, 8, 4],\n",
            "        [4, 1, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.prod(e,dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4OF2vdu9TG3",
        "outputId": "27c687c4-a72a-4656-8f3e-1546ad88edf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([256,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmin(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljOzs-6J9XmJ",
        "outputId": "29839a54-9e64-4f0d-c244-fa37b823dbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(e,0,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHOS1NYN9mF1",
        "outputId": "5944a1fe-0435-47c8-a4a7-e9144e9fac94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[8, 4],\n",
              "        [8, 1],\n",
              "        [4, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a>d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ECqQtkc-CyQ",
        "outputId": "34e1a9a8-373f-4f42-83c9-069c83cdcae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d>a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXcRpixJ-L35",
        "outputId": "440ab6f8-b737-436a-b69a-b41c180c6db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sqrt(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-3RnypM-NQy",
        "outputId": "5754f15a-084e-47ac-b444-58cd83d7f28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.8284, 2.8284, 2.0000],\n",
              "        [2.0000, 1.0000, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(e,dim=1,dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrkd6jjD-eK2",
        "outputId": "40ab7275-90b4-4182-e116-14efb75a6c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4955, 0.4955, 0.0091],\n",
              "        [0.9362, 0.0466, 0.0171]])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjSrCJAt-hcV",
        "outputId": "9c0d15bf-69a0-4313-956c-d61f81942534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda')"
      ],
      "metadata": {
        "id": "409XeKtMAPMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fxbWg3H_ARxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZZmhuoRAYxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.flatten(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3n2QTffAhKW",
        "outputId": "a81c45fc-5882-4457-d699-ea538b9b6f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b=torch.rand(2,3,4)"
      ],
      "metadata": {
        "id": "YLUUm8bzBFlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.flatten(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bgy0G5hBJi3",
        "outputId": "73463ecd-c07f-4649-87f1-69772ad74f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2239, 0.3023, 0.1784, 0.8238, 0.5557, 0.9770, 0.4440, 0.9478, 0.7445,\n",
              "        0.4892, 0.2426, 0.7003, 0.5277, 0.2472, 0.7909, 0.4235, 0.0169, 0.2209,\n",
              "        0.9535, 0.7064, 0.1629, 0.8902, 0.5163, 0.0359])"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dr_dx(x):\n",
        "  return 2*x"
      ],
      "metadata": {
        "id": "kvANgjwNBVXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dr_dx(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyTYqhvA2NyE",
        "outputId": "332404c9-6b75-45dd-da75-0fd90c2f0705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def dy_dx(x):\n",
        "  return torch.sin(x)"
      ],
      "metadata": {
        "id": "NgJfc5XA2P4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8LKCym02bej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor(3.0,requires_grad=True)"
      ],
      "metadata": {
        "id": "aJkN83lR2dft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDozQpIT4aki",
        "outputId": "2166f260-852d-4f37-8a78-3e929304a274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=x**2"
      ],
      "metadata": {
        "id": "i2PA0TMY4d3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1etLlOw4f2p",
        "outputId": "5c005c49-c6a0-496a-e919-2c0ea1406669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "zgy0AFYq4hBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5uUi7PA483w",
        "outputId": "f54ece7e-dfe8-4bb4-bc04-235296e76d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def dz_dx(x):\n",
        "\n",
        "  return 2*x*math.cos(x**2)"
      ],
      "metadata": {
        "id": "mMqahCmG5BE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dz_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoforNIt5ZiP",
        "outputId": "8397a0a3-7ac8-4cf2-9178-102a8f434664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.466781571308061"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor(3.0,requires_grad=True)"
      ],
      "metadata": {
        "id": "CUdq7JDA5Zf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=x**2"
      ],
      "metadata": {
        "id": "vurVbpF65Zdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "mldBnm-_5ZZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z=torch.sin(y)"
      ],
      "metadata": {
        "id": "M7lYmG2P5ZWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcr-_iGu5ZUA",
        "outputId": "ee250c93-848e-417e-cafc-dfaf81391f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vSLTF_V5ZRt",
        "outputId": "1622d55f-dfaa-44ba-8a40-6f8d42f8cf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j5pGXxs5ZPN",
        "outputId": "0a1beed9-fd2e-4c02-9bf1-e09e5626f269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4121, grad_fn=<SinBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z =torch.sin(x)  # Recompute forward pass\n",
        "z.backward()\n"
      ],
      "metadata": {
        "id": "4_AQEHDW5Y7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdxu6age6TG2",
        "outputId": "04020951-1066-429f-9dfa-03f3c2862601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.0100)"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x=torch.tensor(6.7)\n",
        "y=torch.tensor(0.0)\n",
        "\n",
        "w=torch.tensor(1.0)\n",
        "b=torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "lQlNw2a46yxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy_loss(prediction,target):\n",
        "  epsilon=1e-8\n",
        "  prediction=torch.clamp(prediction,epsilon,1-epsilon)\n",
        "  return -(target*torch.log(prediction)+(1-target)*torch.log(1-prediction))"
      ],
      "metadata": {
        "id": "G42xr5wyAdb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z=w*x+b\n",
        "y_pred=torch.sigmoid(x)\n",
        "\n",
        "loss=binary_cross_entropy_loss(y_pred,y)"
      ],
      "metadata": {
        "id": "mgmF8DLIA5pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dloss_dy_pred=(y_pred-y)/(y_pred*(1-y_pred))\n",
        "dy_pred_dz=y_pred*(1-y_pred)\n",
        "\n",
        "\n",
        "dz_dw=x\n",
        "dz_db=1\n",
        "\n",
        "dl_dw=dloss_dy_pred*dy_pred_dz*dz_dw\n",
        "dl_db=dloss_dy_pred*dy_pred_dz*dz_db"
      ],
      "metadata": {
        "id": "NNerz4fKBEnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{dl_dw}\")\n",
        "print(f\"{dl_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRwSI1QcBctf",
        "outputId": "ecfbc44e-940a-49f9-8b99-6c85fd703761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.691762447357178\n",
            "0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor(6.7)\n",
        "y=torch.tensor(0.0)\n"
      ],
      "metadata": {
        "id": "gTIJFVZhBx1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=torch.tensor(1.0,requires_grad=True)\n",
        "b=torch.tensor(0.0,requires_grad=True)"
      ],
      "metadata": {
        "id": "CL_esN7nCBe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1g1CbwkCJE-",
        "outputId": "5227f9b4-067b-4155-aefb-5fe0fdf395d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4saxv2gKCNJ3",
        "outputId": "d6aa5667-f2d3-41d3-f227-dcd272d2f746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z=w*x+b"
      ],
      "metadata": {
        "id": "_Aadpov1CNqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYKNawT_CP8X",
        "outputId": "8331d8da-a4fa-404c-d470-39cccd3aec3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=torch.sigmoid(z)"
      ],
      "metadata": {
        "id": "OVhMDtwGCQxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHB0m3oxCTpn",
        "outputId": "5c81aa28-f59f-4b0d-98ca-077204c1ae5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=binary_cross_entropy_loss(y_pred,y)"
      ],
      "metadata": {
        "id": "m4Gusr_MCVWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Et1yoCGCZPW",
        "outputId": "fd7b6bc0-34dd-472a-ffea-d6e57e234c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "MJ0ug_7lCaE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT1ay8-fDXhA",
        "outputId": "f54e4548-21b8-4a70-8676-1437b5b12d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCJKzZBnDZ0R",
        "outputId": "e6b864db-7b40-409b-e980-f1290aa6eda4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([1.0,2.0,3.0],requires_grad=True)"
      ],
      "metadata": {
        "id": "7gHe3aRzDbt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=(x**2).mean()"
      ],
      "metadata": {
        "id": "WxkpTB3_Dz9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXcEEF36D3aF",
        "outputId": "43584dd7-ad10-44d4-c60e-08ff17246225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "IWpUipZ9D4QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWQ-khYOD6C2",
        "outputId": "3efc1216-e329-4b1f-fcca-997cd280378b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "b6HTWJqdD7st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "M6ZpbsnxtiU2",
        "outputId": "a2ce1f0c-b90b-436f-a74b-1a28a068fc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302         M        17.99         10.38          122.80     1001.0   \n",
              "1    842517         M        20.57         17.77          132.90     1326.0   \n",
              "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3  84348301         M        11.42         20.38           77.58      386.1   \n",
              "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0  ...          17.33           184.60      2019.0            0.1622   \n",
              "1  ...          23.41           158.80      1956.0            0.1238   \n",
              "2  ...          25.53           152.50      1709.0            0.1444   \n",
              "3  ...          26.50            98.87       567.7            0.2098   \n",
              "4  ...          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0             0.6656           0.7119                0.2654          0.4601   \n",
              "1             0.1866           0.2416                0.1860          0.2750   \n",
              "2             0.4245           0.4504                0.2430          0.3613   \n",
              "3             0.8663           0.6869                0.2575          0.6638   \n",
              "4             0.2050           0.4000                0.1625          0.2364   \n",
              "\n",
              "   fractal_dimension_worst  Unnamed: 32  \n",
              "0                  0.11890          NaN  \n",
              "1                  0.08902          NaN  \n",
              "2                  0.08758          NaN  \n",
              "3                  0.17300          NaN  \n",
              "4                  0.07678          NaN  \n",
              "\n",
              "[5 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87a20b24-01df-482a-8f5c-4c3849d28d41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87a20b24-01df-482a-8f5c-4c3849d28d41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87a20b24-01df-482a-8f5c-4c3849d28d41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87a20b24-01df-482a-8f5c-4c3849d28d41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed2d5b6b-ff15-4fb8-b5ac-9177837c4e5c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed2d5b6b-ff15-4fb8-b5ac-9177837c4e5c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed2d5b6b-ff15-4fb8-b5ac-9177837c4e5c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_adkLLpt1Er",
        "outputId": "528d37a7-684c-42ea-9087-152a987fe2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=[\"id\",\"Unnamed: 32\"],inplace=True)"
      ],
      "metadata": {
        "id": "egRlLGk6ttUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXwljp5uBHI",
        "outputId": "9faa26a6-500b-4978-eea9-4a88f35083d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "s7uvwZKZuMY4",
        "outputId": "9b366f0e-27ca-4abf-a2e6-38cdf03346f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0         M        17.99         10.38          122.80     1001.0   \n",
              "1         M        20.57         17.77          132.90     1326.0   \n",
              "2         M        19.69         21.25          130.00     1203.0   \n",
              "3         M        11.42         20.38           77.58      386.1   \n",
              "4         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0         0.2419  ...         25.38          17.33           184.60   \n",
              "1         0.1812  ...         24.99          23.41           158.80   \n",
              "2         0.2069  ...         23.57          25.53           152.50   \n",
              "3         0.2597  ...         14.91          26.50            98.87   \n",
              "4         0.1809  ...         22.54          16.67           152.20   \n",
              "\n",
              "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0      2019.0            0.1622             0.6656           0.7119   \n",
              "1      1956.0            0.1238             0.1866           0.2416   \n",
              "2      1709.0            0.1444             0.4245           0.4504   \n",
              "3       567.7            0.2098             0.8663           0.6869   \n",
              "4      1575.0            0.1374             0.2050           0.4000   \n",
              "\n",
              "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                0.2654          0.4601                  0.11890  \n",
              "1                0.1860          0.2750                  0.08902  \n",
              "2                0.2430          0.3613                  0.08758  \n",
              "3                0.2575          0.6638                  0.17300  \n",
              "4                0.1625          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d93af62-cc23-4d02-af2b-04700ea925b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d93af62-cc23-4d02-af2b-04700ea925b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d93af62-cc23-4d02-af2b-04700ea925b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d93af62-cc23-4d02-af2b-04700ea925b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-47c74f1c-ed21-4028-933a-13774a39baab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47c74f1c-ed21-4028-933a-13774a39baab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-47c74f1c-ed21-4028-933a-13774a39baab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_test,y_train=train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.2)"
      ],
      "metadata": {
        "id": "ovhbRHV2uPiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Z_GS3rS4ucZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "ewmKq_Xyu1PI",
        "outputId": "c71d75d9-2040-42ff-d8aa-9b37b27005e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48     B\n",
              "545    B\n",
              "285    B\n",
              "82     M\n",
              "333    B\n",
              "      ..\n",
              "51     B\n",
              "113    B\n",
              "121    M\n",
              "464    B\n",
              "33     M\n",
              "Name: diagnosis, Length: 455, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>455 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=LabelEncoder()\n",
        "y_train=encoder.fit_transform(y_train)\n",
        "y_test=encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "s5GAErGAvOVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm7xczMUvYj1",
        "outputId": "4500af3a-0edc-4c7e-a3d4-e441e268e78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQCx7bknvncS",
        "outputId": "d4f7283b-80af-4d04-c826-1ee8282a7b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensors=torch.from_numpy(X_train)\n",
        "X_test_tensors=torch.from_numpy(X_test)\n",
        "y_train_tensors=torch.from_numpy(y_train)\n",
        "y_test_tensors=torch.from_numpy(y_test)"
      ],
      "metadata": {
        "id": "rJJkcfubvqGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tftIlhDlv-Bf",
        "outputId": "9dcc9236-e92b-401b-e5d4-6452a1f27463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0921, -1.3481,  0.0528,  ..., -0.0736,  0.2944, -1.2839],\n",
              "        [ 0.5356,  1.1026,  0.5289,  ..., -0.0248, -0.1261, -1.1648],\n",
              "        [-0.0518,  0.0854, -0.1102,  ..., -0.8333, -0.6181, -0.8189],\n",
              "        ...,\n",
              "        [-0.4952,  0.6981, -0.4860,  ..., -0.4818, -1.0253,  0.3845],\n",
              "        [ 0.1656,  0.8057,  0.2179,  ..., -0.0461, -1.0420, -0.2130],\n",
              "        [ 1.2991,  0.6256,  1.2311,  ...,  0.3719,  0.3725, -0.4314]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class mysimpleNN():\n",
        "  def __init__(self,X):\n",
        "\n",
        "    self.weights=torch.rand(X.shape[1],1,dtype=torch.float64,requires_grad=True) # Corrected the spelling of requires_grad and its position.\n",
        "    self.bias=torch.zeros(1,dtype=torch.float64,requires_grad=True) # Corrected the spelling of requires_grad and its position.\n",
        "\n",
        "  def forward(self,X):\n",
        "    z=torch.matmul(X,self.weights)+self.bias\n",
        "    y_pred=torch.sigmoid(z)\n",
        "    return y_pred\n",
        "\n",
        "  def loss_function(self,y_pred,y):\n",
        "    epsilon=1e-8\n",
        "    prediction=torch.clamp(y_pred,epsilon,1-epsilon)\n",
        "    return -(y_train_tensors*torch.log(prediction)+(1-y_train_tensors)*torch.log(1-prediction)).mean()\n"
      ],
      "metadata": {
        "id": "fp5A80ljwHfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.1\n",
        "epochs=25"
      ],
      "metadata": {
        "id": "XOsljKrqw2xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yumnw9_Fw6P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "mFoXSk4JxCwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "-UEwAKaYzGLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):  # Fixed class name (Moddel -> Model)\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(num_features, 3),  # Input to Hidden Layer (5 -> 3)\n",
        "            nn.ReLU(),  # Activation Function\n",
        "            nn.Linear(3, 1),  # Hidden to Output Layer (3 -> 1)\n",
        "            nn.Sigmoid()  # Activation Function for Output\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "        return self.network(features)  # Forward pass using nn.Sequential\n",
        "\n",
        "# Example usage:\n",
        "model = Model(num_features=5)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "wjT1ckLa1VfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bccef9d9-0b15-4ba5-c9bd-d48b5eae9201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (network): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=3, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features=torch.rand(10,5)\n",
        "model=Model(features.shape[1])\n",
        "model(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m06cTC6VYEVo",
        "outputId": "2d3f2112-488a-4aa5-cf2c-ce3c6fa5e491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5302],\n",
              "        [0.5645],\n",
              "        [0.5804],\n",
              "        [0.5745],\n",
              "        [0.5374],\n",
              "        [0.5487],\n",
              "        [0.5539],\n",
              "        [0.5536],\n",
              "        [0.5538],\n",
              "        [0.5649]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTD4b9ZiYL5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfHxMI-ca3BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function=nn.BCELoss()"
      ],
      "metadata": {
        "id": "QohL38RHhfeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mysimpleNN(nn.Module): # Inherit from nn.Module\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(num_features, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Explicitly set the weight and bias data types to float64\n",
        "        self.linear.weight = nn.Parameter(self.linear.weight.type(torch.float64))\n",
        "        self.linear.bias = nn.Parameter(self.linear.bias.type(torch.float64))\n",
        "\n",
        "    def forward(self, features):\n",
        "        out = self.linear(features)\n",
        "        out = self.sigmoid(out)\n",
        "        return out # Add return statement here\n",
        "\n",
        "    def loss_function(self, y_pred, y):\n",
        "        epsilon = 1e-8\n",
        "        prediction = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
        "        return -(y * torch.log(prediction) + (1 - y) * torch.log(1 - prediction)).mean()\n",
        "\n",
        "# Ensure X_train_tensors is also float64\n",
        "X_train_tensors = X_train_tensors.type(torch.float64)\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "model=mysimpleNN(X_train_tensors.shape[1])\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_pred = model(X_train_tensors)\n",
        "    print(y_pred)\n",
        "\n",
        "    loss = model.loss_function(y_pred, y_train_tensors)\n",
        "    print(f\"epoch:{epoch+1} loss:{loss.item()}\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights using the optimizer\n",
        "    optimizer.step()\n",
        "      # Reset gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cebKs-gzihwj",
        "outputId": "8940901c-d3fc-44ec-b9b9-3e20010ec1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        [0.2768],\n",
            "        [0.3334],\n",
            "        [0.3273],\n",
            "        [0.3514],\n",
            "        [0.4751],\n",
            "        [0.3999],\n",
            "        [0.3750],\n",
            "        [0.4249],\n",
            "        [0.3473],\n",
            "        [0.3543],\n",
            "        [0.3485],\n",
            "        [0.4854],\n",
            "        [0.2690],\n",
            "        [0.4444],\n",
            "        [0.3477],\n",
            "        [0.2927],\n",
            "        [0.3893],\n",
            "        [0.3425],\n",
            "        [0.4120],\n",
            "        [0.3563],\n",
            "        [0.3948],\n",
            "        [0.3195],\n",
            "        [0.3234],\n",
            "        [0.4252],\n",
            "        [0.3692],\n",
            "        [0.4062],\n",
            "        [0.3052],\n",
            "        [0.2745],\n",
            "        [0.3785],\n",
            "        [0.4101],\n",
            "        [0.4110],\n",
            "        [0.3299],\n",
            "        [0.3266],\n",
            "        [0.3051],\n",
            "        [0.4367],\n",
            "        [0.4339],\n",
            "        [0.3684],\n",
            "        [0.3883],\n",
            "        [0.3448],\n",
            "        [0.2654],\n",
            "        [0.3885],\n",
            "        [0.3950],\n",
            "        [0.2702],\n",
            "        [0.4131],\n",
            "        [0.3634],\n",
            "        [0.2583],\n",
            "        [0.3004],\n",
            "        [0.4792],\n",
            "        [0.4626],\n",
            "        [0.2357],\n",
            "        [0.3707],\n",
            "        [0.4209],\n",
            "        [0.3022],\n",
            "        [0.6583],\n",
            "        [0.3600],\n",
            "        [0.4551],\n",
            "        [0.4073],\n",
            "        [0.4771],\n",
            "        [0.4496],\n",
            "        [0.3209],\n",
            "        [0.3430],\n",
            "        [0.3102],\n",
            "        [0.3809],\n",
            "        [0.4013],\n",
            "        [0.2258],\n",
            "        [0.3266],\n",
            "        [0.3506],\n",
            "        [0.4292],\n",
            "        [0.3561],\n",
            "        [0.4020],\n",
            "        [0.3561],\n",
            "        [0.2567],\n",
            "        [0.3648],\n",
            "        [0.3160],\n",
            "        [0.3023],\n",
            "        [0.4130],\n",
            "        [0.2931],\n",
            "        [0.3843],\n",
            "        [0.3600],\n",
            "        [0.3191],\n",
            "        [0.3497],\n",
            "        [0.3365],\n",
            "        [0.3425],\n",
            "        [0.5108],\n",
            "        [0.2884],\n",
            "        [0.4087],\n",
            "        [0.3168],\n",
            "        [0.1899],\n",
            "        [0.4364],\n",
            "        [0.2734],\n",
            "        [0.3037],\n",
            "        [0.3375],\n",
            "        [0.2944],\n",
            "        [0.4368],\n",
            "        [0.3156],\n",
            "        [0.4103],\n",
            "        [0.3081],\n",
            "        [0.3434],\n",
            "        [0.3042],\n",
            "        [0.3451],\n",
            "        [0.3069],\n",
            "        [0.3658],\n",
            "        [0.3593],\n",
            "        [0.3462],\n",
            "        [0.4057],\n",
            "        [0.3813],\n",
            "        [0.2828],\n",
            "        [0.3862],\n",
            "        [0.3734],\n",
            "        [0.4080],\n",
            "        [0.3578],\n",
            "        [0.4606],\n",
            "        [0.3454],\n",
            "        [0.3900],\n",
            "        [0.3446],\n",
            "        [0.2012],\n",
            "        [0.5926],\n",
            "        [0.3912],\n",
            "        [0.3134],\n",
            "        [0.4204],\n",
            "        [0.3694],\n",
            "        [0.3709],\n",
            "        [0.3009],\n",
            "        [0.3689],\n",
            "        [0.3508],\n",
            "        [0.4712],\n",
            "        [0.3606],\n",
            "        [0.3567],\n",
            "        [0.2920],\n",
            "        [0.2584],\n",
            "        [0.3640],\n",
            "        [0.3491],\n",
            "        [0.3451],\n",
            "        [0.4131],\n",
            "        [0.3002],\n",
            "        [0.3578],\n",
            "        [0.4224],\n",
            "        [0.4020],\n",
            "        [0.4043],\n",
            "        [0.2765],\n",
            "        [0.2839],\n",
            "        [0.3855],\n",
            "        [0.2870],\n",
            "        [0.3118],\n",
            "        [0.4384],\n",
            "        [0.3884],\n",
            "        [0.4087],\n",
            "        [0.3699],\n",
            "        [0.2815],\n",
            "        [0.3250],\n",
            "        [0.3399],\n",
            "        [0.3454],\n",
            "        [0.3650],\n",
            "        [0.5674],\n",
            "        [0.5695],\n",
            "        [0.2869],\n",
            "        [0.3255],\n",
            "        [0.3290],\n",
            "        [0.3784],\n",
            "        [0.3643],\n",
            "        [0.3303],\n",
            "        [0.4844],\n",
            "        [0.4205],\n",
            "        [0.4876],\n",
            "        [0.3398],\n",
            "        [0.3311],\n",
            "        [0.3042],\n",
            "        [0.3416],\n",
            "        [0.3179],\n",
            "        [0.3781],\n",
            "        [0.3511],\n",
            "        [0.4053],\n",
            "        [0.4510],\n",
            "        [0.4081],\n",
            "        [0.4080],\n",
            "        [0.3792],\n",
            "        [0.3456],\n",
            "        [0.3715],\n",
            "        [0.4530],\n",
            "        [0.3154],\n",
            "        [0.3081],\n",
            "        [0.2788],\n",
            "        [0.4832],\n",
            "        [0.3301],\n",
            "        [0.4710],\n",
            "        [0.3696],\n",
            "        [0.2682],\n",
            "        [0.4177],\n",
            "        [0.3520],\n",
            "        [0.4379],\n",
            "        [0.4803],\n",
            "        [0.2478],\n",
            "        [0.3597],\n",
            "        [0.3842],\n",
            "        [0.3959],\n",
            "        [0.1753],\n",
            "        [0.4284],\n",
            "        [0.3461],\n",
            "        [0.3436],\n",
            "        [0.3786],\n",
            "        [0.2875],\n",
            "        [0.3896],\n",
            "        [0.3235],\n",
            "        [0.3966],\n",
            "        [0.4132],\n",
            "        [0.4186],\n",
            "        [0.3095],\n",
            "        [0.3788],\n",
            "        [0.3933],\n",
            "        [0.3695],\n",
            "        [0.4050],\n",
            "        [0.3055],\n",
            "        [0.2906],\n",
            "        [0.3882],\n",
            "        [0.2460],\n",
            "        [0.5808],\n",
            "        [0.4374],\n",
            "        [0.3578],\n",
            "        [0.4028],\n",
            "        [0.3530],\n",
            "        [0.3700],\n",
            "        [0.4742],\n",
            "        [0.3460],\n",
            "        [0.3529],\n",
            "        [0.3859],\n",
            "        [0.4172],\n",
            "        [0.3299],\n",
            "        [0.2938],\n",
            "        [0.3969],\n",
            "        [0.3390],\n",
            "        [0.3684],\n",
            "        [0.3378],\n",
            "        [0.4100],\n",
            "        [0.4238],\n",
            "        [0.3101],\n",
            "        [0.3780],\n",
            "        [0.3655],\n",
            "        [0.3313],\n",
            "        [0.3426],\n",
            "        [0.3417],\n",
            "        [0.3869],\n",
            "        [0.4001],\n",
            "        [0.3566],\n",
            "        [0.3492],\n",
            "        [0.3206],\n",
            "        [0.3641],\n",
            "        [0.3806],\n",
            "        [0.3147],\n",
            "        [0.3337],\n",
            "        [0.5494],\n",
            "        [0.4120],\n",
            "        [0.3533],\n",
            "        [0.4615],\n",
            "        [0.3305],\n",
            "        [0.3494],\n",
            "        [0.2901],\n",
            "        [0.2462],\n",
            "        [0.4621],\n",
            "        [0.3768],\n",
            "        [0.4199],\n",
            "        [0.3681],\n",
            "        [0.3301],\n",
            "        [0.4671],\n",
            "        [0.2834],\n",
            "        [0.3533],\n",
            "        [0.4021],\n",
            "        [0.3882],\n",
            "        [0.3979],\n",
            "        [0.3755],\n",
            "        [0.3596],\n",
            "        [0.3667],\n",
            "        [0.3757],\n",
            "        [0.3015],\n",
            "        [0.3172],\n",
            "        [0.3430],\n",
            "        [0.3424],\n",
            "        [0.5651],\n",
            "        [0.4597],\n",
            "        [0.4023],\n",
            "        [0.3222],\n",
            "        [0.3639],\n",
            "        [0.5196],\n",
            "        [0.3873],\n",
            "        [0.3686],\n",
            "        [0.3226],\n",
            "        [0.3414],\n",
            "        [0.3734],\n",
            "        [0.3510],\n",
            "        [0.4009],\n",
            "        [0.3189],\n",
            "        [0.2641],\n",
            "        [0.3168],\n",
            "        [0.4259],\n",
            "        [0.3576],\n",
            "        [0.3430],\n",
            "        [0.2984],\n",
            "        [0.3688],\n",
            "        [0.3187],\n",
            "        [0.4467],\n",
            "        [0.3575],\n",
            "        [0.3740],\n",
            "        [0.2999],\n",
            "        [0.2634],\n",
            "        [0.2932],\n",
            "        [0.4084],\n",
            "        [0.3715],\n",
            "        [0.2848],\n",
            "        [0.4139],\n",
            "        [0.2586],\n",
            "        [0.3014],\n",
            "        [0.3750],\n",
            "        [0.3727],\n",
            "        [0.3237],\n",
            "        [0.4142],\n",
            "        [0.3021],\n",
            "        [0.3493],\n",
            "        [0.4240],\n",
            "        [0.4135],\n",
            "        [0.3545],\n",
            "        [0.3439],\n",
            "        [0.3519],\n",
            "        [0.3970],\n",
            "        [0.4253],\n",
            "        [0.3348],\n",
            "        [0.3764],\n",
            "        [0.3526],\n",
            "        [0.3081],\n",
            "        [0.3069],\n",
            "        [0.2441],\n",
            "        [0.3912],\n",
            "        [0.2891],\n",
            "        [0.3759],\n",
            "        [0.4135],\n",
            "        [0.3192],\n",
            "        [0.3529],\n",
            "        [0.3987],\n",
            "        [0.4287],\n",
            "        [0.4060],\n",
            "        [0.4074],\n",
            "        [0.4167],\n",
            "        [0.3823],\n",
            "        [0.2509],\n",
            "        [0.3215],\n",
            "        [0.3772],\n",
            "        [0.3525],\n",
            "        [0.3625],\n",
            "        [0.2921],\n",
            "        [0.2323],\n",
            "        [0.3176],\n",
            "        [0.2538],\n",
            "        [0.3428],\n",
            "        [0.4181],\n",
            "        [0.4105],\n",
            "        [0.3195],\n",
            "        [0.4225],\n",
            "        [0.3597],\n",
            "        [0.4195],\n",
            "        [0.4188],\n",
            "        [0.3618],\n",
            "        [0.2529],\n",
            "        [0.3045],\n",
            "        [0.3354],\n",
            "        [0.4313],\n",
            "        [0.2842],\n",
            "        [0.4308],\n",
            "        [0.3751],\n",
            "        [0.2960],\n",
            "        [0.3323],\n",
            "        [0.5194],\n",
            "        [0.3828],\n",
            "        [0.3935],\n",
            "        [0.4457],\n",
            "        [0.4327],\n",
            "        [0.4041],\n",
            "        [0.3380],\n",
            "        [0.4228],\n",
            "        [0.5256],\n",
            "        [0.3495],\n",
            "        [0.3874],\n",
            "        [0.3765],\n",
            "        [0.3173],\n",
            "        [0.4040],\n",
            "        [0.3874],\n",
            "        [0.3923],\n",
            "        [0.3963],\n",
            "        [0.2873],\n",
            "        [0.4125],\n",
            "        [0.2814],\n",
            "        [0.3653],\n",
            "        [0.3487],\n",
            "        [0.3679],\n",
            "        [0.2928],\n",
            "        [0.3446],\n",
            "        [0.2753],\n",
            "        [0.3809],\n",
            "        [0.3569],\n",
            "        [0.3819],\n",
            "        [0.4127],\n",
            "        [0.3557],\n",
            "        [0.3588],\n",
            "        [0.3954],\n",
            "        [0.3150],\n",
            "        [0.3843],\n",
            "        [0.4396],\n",
            "        [0.3819],\n",
            "        [0.3333],\n",
            "        [0.3731],\n",
            "        [0.3980],\n",
            "        [0.2158],\n",
            "        [0.2935],\n",
            "        [0.3957],\n",
            "        [0.3853],\n",
            "        [0.3424],\n",
            "        [0.3505],\n",
            "        [0.3531],\n",
            "        [0.3545],\n",
            "        [0.5054],\n",
            "        [0.3222],\n",
            "        [0.5155],\n",
            "        [0.3546],\n",
            "        [0.3600],\n",
            "        [0.4282],\n",
            "        [0.4273],\n",
            "        [0.3506],\n",
            "        [0.3913],\n",
            "        [0.4530],\n",
            "        [0.3822],\n",
            "        [0.3403],\n",
            "        [0.4292],\n",
            "        [0.3589],\n",
            "        [0.2695],\n",
            "        [0.2678],\n",
            "        [0.3099],\n",
            "        [0.3161],\n",
            "        [0.3339],\n",
            "        [0.4600],\n",
            "        [0.2931],\n",
            "        [0.3204],\n",
            "        [0.3483]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:15 loss:0.69204941019877\n",
            "tensor([[0.3846],\n",
            "        [0.3668],\n",
            "        [0.3406],\n",
            "        [0.3917],\n",
            "        [0.3452],\n",
            "        [0.3297],\n",
            "        [0.3544],\n",
            "        [0.4595],\n",
            "        [0.3680],\n",
            "        [0.3332],\n",
            "        [0.3464],\n",
            "        [0.3644],\n",
            "        [0.3945],\n",
            "        [0.3719],\n",
            "        [0.3629],\n",
            "        [0.3726],\n",
            "        [0.3800],\n",
            "        [0.3798],\n",
            "        [0.3360],\n",
            "        [0.3338],\n",
            "        [0.3897],\n",
            "        [0.3582],\n",
            "        [0.3653],\n",
            "        [0.4891],\n",
            "        [0.3553],\n",
            "        [0.3404],\n",
            "        [0.3502],\n",
            "        [0.3836],\n",
            "        [0.4094],\n",
            "        [0.3987],\n",
            "        [0.3226],\n",
            "        [0.3725],\n",
            "        [0.3685],\n",
            "        [0.3223],\n",
            "        [0.3774],\n",
            "        [0.3683],\n",
            "        [0.3593],\n",
            "        [0.3595],\n",
            "        [0.3831],\n",
            "        [0.4013],\n",
            "        [0.3932],\n",
            "        [0.3823],\n",
            "        [0.3674],\n",
            "        [0.3346],\n",
            "        [0.3622],\n",
            "        [0.3812],\n",
            "        [0.3782],\n",
            "        [0.3342],\n",
            "        [0.3560],\n",
            "        [0.3592],\n",
            "        [0.3937],\n",
            "        [0.3964],\n",
            "        [0.3564],\n",
            "        [0.3554],\n",
            "        [0.3369],\n",
            "        [0.3812],\n",
            "        [0.3692],\n",
            "        [0.3548],\n",
            "        [0.3562],\n",
            "        [0.3604],\n",
            "        [0.3499],\n",
            "        [0.3346],\n",
            "        [0.3741],\n",
            "        [0.4089],\n",
            "        [0.3984],\n",
            "        [0.3491],\n",
            "        [0.4015],\n",
            "        [0.3737],\n",
            "        [0.3722],\n",
            "        [0.5238],\n",
            "        [0.3882],\n",
            "        [0.3991],\n",
            "        [0.3522],\n",
            "        [0.3925],\n",
            "        [0.3819],\n",
            "        [0.3700],\n",
            "        [0.3459],\n",
            "        [0.3300],\n",
            "        [0.3546],\n",
            "        [0.3681],\n",
            "        [0.3870],\n",
            "        [0.3505],\n",
            "        [0.3414],\n",
            "        [0.3915],\n",
            "        [0.3444],\n",
            "        [0.3581],\n",
            "        [0.3567],\n",
            "        [0.3515],\n",
            "        [0.3652],\n",
            "        [0.3323],\n",
            "        [0.3524],\n",
            "        [0.3851],\n",
            "        [0.3955],\n",
            "        [0.3611],\n",
            "        [0.3376],\n",
            "        [0.3624],\n",
            "        [0.4202],\n",
            "        [0.3233],\n",
            "        [0.3383],\n",
            "        [0.4356],\n",
            "        [0.3655],\n",
            "        [0.3540],\n",
            "        [0.3600],\n",
            "        [0.3638],\n",
            "        [0.3950],\n",
            "        [0.3496],\n",
            "        [0.3628],\n",
            "        [0.3487],\n",
            "        [0.3464],\n",
            "        [0.3849],\n",
            "        [0.3394],\n",
            "        [0.3640],\n",
            "        [0.3967],\n",
            "        [0.4007],\n",
            "        [0.3098],\n",
            "        [0.3583],\n",
            "        [0.3795],\n",
            "        [0.3992],\n",
            "        [0.3976],\n",
            "        [0.3365],\n",
            "        [0.3832],\n",
            "        [0.3858],\n",
            "        [0.3734],\n",
            "        [0.4084],\n",
            "        [0.4115],\n",
            "        [0.3624],\n",
            "        [0.3515],\n",
            "        [0.4119],\n",
            "        [0.3594],\n",
            "        [0.3642],\n",
            "        [0.3185],\n",
            "        [0.3608],\n",
            "        [0.5091],\n",
            "        [0.3583],\n",
            "        [0.3493],\n",
            "        [0.3598],\n",
            "        [0.3510],\n",
            "        [0.3948],\n",
            "        [0.3855],\n",
            "        [0.3518],\n",
            "        [0.3742],\n",
            "        [0.4285],\n",
            "        [0.3597],\n",
            "        [0.3373],\n",
            "        [0.3392],\n",
            "        [0.3127],\n",
            "        [0.3788],\n",
            "        [0.3533],\n",
            "        [0.3372],\n",
            "        [0.3767],\n",
            "        [0.3689],\n",
            "        [0.3705],\n",
            "        [0.3575],\n",
            "        [0.3673],\n",
            "        [0.3881],\n",
            "        [0.3376],\n",
            "        [0.3106],\n",
            "        [0.3647],\n",
            "        [0.3569],\n",
            "        [0.3430],\n",
            "        [0.4422],\n",
            "        [0.3560],\n",
            "        [0.3715],\n",
            "        [0.4121],\n",
            "        [0.2970],\n",
            "        [0.3786],\n",
            "        [0.3595],\n",
            "        [0.3725],\n",
            "        [0.3685],\n",
            "        [0.3905],\n",
            "        [0.4602],\n",
            "        [0.4010],\n",
            "        [0.3681],\n",
            "        [0.3914],\n",
            "        [0.3656],\n",
            "        [0.3513],\n",
            "        [0.3393],\n",
            "        [0.3800],\n",
            "        [0.3851],\n",
            "        [0.4658],\n",
            "        [0.3749],\n",
            "        [0.3776],\n",
            "        [0.2970],\n",
            "        [0.3567],\n",
            "        [0.4108],\n",
            "        [0.3984],\n",
            "        [0.3571],\n",
            "        [0.3797],\n",
            "        [0.4013],\n",
            "        [0.3652],\n",
            "        [0.4094],\n",
            "        [0.3722],\n",
            "        [0.3502],\n",
            "        [0.3632],\n",
            "        [0.4043],\n",
            "        [0.3333],\n",
            "        [0.3431],\n",
            "        [0.3845],\n",
            "        [0.3986],\n",
            "        [0.3238],\n",
            "        [0.4364],\n",
            "        [0.3572],\n",
            "        [0.3616],\n",
            "        [0.3682],\n",
            "        [0.3536],\n",
            "        [0.3648],\n",
            "        [0.4045],\n",
            "        [0.3562],\n",
            "        [0.3450],\n",
            "        [0.3556],\n",
            "        [0.3680],\n",
            "        [0.3285],\n",
            "        [0.3755],\n",
            "        [0.3929],\n",
            "        [0.3334],\n",
            "        [0.3811],\n",
            "        [0.3921],\n",
            "        [0.3690],\n",
            "        [0.3469],\n",
            "        [0.3762],\n",
            "        [0.3486],\n",
            "        [0.4074],\n",
            "        [0.3411],\n",
            "        [0.3686],\n",
            "        [0.3891],\n",
            "        [0.3580],\n",
            "        [0.3997],\n",
            "        [0.2974],\n",
            "        [0.3372],\n",
            "        [0.4419],\n",
            "        [0.3922],\n",
            "        [0.4641],\n",
            "        [0.3631],\n",
            "        [0.3409],\n",
            "        [0.4061],\n",
            "        [0.3449],\n",
            "        [0.3442],\n",
            "        [0.4098],\n",
            "        [0.3643],\n",
            "        [0.3506],\n",
            "        [0.3447],\n",
            "        [0.3885],\n",
            "        [0.3387],\n",
            "        [0.3869],\n",
            "        [0.3757],\n",
            "        [0.3586],\n",
            "        [0.3577],\n",
            "        [0.3770],\n",
            "        [0.3595],\n",
            "        [0.3971],\n",
            "        [0.3788],\n",
            "        [0.3495],\n",
            "        [0.3651],\n",
            "        [0.3508],\n",
            "        [0.3571],\n",
            "        [0.3606],\n",
            "        [0.3626],\n",
            "        [0.3820],\n",
            "        [0.3362],\n",
            "        [0.3590],\n",
            "        [0.4010],\n",
            "        [0.3317],\n",
            "        [0.3750],\n",
            "        [0.3806],\n",
            "        [0.3426],\n",
            "        [0.4502],\n",
            "        [0.3879],\n",
            "        [0.3254],\n",
            "        [0.4021],\n",
            "        [0.3267],\n",
            "        [0.3475],\n",
            "        [0.3919],\n",
            "        [0.3841],\n",
            "        [0.3577],\n",
            "        [0.3668],\n",
            "        [0.3940],\n",
            "        [0.3586],\n",
            "        [0.3424],\n",
            "        [0.4277],\n",
            "        [0.3770],\n",
            "        [0.3422],\n",
            "        [0.3798],\n",
            "        [0.3474],\n",
            "        [0.3557],\n",
            "        [0.3583],\n",
            "        [0.3432],\n",
            "        [0.3331],\n",
            "        [0.3619],\n",
            "        [0.3373],\n",
            "        [0.3439],\n",
            "        [0.3636],\n",
            "        [0.3606],\n",
            "        [0.4741],\n",
            "        [0.4070],\n",
            "        [0.3872],\n",
            "        [0.3305],\n",
            "        [0.3787],\n",
            "        [0.4032],\n",
            "        [0.3938],\n",
            "        [0.3575],\n",
            "        [0.3836],\n",
            "        [0.3410],\n",
            "        [0.3702],\n",
            "        [0.3831],\n",
            "        [0.3843],\n",
            "        [0.3848],\n",
            "        [0.2821],\n",
            "        [0.3620],\n",
            "        [0.3699],\n",
            "        [0.3574],\n",
            "        [0.3477],\n",
            "        [0.3488],\n",
            "        [0.3538],\n",
            "        [0.3764],\n",
            "        [0.3845],\n",
            "        [0.3625],\n",
            "        [0.3665],\n",
            "        [0.4163],\n",
            "        [0.3547],\n",
            "        [0.3774],\n",
            "        [0.3832],\n",
            "        [0.3358],\n",
            "        [0.3675],\n",
            "        [0.3918],\n",
            "        [0.3884],\n",
            "        [0.3527],\n",
            "        [0.3623],\n",
            "        [0.3470],\n",
            "        [0.3458],\n",
            "        [0.3829],\n",
            "        [0.3857],\n",
            "        [0.3401],\n",
            "        [0.3637],\n",
            "        [0.3738],\n",
            "        [0.3623],\n",
            "        [0.3326],\n",
            "        [0.3628],\n",
            "        [0.3612],\n",
            "        [0.3887],\n",
            "        [0.3327],\n",
            "        [0.3474],\n",
            "        [0.3433],\n",
            "        [0.3333],\n",
            "        [0.3512],\n",
            "        [0.4159],\n",
            "        [0.3897],\n",
            "        [0.3648],\n",
            "        [0.4065],\n",
            "        [0.3887],\n",
            "        [0.3425],\n",
            "        [0.3634],\n",
            "        [0.3592],\n",
            "        [0.3836],\n",
            "        [0.3533],\n",
            "        [0.4006],\n",
            "        [0.3733],\n",
            "        [0.3642],\n",
            "        [0.3392],\n",
            "        [0.3699],\n",
            "        [0.3635],\n",
            "        [0.3306],\n",
            "        [0.3533],\n",
            "        [0.3649],\n",
            "        [0.3400],\n",
            "        [0.3780],\n",
            "        [0.3509],\n",
            "        [0.3876],\n",
            "        [0.3657],\n",
            "        [0.3754],\n",
            "        [0.3223],\n",
            "        [0.4294],\n",
            "        [0.3592],\n",
            "        [0.3619],\n",
            "        [0.3653],\n",
            "        [0.3165],\n",
            "        [0.3612],\n",
            "        [0.3972],\n",
            "        [0.3608],\n",
            "        [0.3673],\n",
            "        [0.3886],\n",
            "        [0.3928],\n",
            "        [0.3721],\n",
            "        [0.3405],\n",
            "        [0.3832],\n",
            "        [0.4006],\n",
            "        [0.3677],\n",
            "        [0.3764],\n",
            "        [0.3930],\n",
            "        [0.3819],\n",
            "        [0.3611],\n",
            "        [0.3753],\n",
            "        [0.3843],\n",
            "        [0.4632],\n",
            "        [0.3544],\n",
            "        [0.3584],\n",
            "        [0.3789],\n",
            "        [0.4090],\n",
            "        [0.3643],\n",
            "        [0.3632],\n",
            "        [0.3772],\n",
            "        [0.3711],\n",
            "        [0.3677],\n",
            "        [0.3792],\n",
            "        [0.3895],\n",
            "        [0.3731],\n",
            "        [0.3696],\n",
            "        [0.3793],\n",
            "        [0.3820],\n",
            "        [0.3654],\n",
            "        [0.3721],\n",
            "        [0.3695],\n",
            "        [0.3624],\n",
            "        [0.3517],\n",
            "        [0.3795],\n",
            "        [0.3398],\n",
            "        [0.3779],\n",
            "        [0.3533],\n",
            "        [0.3761],\n",
            "        [0.3806],\n",
            "        [0.3961],\n",
            "        [0.3390],\n",
            "        [0.3304],\n",
            "        [0.3495],\n",
            "        [0.3658],\n",
            "        [0.3404],\n",
            "        [0.3648],\n",
            "        [0.3710],\n",
            "        [0.3761],\n",
            "        [0.3370],\n",
            "        [0.3662],\n",
            "        [0.3686],\n",
            "        [0.3641],\n",
            "        [0.4035],\n",
            "        [0.3571],\n",
            "        [0.4363],\n",
            "        [0.3592],\n",
            "        [0.3747],\n",
            "        [0.3905],\n",
            "        [0.3843],\n",
            "        [0.3883],\n",
            "        [0.3789],\n",
            "        [0.4003],\n",
            "        [0.3718],\n",
            "        [0.3364],\n",
            "        [0.3910],\n",
            "        [0.3802],\n",
            "        [0.3441],\n",
            "        [0.3392],\n",
            "        [0.3357],\n",
            "        [0.3217],\n",
            "        [0.3385],\n",
            "        [0.3871],\n",
            "        [0.3542],\n",
            "        [0.3324],\n",
            "        [0.4337]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:16 loss:0.683492075368141\n",
            "tensor([[0.3719],\n",
            "        [0.3877],\n",
            "        [0.3473],\n",
            "        [0.5292],\n",
            "        [0.3368],\n",
            "        [0.3584],\n",
            "        [0.3670],\n",
            "        [0.3250],\n",
            "        [0.3273],\n",
            "        [0.3606],\n",
            "        [0.3665],\n",
            "        [0.4282],\n",
            "        [0.3874],\n",
            "        [0.3338],\n",
            "        [0.3131],\n",
            "        [0.4303],\n",
            "        [0.4826],\n",
            "        [0.4257],\n",
            "        [0.3563],\n",
            "        [0.3313],\n",
            "        [0.3208],\n",
            "        [0.3320],\n",
            "        [0.3641],\n",
            "        [0.5233],\n",
            "        [0.3719],\n",
            "        [0.3425],\n",
            "        [0.3639],\n",
            "        [0.2898],\n",
            "        [0.5401],\n",
            "        [0.3473],\n",
            "        [0.3137],\n",
            "        [0.4484],\n",
            "        [0.3577],\n",
            "        [0.3176],\n",
            "        [0.3415],\n",
            "        [0.3852],\n",
            "        [0.3399],\n",
            "        [0.3967],\n",
            "        [0.4412],\n",
            "        [0.3741],\n",
            "        [0.4188],\n",
            "        [0.3560],\n",
            "        [0.4225],\n",
            "        [0.3919],\n",
            "        [0.3546],\n",
            "        [0.3574],\n",
            "        [0.3500],\n",
            "        [0.3462],\n",
            "        [0.3967],\n",
            "        [0.4201],\n",
            "        [0.3644],\n",
            "        [0.3685],\n",
            "        [0.3563],\n",
            "        [0.3296],\n",
            "        [0.3422],\n",
            "        [0.5009],\n",
            "        [0.3613],\n",
            "        [0.3269],\n",
            "        [0.4319],\n",
            "        [0.3225],\n",
            "        [0.3409],\n",
            "        [0.4060],\n",
            "        [0.4533],\n",
            "        [0.3445],\n",
            "        [0.3462],\n",
            "        [0.4684],\n",
            "        [0.4241],\n",
            "        [0.3390],\n",
            "        [0.4528],\n",
            "        [0.3744],\n",
            "        [0.4029],\n",
            "        [0.3530],\n",
            "        [0.3130],\n",
            "        [0.3261],\n",
            "        [0.3307],\n",
            "        [0.4251],\n",
            "        [0.3619],\n",
            "        [0.3628],\n",
            "        [0.3372],\n",
            "        [0.3423],\n",
            "        [0.5399],\n",
            "        [0.3862],\n",
            "        [0.3454],\n",
            "        [0.3424],\n",
            "        [0.3442],\n",
            "        [0.3248],\n",
            "        [0.3652],\n",
            "        [0.4389],\n",
            "        [0.3682],\n",
            "        [0.3552],\n",
            "        [0.3884],\n",
            "        [0.3642],\n",
            "        [0.4989],\n",
            "        [0.3474],\n",
            "        [0.3272],\n",
            "        [0.4101],\n",
            "        [0.4694],\n",
            "        [0.3167],\n",
            "        [0.3425],\n",
            "        [0.3728],\n",
            "        [0.4308],\n",
            "        [0.3150],\n",
            "        [0.4047],\n",
            "        [0.5508],\n",
            "        [0.3604],\n",
            "        [0.4192],\n",
            "        [0.4251],\n",
            "        [0.3723],\n",
            "        [0.3997],\n",
            "        [0.3401],\n",
            "        [0.3738],\n",
            "        [0.3343],\n",
            "        [0.4615],\n",
            "        [0.4513],\n",
            "        [0.3269],\n",
            "        [0.3806],\n",
            "        [0.4454],\n",
            "        [0.4126],\n",
            "        [0.4159],\n",
            "        [0.3377],\n",
            "        [0.3474],\n",
            "        [0.3959],\n",
            "        [0.4662],\n",
            "        [0.4139],\n",
            "        [0.4122],\n",
            "        [0.3298],\n",
            "        [0.3539],\n",
            "        [0.3652],\n",
            "        [0.3672],\n",
            "        [0.3496],\n",
            "        [0.3075],\n",
            "        [0.5321],\n",
            "        [0.4057],\n",
            "        [0.3362],\n",
            "        [0.3904],\n",
            "        [0.3188],\n",
            "        [0.3443],\n",
            "        [0.4127],\n",
            "        [0.4723],\n",
            "        [0.3433],\n",
            "        [0.3895],\n",
            "        [0.3893],\n",
            "        [0.3694],\n",
            "        [0.3274],\n",
            "        [0.3776],\n",
            "        [0.3636],\n",
            "        [0.3886],\n",
            "        [0.3574],\n",
            "        [0.3440],\n",
            "        [0.3534],\n",
            "        [0.4420],\n",
            "        [0.3915],\n",
            "        [0.3114],\n",
            "        [0.3444],\n",
            "        [0.3780],\n",
            "        [0.4121],\n",
            "        [0.3490],\n",
            "        [0.3520],\n",
            "        [0.4197],\n",
            "        [0.3784],\n",
            "        [0.4314],\n",
            "        [0.3340],\n",
            "        [0.3463],\n",
            "        [0.4449],\n",
            "        [0.3265],\n",
            "        [0.4237],\n",
            "        [0.3795],\n",
            "        [0.4018],\n",
            "        [0.3796],\n",
            "        [0.2272],\n",
            "        [0.3592],\n",
            "        [0.5047],\n",
            "        [0.4088],\n",
            "        [0.4538],\n",
            "        [0.3637],\n",
            "        [0.3426],\n",
            "        [0.3599],\n",
            "        [0.2998],\n",
            "        [0.3521],\n",
            "        [0.4406],\n",
            "        [0.4155],\n",
            "        [0.4127],\n",
            "        [0.3043],\n",
            "        [0.3742],\n",
            "        [0.4920],\n",
            "        [0.4088],\n",
            "        [0.3709],\n",
            "        [0.3620],\n",
            "        [0.3604],\n",
            "        [0.3366],\n",
            "        [0.4031],\n",
            "        [0.3743],\n",
            "        [0.3679],\n",
            "        [0.3639],\n",
            "        [0.3568],\n",
            "        [0.3602],\n",
            "        [0.3698],\n",
            "        [0.4727],\n",
            "        [0.3247],\n",
            "        [0.3308],\n",
            "        [0.3921],\n",
            "        [0.3562],\n",
            "        [0.4253],\n",
            "        [0.3352],\n",
            "        [0.3476],\n",
            "        [0.3034],\n",
            "        [0.3376],\n",
            "        [0.4572],\n",
            "        [0.3449],\n",
            "        [0.3378],\n",
            "        [0.3488],\n",
            "        [0.5095],\n",
            "        [0.3326],\n",
            "        [0.4270],\n",
            "        [0.3379],\n",
            "        [0.3852],\n",
            "        [0.4782],\n",
            "        [0.3607],\n",
            "        [0.3766],\n",
            "        [0.3618],\n",
            "        [0.3053],\n",
            "        [0.3937],\n",
            "        [0.3796],\n",
            "        [0.3617],\n",
            "        [0.3917],\n",
            "        [0.3486],\n",
            "        [0.3967],\n",
            "        [0.3040],\n",
            "        [0.3607],\n",
            "        [0.4777],\n",
            "        [0.5442],\n",
            "        [0.3285],\n",
            "        [0.3035],\n",
            "        [0.3347],\n",
            "        [0.4020],\n",
            "        [0.3468],\n",
            "        [0.3296],\n",
            "        [0.3358],\n",
            "        [0.3838],\n",
            "        [0.3516],\n",
            "        [0.3166],\n",
            "        [0.3638],\n",
            "        [0.3615],\n",
            "        [0.4726],\n",
            "        [0.3450],\n",
            "        [0.3858],\n",
            "        [0.3573],\n",
            "        [0.4041],\n",
            "        [0.3193],\n",
            "        [0.3778],\n",
            "        [0.4465],\n",
            "        [0.3321],\n",
            "        [0.3621],\n",
            "        [0.3804],\n",
            "        [0.3801],\n",
            "        [0.3743],\n",
            "        [0.3438],\n",
            "        [0.3584],\n",
            "        [0.3311],\n",
            "        [0.3669],\n",
            "        [0.4763],\n",
            "        [0.2946],\n",
            "        [0.3709],\n",
            "        [0.4548],\n",
            "        [0.3520],\n",
            "        [0.3592],\n",
            "        [0.3701],\n",
            "        [0.3095],\n",
            "        [0.3510],\n",
            "        [0.3324],\n",
            "        [0.3484],\n",
            "        [0.4810],\n",
            "        [0.5285],\n",
            "        [0.2774],\n",
            "        [0.3616],\n",
            "        [0.3695],\n",
            "        [0.3608],\n",
            "        [0.3588],\n",
            "        [0.3872],\n",
            "        [0.4546],\n",
            "        [0.3379],\n",
            "        [0.3664],\n",
            "        [0.3196],\n",
            "        [0.3292],\n",
            "        [0.3463],\n",
            "        [0.3367],\n",
            "        [0.3122],\n",
            "        [0.3546],\n",
            "        [0.3820],\n",
            "        [0.3804],\n",
            "        [0.3767],\n",
            "        [0.3722],\n",
            "        [0.3940],\n",
            "        [0.3615],\n",
            "        [0.3743],\n",
            "        [0.3550],\n",
            "        [0.3966],\n",
            "        [0.2974],\n",
            "        [0.3939],\n",
            "        [0.3575],\n",
            "        [0.4337],\n",
            "        [0.3569],\n",
            "        [0.3688],\n",
            "        [0.4011],\n",
            "        [0.3743],\n",
            "        [0.4473],\n",
            "        [0.3185],\n",
            "        [0.4061],\n",
            "        [0.3271],\n",
            "        [0.3653],\n",
            "        [0.3659],\n",
            "        [0.4083],\n",
            "        [0.3463],\n",
            "        [0.4325],\n",
            "        [0.3339],\n",
            "        [0.3724],\n",
            "        [0.3722],\n",
            "        [0.4984],\n",
            "        [0.4521],\n",
            "        [0.4461],\n",
            "        [0.3618],\n",
            "        [0.3177],\n",
            "        [0.4477],\n",
            "        [0.3741],\n",
            "        [0.5180],\n",
            "        [0.3927],\n",
            "        [0.3583],\n",
            "        [0.3345],\n",
            "        [0.3793],\n",
            "        [0.3560],\n",
            "        [0.4636],\n",
            "        [0.3386],\n",
            "        [0.3144],\n",
            "        [0.3451],\n",
            "        [0.3815],\n",
            "        [0.3354],\n",
            "        [0.3739],\n",
            "        [0.3379],\n",
            "        [0.3557],\n",
            "        [0.3421],\n",
            "        [0.3310],\n",
            "        [0.3409],\n",
            "        [0.3682],\n",
            "        [0.3947],\n",
            "        [0.5855],\n",
            "        [0.3735],\n",
            "        [0.4226],\n",
            "        [0.4252],\n",
            "        [0.3637],\n",
            "        [0.3737],\n",
            "        [0.3744],\n",
            "        [0.3355],\n",
            "        [0.3489],\n",
            "        [0.3159],\n",
            "        [0.3832],\n",
            "        [0.3404],\n",
            "        [0.3544],\n",
            "        [0.4284],\n",
            "        [0.4185],\n",
            "        [0.3600],\n",
            "        [0.3166],\n",
            "        [0.3538],\n",
            "        [0.4354],\n",
            "        [0.4560],\n",
            "        [0.4325],\n",
            "        [0.4446],\n",
            "        [0.4296],\n",
            "        [0.3201],\n",
            "        [0.3446],\n",
            "        [0.3347],\n",
            "        [0.4207],\n",
            "        [0.3517],\n",
            "        [0.3197],\n",
            "        [0.3255],\n",
            "        [0.2887],\n",
            "        [0.4761],\n",
            "        [0.4789],\n",
            "        [0.3931],\n",
            "        [0.3166],\n",
            "        [0.4886],\n",
            "        [0.3613],\n",
            "        [0.3746],\n",
            "        [0.3950],\n",
            "        [0.4280],\n",
            "        [0.2939],\n",
            "        [0.3625],\n",
            "        [0.3698],\n",
            "        [0.3493],\n",
            "        [0.3423],\n",
            "        [0.3293],\n",
            "        [0.4071],\n",
            "        [0.3521],\n",
            "        [0.3810],\n",
            "        [0.3660],\n",
            "        [0.3384],\n",
            "        [0.3778],\n",
            "        [0.4940],\n",
            "        [0.3381],\n",
            "        [0.3490],\n",
            "        [0.3603],\n",
            "        [0.3543],\n",
            "        [0.4344],\n",
            "        [0.3548],\n",
            "        [0.4922],\n",
            "        [0.3932],\n",
            "        [0.3962],\n",
            "        [0.3950],\n",
            "        [0.4654],\n",
            "        [0.3942],\n",
            "        [0.4675],\n",
            "        [0.3614],\n",
            "        [0.3773],\n",
            "        [0.3313],\n",
            "        [0.3301],\n",
            "        [0.3394],\n",
            "        [0.4002],\n",
            "        [0.3257],\n",
            "        [0.4391],\n",
            "        [0.3789],\n",
            "        [0.3570],\n",
            "        [0.2940],\n",
            "        [0.3428],\n",
            "        [0.3389],\n",
            "        [0.3436],\n",
            "        [0.4697],\n",
            "        [0.4301],\n",
            "        [0.3474],\n",
            "        [0.3741],\n",
            "        [0.3406],\n",
            "        [0.3877],\n",
            "        [0.3917],\n",
            "        [0.3685],\n",
            "        [0.3071],\n",
            "        [0.4025],\n",
            "        [0.3641],\n",
            "        [0.3527],\n",
            "        [0.3946],\n",
            "        [0.3447],\n",
            "        [0.3541],\n",
            "        [0.4201],\n",
            "        [0.3752],\n",
            "        [0.3578],\n",
            "        [0.3691],\n",
            "        [0.3429],\n",
            "        [0.3618],\n",
            "        [0.3968],\n",
            "        [0.4125],\n",
            "        [0.4089],\n",
            "        [0.3743],\n",
            "        [0.3387],\n",
            "        [0.3543],\n",
            "        [0.3258],\n",
            "        [0.4103],\n",
            "        [0.3537],\n",
            "        [0.5046]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:17 loss:0.6859661868077226\n",
            "tensor([[0.3774],\n",
            "        [0.4162],\n",
            "        [0.3791],\n",
            "        [0.5642],\n",
            "        [0.3581],\n",
            "        [0.4022],\n",
            "        [0.3982],\n",
            "        [0.2392],\n",
            "        [0.3175],\n",
            "        [0.4107],\n",
            "        [0.3964],\n",
            "        [0.4607],\n",
            "        [0.3958],\n",
            "        [0.3291],\n",
            "        [0.3013],\n",
            "        [0.4530],\n",
            "        [0.5374],\n",
            "        [0.4556],\n",
            "        [0.3928],\n",
            "        [0.3566],\n",
            "        [0.2939],\n",
            "        [0.3382],\n",
            "        [0.3791],\n",
            "        [0.4911],\n",
            "        [0.4009],\n",
            "        [0.3740],\n",
            "        [0.3967],\n",
            "        [0.2286],\n",
            "        [0.5865],\n",
            "        [0.3021],\n",
            "        [0.3359],\n",
            "        [0.4853],\n",
            "        [0.3688],\n",
            "        [0.3427],\n",
            "        [0.3134],\n",
            "        [0.4067],\n",
            "        [0.3539],\n",
            "        [0.4172],\n",
            "        [0.4760],\n",
            "        [0.3479],\n",
            "        [0.4405],\n",
            "        [0.3328],\n",
            "        [0.4428],\n",
            "        [0.4181],\n",
            "        [0.3643],\n",
            "        [0.3474],\n",
            "        [0.3336],\n",
            "        [0.3672],\n",
            "        [0.4465],\n",
            "        [0.4714],\n",
            "        [0.3693],\n",
            "        [0.3653],\n",
            "        [0.3785],\n",
            "        [0.3211],\n",
            "        [0.3713],\n",
            "        [0.5683],\n",
            "        [0.3759],\n",
            "        [0.3273],\n",
            "        [0.4522],\n",
            "        [0.3183],\n",
            "        [0.3423],\n",
            "        [0.4339],\n",
            "        [0.5113],\n",
            "        [0.3032],\n",
            "        [0.3279],\n",
            "        [0.5326],\n",
            "        [0.4252],\n",
            "        [0.3325],\n",
            "        [0.5190],\n",
            "        [0.2612],\n",
            "        [0.3896],\n",
            "        [0.3343],\n",
            "        [0.3086],\n",
            "        [0.3062],\n",
            "        [0.3195],\n",
            "        [0.4724],\n",
            "        [0.3980],\n",
            "        [0.4106],\n",
            "        [0.3393],\n",
            "        [0.3358],\n",
            "        [0.5840],\n",
            "        [0.4331],\n",
            "        [0.3733],\n",
            "        [0.2887],\n",
            "        [0.3668],\n",
            "        [0.3190],\n",
            "        [0.3867],\n",
            "        [0.4682],\n",
            "        [0.3754],\n",
            "        [0.3832],\n",
            "        [0.3820],\n",
            "        [0.3606],\n",
            "        [0.5578],\n",
            "        [0.3542],\n",
            "        [0.3404],\n",
            "        [0.4503],\n",
            "        [0.4628],\n",
            "        [0.3226],\n",
            "        [0.3598],\n",
            "        [0.3473],\n",
            "        [0.4446],\n",
            "        [0.3131],\n",
            "        [0.4378],\n",
            "        [0.6283],\n",
            "        [0.3471],\n",
            "        [0.4451],\n",
            "        [0.4694],\n",
            "        [0.4124],\n",
            "        [0.4343],\n",
            "        [0.3183],\n",
            "        [0.4179],\n",
            "        [0.3409],\n",
            "        [0.4534],\n",
            "        [0.4690],\n",
            "        [0.3618],\n",
            "        [0.4134],\n",
            "        [0.4730],\n",
            "        [0.3875],\n",
            "        [0.3839],\n",
            "        [0.3586],\n",
            "        [0.2999],\n",
            "        [0.4141],\n",
            "        [0.5214],\n",
            "        [0.3900],\n",
            "        [0.3475],\n",
            "        [0.3280],\n",
            "        [0.3720],\n",
            "        [0.3338],\n",
            "        [0.3615],\n",
            "        [0.3594],\n",
            "        [0.3265],\n",
            "        [0.6101],\n",
            "        [0.3032],\n",
            "        [0.3376],\n",
            "        [0.4277],\n",
            "        [0.3200],\n",
            "        [0.3612],\n",
            "        [0.4158],\n",
            "        [0.5255],\n",
            "        [0.3532],\n",
            "        [0.3864],\n",
            "        [0.3665],\n",
            "        [0.3970],\n",
            "        [0.3368],\n",
            "        [0.3816],\n",
            "        [0.3856],\n",
            "        [0.3890],\n",
            "        [0.3603],\n",
            "        [0.3767],\n",
            "        [0.3588],\n",
            "        [0.4953],\n",
            "        [0.4222],\n",
            "        [0.3066],\n",
            "        [0.3496],\n",
            "        [0.3809],\n",
            "        [0.4820],\n",
            "        [0.3975],\n",
            "        [0.3576],\n",
            "        [0.4415],\n",
            "        [0.4105],\n",
            "        [0.3989],\n",
            "        [0.3367],\n",
            "        [0.3472],\n",
            "        [0.4521],\n",
            "        [0.3737],\n",
            "        [0.4328],\n",
            "        [0.3953],\n",
            "        [0.4274],\n",
            "        [0.4016],\n",
            "        [0.1276],\n",
            "        [0.3026],\n",
            "        [0.5399],\n",
            "        [0.4319],\n",
            "        [0.4923],\n",
            "        [0.3829],\n",
            "        [0.3441],\n",
            "        [0.3971],\n",
            "        [0.2740],\n",
            "        [0.3303],\n",
            "        [0.4158],\n",
            "        [0.4533],\n",
            "        [0.4156],\n",
            "        [0.3350],\n",
            "        [0.3913],\n",
            "        [0.5203],\n",
            "        [0.3996],\n",
            "        [0.3963],\n",
            "        [0.3626],\n",
            "        [0.3450],\n",
            "        [0.3403],\n",
            "        [0.3865],\n",
            "        [0.3930],\n",
            "        [0.4054],\n",
            "        [0.3813],\n",
            "        [0.3245],\n",
            "        [0.3949],\n",
            "        [0.3727],\n",
            "        [0.4840],\n",
            "        [0.2860],\n",
            "        [0.3609],\n",
            "        [0.3456],\n",
            "        [0.3773],\n",
            "        [0.4004],\n",
            "        [0.3380],\n",
            "        [0.3310],\n",
            "        [0.2731],\n",
            "        [0.2994],\n",
            "        [0.4888],\n",
            "        [0.3718],\n",
            "        [0.3444],\n",
            "        [0.3500],\n",
            "        [0.6087],\n",
            "        [0.3171],\n",
            "        [0.4274],\n",
            "        [0.3692],\n",
            "        [0.3925],\n",
            "        [0.4884],\n",
            "        [0.3769],\n",
            "        [0.4090],\n",
            "        [0.3611],\n",
            "        [0.3069],\n",
            "        [0.3784],\n",
            "        [0.4178],\n",
            "        [0.3628],\n",
            "        [0.4058],\n",
            "        [0.3439],\n",
            "        [0.4004],\n",
            "        [0.3357],\n",
            "        [0.3311],\n",
            "        [0.4693],\n",
            "        [0.6234],\n",
            "        [0.2153],\n",
            "        [0.2788],\n",
            "        [0.3501],\n",
            "        [0.3859],\n",
            "        [0.3670],\n",
            "        [0.3367],\n",
            "        [0.2692],\n",
            "        [0.3990],\n",
            "        [0.3578],\n",
            "        [0.3169],\n",
            "        [0.3526],\n",
            "        [0.4047],\n",
            "        [0.5071],\n",
            "        [0.3080],\n",
            "        [0.4183],\n",
            "        [0.3761],\n",
            "        [0.4009],\n",
            "        [0.3051],\n",
            "        [0.3772],\n",
            "        [0.4867],\n",
            "        [0.3386],\n",
            "        [0.3570],\n",
            "        [0.4207],\n",
            "        [0.4115],\n",
            "        [0.3753],\n",
            "        [0.3403],\n",
            "        [0.3340],\n",
            "        [0.3563],\n",
            "        [0.3705],\n",
            "        [0.5120],\n",
            "        [0.2572],\n",
            "        [0.3717],\n",
            "        [0.5159],\n",
            "        [0.3591],\n",
            "        [0.3082],\n",
            "        [0.3690],\n",
            "        [0.3179],\n",
            "        [0.3258],\n",
            "        [0.3552],\n",
            "        [0.3534],\n",
            "        [0.5059],\n",
            "        [0.6060],\n",
            "        [0.2500],\n",
            "        [0.3678],\n",
            "        [0.3554],\n",
            "        [0.3844],\n",
            "        [0.3773],\n",
            "        [0.3558],\n",
            "        [0.4663],\n",
            "        [0.3466],\n",
            "        [0.3725],\n",
            "        [0.3186],\n",
            "        [0.3361],\n",
            "        [0.3468],\n",
            "        [0.3507],\n",
            "        [0.3179],\n",
            "        [0.3620],\n",
            "        [0.4282],\n",
            "        [0.4244],\n",
            "        [0.3719],\n",
            "        [0.3683],\n",
            "        [0.3556],\n",
            "        [0.3391],\n",
            "        [0.3681],\n",
            "        [0.4035],\n",
            "        [0.4142],\n",
            "        [0.2318],\n",
            "        [0.3846],\n",
            "        [0.3793],\n",
            "        [0.4458],\n",
            "        [0.3998],\n",
            "        [0.3722],\n",
            "        [0.3894],\n",
            "        [0.3819],\n",
            "        [0.4796],\n",
            "        [0.3799],\n",
            "        [0.4327],\n",
            "        [0.3165],\n",
            "        [0.3861],\n",
            "        [0.4052],\n",
            "        [0.4605],\n",
            "        [0.3528],\n",
            "        [0.4654],\n",
            "        [0.3132],\n",
            "        [0.3896],\n",
            "        [0.4010],\n",
            "        [0.4777],\n",
            "        [0.5149],\n",
            "        [0.4543],\n",
            "        [0.3548],\n",
            "        [0.3346],\n",
            "        [0.4872],\n",
            "        [0.3696],\n",
            "        [0.5808],\n",
            "        [0.3973],\n",
            "        [0.3694],\n",
            "        [0.3493],\n",
            "        [0.4246],\n",
            "        [0.3440],\n",
            "        [0.4990],\n",
            "        [0.3512],\n",
            "        [0.2939],\n",
            "        [0.3440],\n",
            "        [0.4166],\n",
            "        [0.3645],\n",
            "        [0.3842],\n",
            "        [0.3412],\n",
            "        [0.3374],\n",
            "        [0.3713],\n",
            "        [0.3414],\n",
            "        [0.3500],\n",
            "        [0.4100],\n",
            "        [0.4214],\n",
            "        [0.6568],\n",
            "        [0.3366],\n",
            "        [0.4226],\n",
            "        [0.4170],\n",
            "        [0.3460],\n",
            "        [0.4106],\n",
            "        [0.3825],\n",
            "        [0.3447],\n",
            "        [0.3400],\n",
            "        [0.3145],\n",
            "        [0.3524],\n",
            "        [0.3328],\n",
            "        [0.3635],\n",
            "        [0.4732],\n",
            "        [0.4509],\n",
            "        [0.3768],\n",
            "        [0.3185],\n",
            "        [0.3717],\n",
            "        [0.4718],\n",
            "        [0.5218],\n",
            "        [0.4551],\n",
            "        [0.4839],\n",
            "        [0.4520],\n",
            "        [0.2967],\n",
            "        [0.3281],\n",
            "        [0.3605],\n",
            "        [0.3877],\n",
            "        [0.3356],\n",
            "        [0.3137],\n",
            "        [0.3176],\n",
            "        [0.2944],\n",
            "        [0.5444],\n",
            "        [0.5072],\n",
            "        [0.4279],\n",
            "        [0.2996],\n",
            "        [0.5328],\n",
            "        [0.3485],\n",
            "        [0.3869],\n",
            "        [0.4494],\n",
            "        [0.4454],\n",
            "        [0.2320],\n",
            "        [0.3766],\n",
            "        [0.3843],\n",
            "        [0.3319],\n",
            "        [0.3296],\n",
            "        [0.3241],\n",
            "        [0.4187],\n",
            "        [0.3403],\n",
            "        [0.2937],\n",
            "        [0.3881],\n",
            "        [0.3391],\n",
            "        [0.3724],\n",
            "        [0.5331],\n",
            "        [0.3439],\n",
            "        [0.3573],\n",
            "        [0.3437],\n",
            "        [0.3565],\n",
            "        [0.4454],\n",
            "        [0.3530],\n",
            "        [0.5364],\n",
            "        [0.4310],\n",
            "        [0.4250],\n",
            "        [0.4154],\n",
            "        [0.5010],\n",
            "        [0.4304],\n",
            "        [0.5151],\n",
            "        [0.3622],\n",
            "        [0.4058],\n",
            "        [0.3334],\n",
            "        [0.2694],\n",
            "        [0.3689],\n",
            "        [0.4209],\n",
            "        [0.3294],\n",
            "        [0.4800],\n",
            "        [0.3808],\n",
            "        [0.3349],\n",
            "        [0.2528],\n",
            "        [0.3811],\n",
            "        [0.3547],\n",
            "        [0.3448],\n",
            "        [0.5274],\n",
            "        [0.4570],\n",
            "        [0.3333],\n",
            "        [0.3873],\n",
            "        [0.3595],\n",
            "        [0.4140],\n",
            "        [0.4235],\n",
            "        [0.3633],\n",
            "        [0.2410],\n",
            "        [0.4530],\n",
            "        [0.3223],\n",
            "        [0.3298],\n",
            "        [0.4189],\n",
            "        [0.2995],\n",
            "        [0.3548],\n",
            "        [0.4318],\n",
            "        [0.3889],\n",
            "        [0.3413],\n",
            "        [0.3811],\n",
            "        [0.3676],\n",
            "        [0.3559],\n",
            "        [0.4004],\n",
            "        [0.4365],\n",
            "        [0.4436],\n",
            "        [0.4255],\n",
            "        [0.3719],\n",
            "        [0.3870],\n",
            "        [0.2960],\n",
            "        [0.4346],\n",
            "        [0.3877],\n",
            "        [0.5240]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:18 loss:0.6899923796183003\n",
            "tensor([[0.3984],\n",
            "        [0.4452],\n",
            "        [0.4274],\n",
            "        [0.5047],\n",
            "        [0.4020],\n",
            "        [0.4498],\n",
            "        [0.4391],\n",
            "        [0.2076],\n",
            "        [0.3388],\n",
            "        [0.4689],\n",
            "        [0.4288],\n",
            "        [0.4579],\n",
            "        [0.4165],\n",
            "        [0.3559],\n",
            "        [0.3248],\n",
            "        [0.4416],\n",
            "        [0.5360],\n",
            "        [0.4644],\n",
            "        [0.4341],\n",
            "        [0.4019],\n",
            "        [0.3098],\n",
            "        [0.3729],\n",
            "        [0.4046],\n",
            "        [0.4148],\n",
            "        [0.4346],\n",
            "        [0.4249],\n",
            "        [0.4397],\n",
            "        [0.2061],\n",
            "        [0.5518],\n",
            "        [0.2756],\n",
            "        [0.3823],\n",
            "        [0.4789],\n",
            "        [0.3979],\n",
            "        [0.3905],\n",
            "        [0.3002],\n",
            "        [0.4269],\n",
            "        [0.3959],\n",
            "        [0.4188],\n",
            "        [0.4827],\n",
            "        [0.3318],\n",
            "        [0.4533],\n",
            "        [0.3199],\n",
            "        [0.4286],\n",
            "        [0.4104],\n",
            "        [0.3887],\n",
            "        [0.3529],\n",
            "        [0.3334],\n",
            "        [0.3927],\n",
            "        [0.4914],\n",
            "        [0.5008],\n",
            "        [0.4059],\n",
            "        [0.3850],\n",
            "        [0.4155],\n",
            "        [0.3309],\n",
            "        [0.4159],\n",
            "        [0.5733],\n",
            "        [0.4078],\n",
            "        [0.3525],\n",
            "        [0.4212],\n",
            "        [0.3444],\n",
            "        [0.3531],\n",
            "        [0.4167],\n",
            "        [0.5352],\n",
            "        [0.2919],\n",
            "        [0.3447],\n",
            "        [0.5305],\n",
            "        [0.4091],\n",
            "        [0.3533],\n",
            "        [0.5546],\n",
            "        [0.2072],\n",
            "        [0.3593],\n",
            "        [0.3429],\n",
            "        [0.3374],\n",
            "        [0.3301],\n",
            "        [0.3480],\n",
            "        [0.5019],\n",
            "        [0.4437],\n",
            "        [0.4597],\n",
            "        [0.3586],\n",
            "        [0.3479],\n",
            "        [0.5265],\n",
            "        [0.4776],\n",
            "        [0.4179],\n",
            "        [0.2484],\n",
            "        [0.4061],\n",
            "        [0.3424],\n",
            "        [0.4150],\n",
            "        [0.4397],\n",
            "        [0.3849],\n",
            "        [0.4096],\n",
            "        [0.3440],\n",
            "        [0.3732],\n",
            "        [0.5631],\n",
            "        [0.3799],\n",
            "        [0.3721],\n",
            "        [0.4734],\n",
            "        [0.4128],\n",
            "        [0.3417],\n",
            "        [0.3853],\n",
            "        [0.3615],\n",
            "        [0.4114],\n",
            "        [0.3471],\n",
            "        [0.4532],\n",
            "        [0.5974],\n",
            "        [0.3569],\n",
            "        [0.4271],\n",
            "        [0.4861],\n",
            "        [0.4566],\n",
            "        [0.4438],\n",
            "        [0.3225],\n",
            "        [0.4586],\n",
            "        [0.3783],\n",
            "        [0.3878],\n",
            "        [0.4549],\n",
            "        [0.4056],\n",
            "        [0.4482],\n",
            "        [0.4613],\n",
            "        [0.3407],\n",
            "        [0.3244],\n",
            "        [0.3942],\n",
            "        [0.2588],\n",
            "        [0.4355],\n",
            "        [0.5287],\n",
            "        [0.3515],\n",
            "        [0.2604],\n",
            "        [0.3548],\n",
            "        [0.4003],\n",
            "        [0.3250],\n",
            "        [0.3479],\n",
            "        [0.3904],\n",
            "        [0.3704],\n",
            "        [0.5907],\n",
            "        [0.2359],\n",
            "        [0.3608],\n",
            "        [0.4521],\n",
            "        [0.3582],\n",
            "        [0.3960],\n",
            "        [0.4070],\n",
            "        [0.5353],\n",
            "        [0.3780],\n",
            "        [0.3703],\n",
            "        [0.3640],\n",
            "        [0.4336],\n",
            "        [0.3627],\n",
            "        [0.3586],\n",
            "        [0.3775],\n",
            "        [0.3830],\n",
            "        [0.3639],\n",
            "        [0.4251],\n",
            "        [0.3896],\n",
            "        [0.5158],\n",
            "        [0.4537],\n",
            "        [0.3396],\n",
            "        [0.3798],\n",
            "        [0.3959],\n",
            "        [0.5271],\n",
            "        [0.4425],\n",
            "        [0.3799],\n",
            "        [0.4238],\n",
            "        [0.4330],\n",
            "        [0.3601],\n",
            "        [0.3613],\n",
            "        [0.3710],\n",
            "        [0.4376],\n",
            "        [0.4295],\n",
            "        [0.4118],\n",
            "        [0.4050],\n",
            "        [0.4442],\n",
            "        [0.4289],\n",
            "        [0.0875],\n",
            "        [0.2953],\n",
            "        [0.5085],\n",
            "        [0.4345],\n",
            "        [0.4998],\n",
            "        [0.4166],\n",
            "        [0.3563],\n",
            "        [0.4409],\n",
            "        [0.2986],\n",
            "        [0.3252],\n",
            "        [0.3985],\n",
            "        [0.4795],\n",
            "        [0.3925],\n",
            "        [0.3818],\n",
            "        [0.4042],\n",
            "        [0.4976],\n",
            "        [0.3779],\n",
            "        [0.4271],\n",
            "        [0.3796],\n",
            "        [0.3558],\n",
            "        [0.3724],\n",
            "        [0.3699],\n",
            "        [0.4218],\n",
            "        [0.4526],\n",
            "        [0.4120],\n",
            "        [0.3153],\n",
            "        [0.4303],\n",
            "        [0.3557],\n",
            "        [0.4280],\n",
            "        [0.2851],\n",
            "        [0.4068],\n",
            "        [0.3127],\n",
            "        [0.4140],\n",
            "        [0.3186],\n",
            "        [0.3740],\n",
            "        [0.3121],\n",
            "        [0.2758],\n",
            "        [0.2933],\n",
            "        [0.4522],\n",
            "        [0.4175],\n",
            "        [0.3715],\n",
            "        [0.3701],\n",
            "        [0.6103],\n",
            "        [0.3294],\n",
            "        [0.4010],\n",
            "        [0.4175],\n",
            "        [0.4018],\n",
            "        [0.4333],\n",
            "        [0.4108],\n",
            "        [0.4358],\n",
            "        [0.3741],\n",
            "        [0.3487],\n",
            "        [0.3667],\n",
            "        [0.4462],\n",
            "        [0.3720],\n",
            "        [0.4271],\n",
            "        [0.3467],\n",
            "        [0.4094],\n",
            "        [0.3856],\n",
            "        [0.2736],\n",
            "        [0.4275],\n",
            "        [0.6237],\n",
            "        [0.1541],\n",
            "        [0.2894],\n",
            "        [0.3812],\n",
            "        [0.3657],\n",
            "        [0.3992],\n",
            "        [0.3621],\n",
            "        [0.2283],\n",
            "        [0.4077],\n",
            "        [0.3683],\n",
            "        [0.3441],\n",
            "        [0.3586],\n",
            "        [0.4554],\n",
            "        [0.4895],\n",
            "        [0.2784],\n",
            "        [0.4476],\n",
            "        [0.4086],\n",
            "        [0.3755],\n",
            "        [0.3163],\n",
            "        [0.3947],\n",
            "        [0.4924],\n",
            "        [0.3657],\n",
            "        [0.3527],\n",
            "        [0.4611],\n",
            "        [0.4425],\n",
            "        [0.3674],\n",
            "        [0.3512],\n",
            "        [0.3175],\n",
            "        [0.4042],\n",
            "        [0.3715],\n",
            "        [0.5043],\n",
            "        [0.2315],\n",
            "        [0.3770],\n",
            "        [0.5490],\n",
            "        [0.3646],\n",
            "        [0.3024],\n",
            "        [0.3829],\n",
            "        [0.3482],\n",
            "        [0.3290],\n",
            "        [0.3900],\n",
            "        [0.3617],\n",
            "        [0.4705],\n",
            "        [0.6078],\n",
            "        [0.2740],\n",
            "        [0.3836],\n",
            "        [0.3546],\n",
            "        [0.4214],\n",
            "        [0.3932],\n",
            "        [0.3404],\n",
            "        [0.4198],\n",
            "        [0.3666],\n",
            "        [0.3955],\n",
            "        [0.3429],\n",
            "        [0.3736],\n",
            "        [0.3596],\n",
            "        [0.3811],\n",
            "        [0.3468],\n",
            "        [0.3817],\n",
            "        [0.4647],\n",
            "        [0.4634],\n",
            "        [0.3552],\n",
            "        [0.3536],\n",
            "        [0.3619],\n",
            "        [0.3430],\n",
            "        [0.3705],\n",
            "        [0.4613],\n",
            "        [0.4283],\n",
            "        [0.2111],\n",
            "        [0.3725],\n",
            "        [0.4167],\n",
            "        [0.4239],\n",
            "        [0.4566],\n",
            "        [0.3794],\n",
            "        [0.3588],\n",
            "        [0.4052],\n",
            "        [0.4782],\n",
            "        [0.4526],\n",
            "        [0.4384],\n",
            "        [0.3370],\n",
            "        [0.4150],\n",
            "        [0.4544],\n",
            "        [0.4922],\n",
            "        [0.3711],\n",
            "        [0.4696],\n",
            "        [0.3235],\n",
            "        [0.4100],\n",
            "        [0.4432],\n",
            "        [0.3803],\n",
            "        [0.5290],\n",
            "        [0.4114],\n",
            "        [0.3640],\n",
            "        [0.3819],\n",
            "        [0.4807],\n",
            "        [0.3779],\n",
            "        [0.5716],\n",
            "        [0.3733],\n",
            "        [0.3921],\n",
            "        [0.3855],\n",
            "        [0.4692],\n",
            "        [0.3496],\n",
            "        [0.4886],\n",
            "        [0.3753],\n",
            "        [0.3027],\n",
            "        [0.3674],\n",
            "        [0.4569],\n",
            "        [0.4152],\n",
            "        [0.3922],\n",
            "        [0.3674],\n",
            "        [0.3376],\n",
            "        [0.4130],\n",
            "        [0.3757],\n",
            "        [0.3689],\n",
            "        [0.4481],\n",
            "        [0.4268],\n",
            "        [0.6346],\n",
            "        [0.2956],\n",
            "        [0.3764],\n",
            "        [0.3919],\n",
            "        [0.3405],\n",
            "        [0.4432],\n",
            "        [0.3890],\n",
            "        [0.3819],\n",
            "        [0.3560],\n",
            "        [0.3465],\n",
            "        [0.3211],\n",
            "        [0.3500],\n",
            "        [0.3894],\n",
            "        [0.4667],\n",
            "        [0.4603],\n",
            "        [0.4082],\n",
            "        [0.3350],\n",
            "        [0.4012],\n",
            "        [0.4707],\n",
            "        [0.5256],\n",
            "        [0.4458],\n",
            "        [0.4655],\n",
            "        [0.4519],\n",
            "        [0.2982],\n",
            "        [0.3300],\n",
            "        [0.3938],\n",
            "        [0.3466],\n",
            "        [0.3189],\n",
            "        [0.3423],\n",
            "        [0.3403],\n",
            "        [0.3307],\n",
            "        [0.5526],\n",
            "        [0.4836],\n",
            "        [0.4563],\n",
            "        [0.3158],\n",
            "        [0.5181],\n",
            "        [0.3552],\n",
            "        [0.4083],\n",
            "        [0.4884],\n",
            "        [0.4353],\n",
            "        [0.2173],\n",
            "        [0.4053],\n",
            "        [0.4149],\n",
            "        [0.3438],\n",
            "        [0.3433],\n",
            "        [0.3445],\n",
            "        [0.4113],\n",
            "        [0.3502],\n",
            "        [0.2322],\n",
            "        [0.4160],\n",
            "        [0.3605],\n",
            "        [0.3666],\n",
            "        [0.5229],\n",
            "        [0.3792],\n",
            "        [0.3853],\n",
            "        [0.3328],\n",
            "        [0.3767],\n",
            "        [0.4078],\n",
            "        [0.3726],\n",
            "        [0.5195],\n",
            "        [0.4744],\n",
            "        [0.4484],\n",
            "        [0.4352],\n",
            "        [0.4870],\n",
            "        [0.4633],\n",
            "        [0.5083],\n",
            "        [0.3722],\n",
            "        [0.4387],\n",
            "        [0.3557],\n",
            "        [0.2215],\n",
            "        [0.4209],\n",
            "        [0.4349],\n",
            "        [0.3613],\n",
            "        [0.4906],\n",
            "        [0.3856],\n",
            "        [0.3342],\n",
            "        [0.2271],\n",
            "        [0.4342],\n",
            "        [0.3907],\n",
            "        [0.3668],\n",
            "        [0.5068],\n",
            "        [0.4440],\n",
            "        [0.3333],\n",
            "        [0.4116],\n",
            "        [0.3883],\n",
            "        [0.4372],\n",
            "        [0.4563],\n",
            "        [0.3548],\n",
            "        [0.2130],\n",
            "        [0.4944],\n",
            "        [0.3157],\n",
            "        [0.3018],\n",
            "        [0.4406],\n",
            "        [0.2692],\n",
            "        [0.3856],\n",
            "        [0.4262],\n",
            "        [0.4144],\n",
            "        [0.3511],\n",
            "        [0.4040],\n",
            "        [0.4030],\n",
            "        [0.3728],\n",
            "        [0.3929],\n",
            "        [0.4177],\n",
            "        [0.4396],\n",
            "        [0.4744],\n",
            "        [0.4127],\n",
            "        [0.4272],\n",
            "        [0.3002],\n",
            "        [0.4264],\n",
            "        [0.4249],\n",
            "        [0.4953]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:19 loss:0.6900476634791712\n",
            "tensor([[0.4225],\n",
            "        [0.4633],\n",
            "        [0.4673],\n",
            "        [0.4146],\n",
            "        [0.4438],\n",
            "        [0.4807],\n",
            "        [0.4687],\n",
            "        [0.2242],\n",
            "        [0.3797],\n",
            "        [0.5057],\n",
            "        [0.4515],\n",
            "        [0.4354],\n",
            "        [0.4384],\n",
            "        [0.3979],\n",
            "        [0.3669],\n",
            "        [0.4188],\n",
            "        [0.4982],\n",
            "        [0.4567],\n",
            "        [0.4589],\n",
            "        [0.4422],\n",
            "        [0.3586],\n",
            "        [0.4155],\n",
            "        [0.4260],\n",
            "        [0.3544],\n",
            "        [0.4572],\n",
            "        [0.4659],\n",
            "        [0.4719],\n",
            "        [0.2229],\n",
            "        [0.4811],\n",
            "        [0.2778],\n",
            "        [0.4269],\n",
            "        [0.4493],\n",
            "        [0.4287],\n",
            "        [0.4356],\n",
            "        [0.3073],\n",
            "        [0.4378],\n",
            "        [0.4419],\n",
            "        [0.4098],\n",
            "        [0.4703],\n",
            "        [0.3355],\n",
            "        [0.4543],\n",
            "        [0.3241],\n",
            "        [0.4019],\n",
            "        [0.3872],\n",
            "        [0.4167],\n",
            "        [0.3705],\n",
            "        [0.3507],\n",
            "        [0.4138],\n",
            "        [0.5127],\n",
            "        [0.5027],\n",
            "        [0.4539],\n",
            "        [0.4134],\n",
            "        [0.4472],\n",
            "        [0.3536],\n",
            "        [0.4522],\n",
            "        [0.5356],\n",
            "        [0.4394],\n",
            "        [0.3858],\n",
            "        [0.3777],\n",
            "        [0.3821],\n",
            "        [0.3677],\n",
            "        [0.3821],\n",
            "        [0.5269],\n",
            "        [0.3120],\n",
            "        [0.3854],\n",
            "        [0.4872],\n",
            "        [0.3923],\n",
            "        [0.3887],\n",
            "        [0.5558],\n",
            "        [0.2134],\n",
            "        [0.3379],\n",
            "        [0.3678],\n",
            "        [0.3844],\n",
            "        [0.3790],\n",
            "        [0.4008],\n",
            "        [0.5093],\n",
            "        [0.4753],\n",
            "        [0.4871],\n",
            "        [0.3834],\n",
            "        [0.3697],\n",
            "        [0.4302],\n",
            "        [0.4995],\n",
            "        [0.4569],\n",
            "        [0.2389],\n",
            "        [0.4422],\n",
            "        [0.3865],\n",
            "        [0.4360],\n",
            "        [0.3910],\n",
            "        [0.3932],\n",
            "        [0.4250],\n",
            "        [0.3097],\n",
            "        [0.3927],\n",
            "        [0.5302],\n",
            "        [0.4123],\n",
            "        [0.4043],\n",
            "        [0.4746],\n",
            "        [0.3630],\n",
            "        [0.3694],\n",
            "        [0.4072],\n",
            "        [0.4042],\n",
            "        [0.3667],\n",
            "        [0.3997],\n",
            "        [0.4523],\n",
            "        [0.5086],\n",
            "        [0.3826],\n",
            "        [0.3937],\n",
            "        [0.4776],\n",
            "        [0.4821],\n",
            "        [0.4349],\n",
            "        [0.3478],\n",
            "        [0.4772],\n",
            "        [0.4218],\n",
            "        [0.3225],\n",
            "        [0.4301],\n",
            "        [0.4384],\n",
            "        [0.4703],\n",
            "        [0.4326],\n",
            "        [0.3082],\n",
            "        [0.2860],\n",
            "        [0.4287],\n",
            "        [0.2449],\n",
            "        [0.4502],\n",
            "        [0.5007],\n",
            "        [0.3280],\n",
            "        [0.2144],\n",
            "        [0.3942],\n",
            "        [0.4237],\n",
            "        [0.3418],\n",
            "        [0.3393],\n",
            "        [0.4263],\n",
            "        [0.4170],\n",
            "        [0.5156],\n",
            "        [0.2242],\n",
            "        [0.3939],\n",
            "        [0.4579],\n",
            "        [0.4084],\n",
            "        [0.4294],\n",
            "        [0.3976],\n",
            "        [0.5130],\n",
            "        [0.4046],\n",
            "        [0.3575],\n",
            "        [0.3793],\n",
            "        [0.4596],\n",
            "        [0.3927],\n",
            "        [0.3378],\n",
            "        [0.3584],\n",
            "        [0.3788],\n",
            "        [0.3718],\n",
            "        [0.4614],\n",
            "        [0.4284],\n",
            "        [0.5036],\n",
            "        [0.4711],\n",
            "        [0.3898],\n",
            "        [0.4172],\n",
            "        [0.4160],\n",
            "        [0.5335],\n",
            "        [0.4642],\n",
            "        [0.4086],\n",
            "        [0.3940],\n",
            "        [0.4425],\n",
            "        [0.3404],\n",
            "        [0.3925],\n",
            "        [0.4026],\n",
            "        [0.4195],\n",
            "        [0.4738],\n",
            "        [0.3889],\n",
            "        [0.4093],\n",
            "        [0.4491],\n",
            "        [0.4496],\n",
            "        [0.0903],\n",
            "        [0.3304],\n",
            "        [0.4502],\n",
            "        [0.4250],\n",
            "        [0.4839],\n",
            "        [0.4462],\n",
            "        [0.3751],\n",
            "        [0.4705],\n",
            "        [0.3558],\n",
            "        [0.3385],\n",
            "        [0.3965],\n",
            "        [0.4876],\n",
            "        [0.3684],\n",
            "        [0.4228],\n",
            "        [0.4103],\n",
            "        [0.4552],\n",
            "        [0.3615],\n",
            "        [0.4500],\n",
            "        [0.4022],\n",
            "        [0.3828],\n",
            "        [0.4132],\n",
            "        [0.3697],\n",
            "        [0.4446],\n",
            "        [0.4864],\n",
            "        [0.4437],\n",
            "        [0.3329],\n",
            "        [0.4556],\n",
            "        [0.3369],\n",
            "        [0.3608],\n",
            "        [0.3155],\n",
            "        [0.4462],\n",
            "        [0.3090],\n",
            "        [0.4468],\n",
            "        [0.2590],\n",
            "        [0.4237],\n",
            "        [0.3046],\n",
            "        [0.3057],\n",
            "        [0.3153],\n",
            "        [0.3921],\n",
            "        [0.4568],\n",
            "        [0.4028],\n",
            "        [0.3979],\n",
            "        [0.5466],\n",
            "        [0.3594],\n",
            "        [0.3739],\n",
            "        [0.4555],\n",
            "        [0.4106],\n",
            "        [0.3692],\n",
            "        [0.4420],\n",
            "        [0.4485],\n",
            "        [0.3954],\n",
            "        [0.4052],\n",
            "        [0.3652],\n",
            "        [0.4567],\n",
            "        [0.3863],\n",
            "        [0.4453],\n",
            "        [0.3589],\n",
            "        [0.4193],\n",
            "        [0.4304],\n",
            "        [0.2375],\n",
            "        [0.3873],\n",
            "        [0.5720],\n",
            "        [0.1479],\n",
            "        [0.3260],\n",
            "        [0.4096],\n",
            "        [0.3563],\n",
            "        [0.4270],\n",
            "        [0.3922],\n",
            "        [0.2247],\n",
            "        [0.4110],\n",
            "        [0.3802],\n",
            "        [0.3842],\n",
            "        [0.3805],\n",
            "        [0.4871],\n",
            "        [0.4483],\n",
            "        [0.2720],\n",
            "        [0.4624],\n",
            "        [0.4376],\n",
            "        [0.3534],\n",
            "        [0.3429],\n",
            "        [0.4205],\n",
            "        [0.4731],\n",
            "        [0.3987],\n",
            "        [0.3534],\n",
            "        [0.4846],\n",
            "        [0.4596],\n",
            "        [0.3622],\n",
            "        [0.3690],\n",
            "        [0.3185],\n",
            "        [0.4478],\n",
            "        [0.3736],\n",
            "        [0.4723],\n",
            "        [0.2299],\n",
            "        [0.3841],\n",
            "        [0.5484],\n",
            "        [0.3718],\n",
            "        [0.3366],\n",
            "        [0.4022],\n",
            "        [0.3866],\n",
            "        [0.3545],\n",
            "        [0.4210],\n",
            "        [0.3707],\n",
            "        [0.4163],\n",
            "        [0.5588],\n",
            "        [0.3388],\n",
            "        [0.4019],\n",
            "        [0.3659],\n",
            "        [0.4510],\n",
            "        [0.4013],\n",
            "        [0.3446],\n",
            "        [0.3627],\n",
            "        [0.3905],\n",
            "        [0.4231],\n",
            "        [0.3795],\n",
            "        [0.4224],\n",
            "        [0.3790],\n",
            "        [0.4126],\n",
            "        [0.3820],\n",
            "        [0.4033],\n",
            "        [0.4806],\n",
            "        [0.4809],\n",
            "        [0.3430],\n",
            "        [0.3420],\n",
            "        [0.4011],\n",
            "        [0.3684],\n",
            "        [0.3804],\n",
            "        [0.4969],\n",
            "        [0.4367],\n",
            "        [0.2337],\n",
            "        [0.3691],\n",
            "        [0.4507],\n",
            "        [0.3947],\n",
            "        [0.4956],\n",
            "        [0.3875],\n",
            "        [0.3362],\n",
            "        [0.4328],\n",
            "        [0.4585],\n",
            "        [0.5062],\n",
            "        [0.4311],\n",
            "        [0.3750],\n",
            "        [0.4404],\n",
            "        [0.4883],\n",
            "        [0.4972],\n",
            "        [0.3924],\n",
            "        [0.4540],\n",
            "        [0.3566],\n",
            "        [0.4257],\n",
            "        [0.4744],\n",
            "        [0.2931],\n",
            "        [0.5043],\n",
            "        [0.3635],\n",
            "        [0.3845],\n",
            "        [0.4353],\n",
            "        [0.4484],\n",
            "        [0.3931],\n",
            "        [0.5196],\n",
            "        [0.3488],\n",
            "        [0.4149],\n",
            "        [0.4215],\n",
            "        [0.4934],\n",
            "        [0.3700],\n",
            "        [0.4546],\n",
            "        [0.4012],\n",
            "        [0.3319],\n",
            "        [0.3990],\n",
            "        [0.4820],\n",
            "        [0.4668],\n",
            "        [0.3966],\n",
            "        [0.4003],\n",
            "        [0.3549],\n",
            "        [0.4477],\n",
            "        [0.4169],\n",
            "        [0.3906],\n",
            "        [0.4680],\n",
            "        [0.4173],\n",
            "        [0.5573],\n",
            "        [0.2753],\n",
            "        [0.3296],\n",
            "        [0.3750],\n",
            "        [0.3498],\n",
            "        [0.4578],\n",
            "        [0.3981],\n",
            "        [0.4246],\n",
            "        [0.3857],\n",
            "        [0.3928],\n",
            "        [0.3090],\n",
            "        [0.3813],\n",
            "        [0.4200],\n",
            "        [0.4331],\n",
            "        [0.4503],\n",
            "        [0.4369],\n",
            "        [0.3580],\n",
            "        [0.4270],\n",
            "        [0.4506],\n",
            "        [0.4910],\n",
            "        [0.4238],\n",
            "        [0.4217],\n",
            "        [0.4386],\n",
            "        [0.3202],\n",
            "        [0.3500],\n",
            "        [0.4213],\n",
            "        [0.3256],\n",
            "        [0.3145],\n",
            "        [0.3890],\n",
            "        [0.3800],\n",
            "        [0.3802],\n",
            "        [0.5167],\n",
            "        [0.4399],\n",
            "        [0.4684],\n",
            "        [0.3528],\n",
            "        [0.4743],\n",
            "        [0.3749],\n",
            "        [0.4337],\n",
            "        [0.4984],\n",
            "        [0.4141],\n",
            "        [0.2461],\n",
            "        [0.4337],\n",
            "        [0.4448],\n",
            "        [0.3782],\n",
            "        [0.3728],\n",
            "        [0.3786],\n",
            "        [0.3987],\n",
            "        [0.3744],\n",
            "        [0.2186],\n",
            "        [0.4388],\n",
            "        [0.3939],\n",
            "        [0.3667],\n",
            "        [0.4851],\n",
            "        [0.4251],\n",
            "        [0.4183],\n",
            "        [0.3343],\n",
            "        [0.4048],\n",
            "        [0.3622],\n",
            "        [0.4017],\n",
            "        [0.4719],\n",
            "        [0.5004],\n",
            "        [0.4571],\n",
            "        [0.4457],\n",
            "        [0.4508],\n",
            "        [0.4773],\n",
            "        [0.4693],\n",
            "        [0.3877],\n",
            "        [0.4585],\n",
            "        [0.3852],\n",
            "        [0.2093],\n",
            "        [0.4689],\n",
            "        [0.4387],\n",
            "        [0.4034],\n",
            "        [0.4776],\n",
            "        [0.3914],\n",
            "        [0.3537],\n",
            "        [0.2276],\n",
            "        [0.4732],\n",
            "        [0.4258],\n",
            "        [0.3956],\n",
            "        [0.4473],\n",
            "        [0.4131],\n",
            "        [0.3488],\n",
            "        [0.4345],\n",
            "        [0.4128],\n",
            "        [0.4470],\n",
            "        [0.4779],\n",
            "        [0.3549],\n",
            "        [0.2237],\n",
            "        [0.5103],\n",
            "        [0.3393],\n",
            "        [0.2879],\n",
            "        [0.4507],\n",
            "        [0.2669],\n",
            "        [0.4312],\n",
            "        [0.4177],\n",
            "        [0.4360],\n",
            "        [0.3773],\n",
            "        [0.4261],\n",
            "        [0.4308],\n",
            "        [0.4008],\n",
            "        [0.3838],\n",
            "        [0.3869],\n",
            "        [0.4179],\n",
            "        [0.4985],\n",
            "        [0.4426],\n",
            "        [0.4544],\n",
            "        [0.3321],\n",
            "        [0.4056],\n",
            "        [0.4474],\n",
            "        [0.4494]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:20 loss:0.6880514963200385\n",
            "tensor([[0.4370],\n",
            "        [0.4623],\n",
            "        [0.4761],\n",
            "        [0.3630],\n",
            "        [0.4582],\n",
            "        [0.4792],\n",
            "        [0.4699],\n",
            "        [0.2893],\n",
            "        [0.4228],\n",
            "        [0.4989],\n",
            "        [0.4553],\n",
            "        [0.4131],\n",
            "        [0.4500],\n",
            "        [0.4334],\n",
            "        [0.4057],\n",
            "        [0.4080],\n",
            "        [0.4516],\n",
            "        [0.4415],\n",
            "        [0.4515],\n",
            "        [0.4530],\n",
            "        [0.4227],\n",
            "        [0.4425],\n",
            "        [0.4306],\n",
            "        [0.3545],\n",
            "        [0.4569],\n",
            "        [0.4705],\n",
            "        [0.4758],\n",
            "        [0.2825],\n",
            "        [0.4275],\n",
            "        [0.3141],\n",
            "        [0.4426],\n",
            "        [0.4214],\n",
            "        [0.4444],\n",
            "        [0.4517],\n",
            "        [0.3362],\n",
            "        [0.4343],\n",
            "        [0.4656],\n",
            "        [0.4007],\n",
            "        [0.4520],\n",
            "        [0.3644],\n",
            "        [0.4443],\n",
            "        [0.3486],\n",
            "        [0.3860],\n",
            "        [0.3700],\n",
            "        [0.4356],\n",
            "        [0.3946],\n",
            "        [0.3836],\n",
            "        [0.4226],\n",
            "        [0.4990],\n",
            "        [0.4793],\n",
            "        [0.4887],\n",
            "        [0.4347],\n",
            "        [0.4548],\n",
            "        [0.3813],\n",
            "        [0.4586],\n",
            "        [0.4832],\n",
            "        [0.4531],\n",
            "        [0.4079],\n",
            "        [0.3587],\n",
            "        [0.4094],\n",
            "        [0.3805],\n",
            "        [0.3599],\n",
            "        [0.4957],\n",
            "        [0.3611],\n",
            "        [0.4325],\n",
            "        [0.4366],\n",
            "        [0.3898],\n",
            "        [0.4219],\n",
            "        [0.5270],\n",
            "        [0.2884],\n",
            "        [0.3456],\n",
            "        [0.3952],\n",
            "        [0.4279],\n",
            "        [0.4266],\n",
            "        [0.4541],\n",
            "        [0.4958],\n",
            "        [0.4737],\n",
            "        [0.4771],\n",
            "        [0.4011],\n",
            "        [0.3907],\n",
            "        [0.3679],\n",
            "        [0.4863],\n",
            "        [0.4693],\n",
            "        [0.2709],\n",
            "        [0.4557],\n",
            "        [0.4360],\n",
            "        [0.4382],\n",
            "        [0.3613],\n",
            "        [0.3978],\n",
            "        [0.4236],\n",
            "        [0.3069],\n",
            "        [0.4087],\n",
            "        [0.4827],\n",
            "        [0.4374],\n",
            "        [0.4186],\n",
            "        [0.4557],\n",
            "        [0.3505],\n",
            "        [0.3990],\n",
            "        [0.4153],\n",
            "        [0.4575],\n",
            "        [0.3449],\n",
            "        [0.4458],\n",
            "        [0.4410],\n",
            "        [0.4277],\n",
            "        [0.4134],\n",
            "        [0.3746],\n",
            "        [0.4529],\n",
            "        [0.4727],\n",
            "        [0.4190],\n",
            "        [0.3850],\n",
            "        [0.4623],\n",
            "        [0.4443],\n",
            "        [0.3028],\n",
            "        [0.4165],\n",
            "        [0.4429],\n",
            "        [0.4690],\n",
            "        [0.4117],\n",
            "        [0.3156],\n",
            "        [0.3010],\n",
            "        [0.4455],\n",
            "        [0.2707],\n",
            "        [0.4507],\n",
            "        [0.4581],\n",
            "        [0.3407],\n",
            "        [0.2386],\n",
            "        [0.4260],\n",
            "        [0.4289],\n",
            "        [0.3819],\n",
            "        [0.3466],\n",
            "        [0.4488],\n",
            "        [0.4412],\n",
            "        [0.4389],\n",
            "        [0.2808],\n",
            "        [0.4215],\n",
            "        [0.4452],\n",
            "        [0.4396],\n",
            "        [0.4417],\n",
            "        [0.3983],\n",
            "        [0.4769],\n",
            "        [0.4196],\n",
            "        [0.3619],\n",
            "        [0.4067],\n",
            "        [0.4586],\n",
            "        [0.4129],\n",
            "        [0.3436],\n",
            "        [0.3485],\n",
            "        [0.3838],\n",
            "        [0.3864],\n",
            "        [0.4621],\n",
            "        [0.4547],\n",
            "        [0.4679],\n",
            "        [0.4643],\n",
            "        [0.4294],\n",
            "        [0.4413],\n",
            "        [0.4329],\n",
            "        [0.4991],\n",
            "        [0.4511],\n",
            "        [0.4314],\n",
            "        [0.3801],\n",
            "        [0.4396],\n",
            "        [0.3572],\n",
            "        [0.4129],\n",
            "        [0.4251],\n",
            "        [0.4154],\n",
            "        [0.4891],\n",
            "        [0.3900],\n",
            "        [0.4106],\n",
            "        [0.4427],\n",
            "        [0.4538],\n",
            "        [0.1467],\n",
            "        [0.3965],\n",
            "        [0.4095],\n",
            "        [0.4145],\n",
            "        [0.4579],\n",
            "        [0.4542],\n",
            "        [0.3950],\n",
            "        [0.4700],\n",
            "        [0.4182],\n",
            "        [0.3690],\n",
            "        [0.4140],\n",
            "        [0.4767],\n",
            "        [0.3648],\n",
            "        [0.4365],\n",
            "        [0.4087],\n",
            "        [0.4271],\n",
            "        [0.3648],\n",
            "        [0.4540],\n",
            "        [0.4189],\n",
            "        [0.4125],\n",
            "        [0.4394],\n",
            "        [0.3968],\n",
            "        [0.4474],\n",
            "        [0.4881],\n",
            "        [0.4630],\n",
            "        [0.3762],\n",
            "        [0.4629],\n",
            "        [0.3330],\n",
            "        [0.3334],\n",
            "        [0.3667],\n",
            "        [0.4582],\n",
            "        [0.3434],\n",
            "        [0.4573],\n",
            "        [0.2690],\n",
            "        [0.4624],\n",
            "        [0.3185],\n",
            "        [0.3530],\n",
            "        [0.3583],\n",
            "        [0.3556],\n",
            "        [0.4661],\n",
            "        [0.4203],\n",
            "        [0.4202],\n",
            "        [0.4611],\n",
            "        [0.3931],\n",
            "        [0.3691],\n",
            "        [0.4589],\n",
            "        [0.4171],\n",
            "        [0.3464],\n",
            "        [0.4508],\n",
            "        [0.4428],\n",
            "        [0.4177],\n",
            "        [0.4435],\n",
            "        [0.3783],\n",
            "        [0.4463],\n",
            "        [0.4012],\n",
            "        [0.4512],\n",
            "        [0.3805],\n",
            "        [0.4254],\n",
            "        [0.4469],\n",
            "        [0.2529],\n",
            "        [0.3787],\n",
            "        [0.5032],\n",
            "        [0.2057],\n",
            "        [0.3743],\n",
            "        [0.4177],\n",
            "        [0.3689],\n",
            "        [0.4349],\n",
            "        [0.4116],\n",
            "        [0.2664],\n",
            "        [0.4114],\n",
            "        [0.3907],\n",
            "        [0.4189],\n",
            "        [0.4127],\n",
            "        [0.4800],\n",
            "        [0.4162],\n",
            "        [0.2987],\n",
            "        [0.4559],\n",
            "        [0.4473],\n",
            "        [0.3560],\n",
            "        [0.3721],\n",
            "        [0.4428],\n",
            "        [0.4445],\n",
            "        [0.4205],\n",
            "        [0.3625],\n",
            "        [0.4794],\n",
            "        [0.4540],\n",
            "        [0.3692],\n",
            "        [0.3851],\n",
            "        [0.3424],\n",
            "        [0.4605],\n",
            "        [0.3807],\n",
            "        [0.4401],\n",
            "        [0.2612],\n",
            "        [0.3908],\n",
            "        [0.5172],\n",
            "        [0.3835],\n",
            "        [0.4011],\n",
            "        [0.4167],\n",
            "        [0.4154],\n",
            "        [0.3920],\n",
            "        [0.4333],\n",
            "        [0.3782],\n",
            "        [0.3857],\n",
            "        [0.4924],\n",
            "        [0.4224],\n",
            "        [0.4148],\n",
            "        [0.3858],\n",
            "        [0.4549],\n",
            "        [0.3991],\n",
            "        [0.3689],\n",
            "        [0.3387],\n",
            "        [0.4097],\n",
            "        [0.4415],\n",
            "        [0.4120],\n",
            "        [0.4581],\n",
            "        [0.3978],\n",
            "        [0.4291],\n",
            "        [0.4045],\n",
            "        [0.4163],\n",
            "        [0.4713],\n",
            "        [0.4673],\n",
            "        [0.3494],\n",
            "        [0.3455],\n",
            "        [0.4551],\n",
            "        [0.4059],\n",
            "        [0.3955],\n",
            "        [0.4871],\n",
            "        [0.4385],\n",
            "        [0.3018],\n",
            "        [0.3828],\n",
            "        [0.4626],\n",
            "        [0.3838],\n",
            "        [0.4915],\n",
            "        [0.3938],\n",
            "        [0.3423],\n",
            "        [0.4518],\n",
            "        [0.4396],\n",
            "        [0.5162],\n",
            "        [0.4217],\n",
            "        [0.4120],\n",
            "        [0.4514],\n",
            "        [0.4869],\n",
            "        [0.4772],\n",
            "        [0.4079],\n",
            "        [0.4323],\n",
            "        [0.3993],\n",
            "        [0.4304],\n",
            "        [0.4742],\n",
            "        [0.2760],\n",
            "        [0.4596],\n",
            "        [0.3508],\n",
            "        [0.4088],\n",
            "        [0.4661],\n",
            "        [0.4160],\n",
            "        [0.4077],\n",
            "        [0.4630],\n",
            "        [0.3481],\n",
            "        [0.4267],\n",
            "        [0.4358],\n",
            "        [0.4842],\n",
            "        [0.3993],\n",
            "        [0.4236],\n",
            "        [0.4184],\n",
            "        [0.3693],\n",
            "        [0.4203],\n",
            "        [0.4765],\n",
            "        [0.4967],\n",
            "        [0.3978],\n",
            "        [0.4220],\n",
            "        [0.3851],\n",
            "        [0.4579],\n",
            "        [0.4452],\n",
            "        [0.4075],\n",
            "        [0.4610],\n",
            "        [0.4033],\n",
            "        [0.4762],\n",
            "        [0.2925],\n",
            "        [0.3193],\n",
            "        [0.3863],\n",
            "        [0.3733],\n",
            "        [0.4469],\n",
            "        [0.4131],\n",
            "        [0.4477],\n",
            "        [0.4145],\n",
            "        [0.4285],\n",
            "        [0.3293],\n",
            "        [0.4125],\n",
            "        [0.4415],\n",
            "        [0.4021],\n",
            "        [0.4301],\n",
            "        [0.4464],\n",
            "        [0.3780],\n",
            "        [0.4347],\n",
            "        [0.4335],\n",
            "        [0.4495],\n",
            "        [0.4101],\n",
            "        [0.3890],\n",
            "        [0.4242],\n",
            "        [0.3550],\n",
            "        [0.3842],\n",
            "        [0.4312],\n",
            "        [0.3438],\n",
            "        [0.3313],\n",
            "        [0.4309],\n",
            "        [0.4182],\n",
            "        [0.4193],\n",
            "        [0.4625],\n",
            "        [0.4105],\n",
            "        [0.4589],\n",
            "        [0.3934],\n",
            "        [0.4364],\n",
            "        [0.3983],\n",
            "        [0.4561],\n",
            "        [0.4750],\n",
            "        [0.3994],\n",
            "        [0.3178],\n",
            "        [0.4466],\n",
            "        [0.4571],\n",
            "        [0.4227],\n",
            "        [0.4041],\n",
            "        [0.4107],\n",
            "        [0.3947],\n",
            "        [0.4026],\n",
            "        [0.2663],\n",
            "        [0.4470],\n",
            "        [0.4267],\n",
            "        [0.3776],\n",
            "        [0.4471],\n",
            "        [0.4583],\n",
            "        [0.4397],\n",
            "        [0.3519],\n",
            "        [0.4286],\n",
            "        [0.3451],\n",
            "        [0.4260],\n",
            "        [0.4303],\n",
            "        [0.4918],\n",
            "        [0.4469],\n",
            "        [0.4419],\n",
            "        [0.4230],\n",
            "        [0.4635],\n",
            "        [0.4270],\n",
            "        [0.4038],\n",
            "        [0.4524],\n",
            "        [0.4070],\n",
            "        [0.2465],\n",
            "        [0.4861],\n",
            "        [0.4320],\n",
            "        [0.4338],\n",
            "        [0.4536],\n",
            "        [0.3962],\n",
            "        [0.3884],\n",
            "        [0.2616],\n",
            "        [0.4738],\n",
            "        [0.4386],\n",
            "        [0.4153],\n",
            "        [0.3962],\n",
            "        [0.3899],\n",
            "        [0.3774],\n",
            "        [0.4441],\n",
            "        [0.4205],\n",
            "        [0.4375],\n",
            "        [0.4791],\n",
            "        [0.3724],\n",
            "        [0.2761],\n",
            "        [0.4922],\n",
            "        [0.3837],\n",
            "        [0.3017],\n",
            "        [0.4436],\n",
            "        [0.3009],\n",
            "        [0.4708],\n",
            "        [0.4199],\n",
            "        [0.4400],\n",
            "        [0.4064],\n",
            "        [0.4362],\n",
            "        [0.4346],\n",
            "        [0.4258],\n",
            "        [0.3827],\n",
            "        [0.3747],\n",
            "        [0.4022],\n",
            "        [0.4838],\n",
            "        [0.4462],\n",
            "        [0.4526],\n",
            "        [0.3811],\n",
            "        [0.3935],\n",
            "        [0.4422],\n",
            "        [0.4194]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:21 loss:0.6840114437570599\n",
            "tensor([[0.4374],\n",
            "        [0.4432],\n",
            "        [0.4502],\n",
            "        [0.3737],\n",
            "        [0.4392],\n",
            "        [0.4459],\n",
            "        [0.4422],\n",
            "        [0.3977],\n",
            "        [0.4541],\n",
            "        [0.4500],\n",
            "        [0.4403],\n",
            "        [0.4027],\n",
            "        [0.4474],\n",
            "        [0.4498],\n",
            "        [0.4265],\n",
            "        [0.4183],\n",
            "        [0.4170],\n",
            "        [0.4264],\n",
            "        [0.4141],\n",
            "        [0.4293],\n",
            "        [0.4817],\n",
            "        [0.4434],\n",
            "        [0.4169],\n",
            "        [0.4242],\n",
            "        [0.4343],\n",
            "        [0.4358],\n",
            "        [0.4498],\n",
            "        [0.3815],\n",
            "        [0.4171],\n",
            "        [0.3816],\n",
            "        [0.4228],\n",
            "        [0.4091],\n",
            "        [0.4394],\n",
            "        [0.4322],\n",
            "        [0.3829],\n",
            "        [0.4188],\n",
            "        [0.4576],\n",
            "        [0.3980],\n",
            "        [0.4376],\n",
            "        [0.4149],\n",
            "        [0.4278],\n",
            "        [0.3902],\n",
            "        [0.3909],\n",
            "        [0.3700],\n",
            "        [0.4400],\n",
            "        [0.4191],\n",
            "        [0.4255],\n",
            "        [0.4178],\n",
            "        [0.4546],\n",
            "        [0.4405],\n",
            "        [0.4980],\n",
            "        [0.4412],\n",
            "        [0.4352],\n",
            "        [0.4069],\n",
            "        [0.4322],\n",
            "        [0.4390],\n",
            "        [0.4441],\n",
            "        [0.4105],\n",
            "        [0.3769],\n",
            "        [0.4154],\n",
            "        [0.3890],\n",
            "        [0.3629],\n",
            "        [0.4546],\n",
            "        [0.4287],\n",
            "        [0.4707],\n",
            "        [0.4023],\n",
            "        [0.4064],\n",
            "        [0.4417],\n",
            "        [0.4795],\n",
            "        [0.4397],\n",
            "        [0.3865],\n",
            "        [0.4159],\n",
            "        [0.4528],\n",
            "        [0.4549],\n",
            "        [0.4894],\n",
            "        [0.4678],\n",
            "        [0.4387],\n",
            "        [0.4332],\n",
            "        [0.4066],\n",
            "        [0.4042],\n",
            "        [0.3676],\n",
            "        [0.4424],\n",
            "        [0.4505],\n",
            "        [0.3466],\n",
            "        [0.4418],\n",
            "        [0.4761],\n",
            "        [0.4214],\n",
            "        [0.3666],\n",
            "        [0.3989],\n",
            "        [0.4075],\n",
            "        [0.3431],\n",
            "        [0.4161],\n",
            "        [0.4404],\n",
            "        [0.4475],\n",
            "        [0.4096],\n",
            "        [0.4247],\n",
            "        [0.3853],\n",
            "        [0.4240],\n",
            "        [0.4078],\n",
            "        [0.5034],\n",
            "        [0.3578],\n",
            "        [0.4690],\n",
            "        [0.4260],\n",
            "        [0.3938],\n",
            "        [0.4402],\n",
            "        [0.3822],\n",
            "        [0.4231],\n",
            "        [0.4312],\n",
            "        [0.4050],\n",
            "        [0.4239],\n",
            "        [0.4194],\n",
            "        [0.4359],\n",
            "        [0.3397],\n",
            "        [0.4231],\n",
            "        [0.4182],\n",
            "        [0.4452],\n",
            "        [0.4110],\n",
            "        [0.3687],\n",
            "        [0.3778],\n",
            "        [0.4394],\n",
            "        [0.3395],\n",
            "        [0.4374],\n",
            "        [0.4195],\n",
            "        [0.3933],\n",
            "        [0.3497],\n",
            "        [0.4390],\n",
            "        [0.4143],\n",
            "        [0.4362],\n",
            "        [0.3718],\n",
            "        [0.4505],\n",
            "        [0.4344],\n",
            "        [0.3961],\n",
            "        [0.4149],\n",
            "        [0.4348],\n",
            "        [0.4202],\n",
            "        [0.4386],\n",
            "        [0.4283],\n",
            "        [0.4123],\n",
            "        [0.4431],\n",
            "        [0.4182],\n",
            "        [0.3866],\n",
            "        [0.4388],\n",
            "        [0.4307],\n",
            "        [0.4174],\n",
            "        [0.3823],\n",
            "        [0.3566],\n",
            "        [0.3997],\n",
            "        [0.4067],\n",
            "        [0.4259],\n",
            "        [0.4587],\n",
            "        [0.4232],\n",
            "        [0.4358],\n",
            "        [0.4428],\n",
            "        [0.4433],\n",
            "        [0.4417],\n",
            "        [0.4371],\n",
            "        [0.4088],\n",
            "        [0.4417],\n",
            "        [0.3922],\n",
            "        [0.4279],\n",
            "        [0.4117],\n",
            "        [0.4156],\n",
            "        [0.4307],\n",
            "        [0.4309],\n",
            "        [0.4722],\n",
            "        [0.4219],\n",
            "        [0.4111],\n",
            "        [0.4287],\n",
            "        [0.4410],\n",
            "        [0.3030],\n",
            "        [0.4747],\n",
            "        [0.4057],\n",
            "        [0.4096],\n",
            "        [0.4332],\n",
            "        [0.4376],\n",
            "        [0.4115],\n",
            "        [0.4398],\n",
            "        [0.4623],\n",
            "        [0.4103],\n",
            "        [0.4483],\n",
            "        [0.4521],\n",
            "        [0.3878],\n",
            "        [0.4186],\n",
            "        [0.4017],\n",
            "        [0.4280],\n",
            "        [0.3912],\n",
            "        [0.4389],\n",
            "        [0.4243],\n",
            "        [0.4352],\n",
            "        [0.4409],\n",
            "        [0.4488],\n",
            "        [0.4291],\n",
            "        [0.4567],\n",
            "        [0.4643],\n",
            "        [0.4364],\n",
            "        [0.4517],\n",
            "        [0.3497],\n",
            "        [0.3603],\n",
            "        [0.4245],\n",
            "        [0.4383],\n",
            "        [0.4136],\n",
            "        [0.4415],\n",
            "        [0.3634],\n",
            "        [0.4760],\n",
            "        [0.3554],\n",
            "        [0.4052],\n",
            "        [0.4115],\n",
            "        [0.3602],\n",
            "        [0.4414],\n",
            "        [0.4179],\n",
            "        [0.4300],\n",
            "        [0.3918],\n",
            "        [0.4199],\n",
            "        [0.3930],\n",
            "        [0.4255],\n",
            "        [0.4209],\n",
            "        [0.3796],\n",
            "        [0.4332],\n",
            "        [0.4225],\n",
            "        [0.4354],\n",
            "        [0.4474],\n",
            "        [0.4047],\n",
            "        [0.4204],\n",
            "        [0.4136],\n",
            "        [0.4432],\n",
            "        [0.4081],\n",
            "        [0.4265],\n",
            "        [0.4299],\n",
            "        [0.3301],\n",
            "        [0.4094],\n",
            "        [0.4466],\n",
            "        [0.3544],\n",
            "        [0.4192],\n",
            "        [0.4025],\n",
            "        [0.4040],\n",
            "        [0.4207],\n",
            "        [0.4140],\n",
            "        [0.3556],\n",
            "        [0.4114],\n",
            "        [0.3983],\n",
            "        [0.4366],\n",
            "        [0.4476],\n",
            "        [0.4360],\n",
            "        [0.4091],\n",
            "        [0.3584],\n",
            "        [0.4313],\n",
            "        [0.4340],\n",
            "        [0.3885],\n",
            "        [0.3946],\n",
            "        [0.4542],\n",
            "        [0.4194],\n",
            "        [0.4237],\n",
            "        [0.3797],\n",
            "        [0.4475],\n",
            "        [0.4281],\n",
            "        [0.3903],\n",
            "        [0.3949],\n",
            "        [0.3870],\n",
            "        [0.4369],\n",
            "        [0.3934],\n",
            "        [0.4228],\n",
            "        [0.3274],\n",
            "        [0.3965],\n",
            "        [0.4667],\n",
            "        [0.3996],\n",
            "        [0.4778],\n",
            "        [0.4218],\n",
            "        [0.4255],\n",
            "        [0.4309],\n",
            "        [0.4235],\n",
            "        [0.3837],\n",
            "        [0.3952],\n",
            "        [0.4372],\n",
            "        [0.4962],\n",
            "        [0.4194],\n",
            "        [0.4097],\n",
            "        [0.4312],\n",
            "        [0.3891],\n",
            "        [0.4092],\n",
            "        [0.3612],\n",
            "        [0.4195],\n",
            "        [0.4451],\n",
            "        [0.4301],\n",
            "        [0.4680],\n",
            "        [0.4108],\n",
            "        [0.4253],\n",
            "        [0.4061],\n",
            "        [0.4172],\n",
            "        [0.4412],\n",
            "        [0.4280],\n",
            "        [0.3773],\n",
            "        [0.3675],\n",
            "        [0.5057],\n",
            "        [0.4452],\n",
            "        [0.4125],\n",
            "        [0.4341],\n",
            "        [0.4349],\n",
            "        [0.4091],\n",
            "        [0.4133],\n",
            "        [0.4483],\n",
            "        [0.4004],\n",
            "        [0.4443],\n",
            "        [0.3978],\n",
            "        [0.3816],\n",
            "        [0.4565],\n",
            "        [0.4320],\n",
            "        [0.4807],\n",
            "        [0.4171],\n",
            "        [0.4349],\n",
            "        [0.4449],\n",
            "        [0.4501],\n",
            "        [0.4415],\n",
            "        [0.4139],\n",
            "        [0.4153],\n",
            "        [0.4389],\n",
            "        [0.4238],\n",
            "        [0.4414],\n",
            "        [0.3411],\n",
            "        [0.4143],\n",
            "        [0.3849],\n",
            "        [0.4308],\n",
            "        [0.4629],\n",
            "        [0.3990],\n",
            "        [0.4179],\n",
            "        [0.4275],\n",
            "        [0.3780],\n",
            "        [0.4243],\n",
            "        [0.4226],\n",
            "        [0.4444],\n",
            "        [0.4298],\n",
            "        [0.4109],\n",
            "        [0.4224],\n",
            "        [0.4035],\n",
            "        [0.4235],\n",
            "        [0.4418],\n",
            "        [0.4957],\n",
            "        [0.3971],\n",
            "        [0.4247],\n",
            "        [0.4212],\n",
            "        [0.4408],\n",
            "        [0.4509],\n",
            "        [0.4156],\n",
            "        [0.4304],\n",
            "        [0.3929],\n",
            "        [0.4285],\n",
            "        [0.3504],\n",
            "        [0.3560],\n",
            "        [0.4291],\n",
            "        [0.4062],\n",
            "        [0.4150],\n",
            "        [0.4331],\n",
            "        [0.4418],\n",
            "        [0.4332],\n",
            "        [0.4402],\n",
            "        [0.3830],\n",
            "        [0.4337],\n",
            "        [0.4470],\n",
            "        [0.3899],\n",
            "        [0.4093],\n",
            "        [0.4333],\n",
            "        [0.3898],\n",
            "        [0.4219],\n",
            "        [0.4306],\n",
            "        [0.4217],\n",
            "        [0.4138],\n",
            "        [0.3841],\n",
            "        [0.4170],\n",
            "        [0.3935],\n",
            "        [0.4248],\n",
            "        [0.4217],\n",
            "        [0.4034],\n",
            "        [0.3695],\n",
            "        [0.4530],\n",
            "        [0.4421],\n",
            "        [0.4339],\n",
            "        [0.4128],\n",
            "        [0.4104],\n",
            "        [0.4320],\n",
            "        [0.4238],\n",
            "        [0.4232],\n",
            "        [0.4184],\n",
            "        [0.4702],\n",
            "        [0.4274],\n",
            "        [0.3998],\n",
            "        [0.4218],\n",
            "        [0.4399],\n",
            "        [0.4474],\n",
            "        [0.4641],\n",
            "        [0.4266],\n",
            "        [0.4302],\n",
            "        [0.4048],\n",
            "        [0.4264],\n",
            "        [0.3846],\n",
            "        [0.4391],\n",
            "        [0.4492],\n",
            "        [0.3989],\n",
            "        [0.4263],\n",
            "        [0.4672],\n",
            "        [0.4424],\n",
            "        [0.3836],\n",
            "        [0.4413],\n",
            "        [0.3676],\n",
            "        [0.4373],\n",
            "        [0.4142],\n",
            "        [0.4506],\n",
            "        [0.4225],\n",
            "        [0.4260],\n",
            "        [0.4188],\n",
            "        [0.4272],\n",
            "        [0.3995],\n",
            "        [0.4171],\n",
            "        [0.4225],\n",
            "        [0.4139],\n",
            "        [0.3416],\n",
            "        [0.4659],\n",
            "        [0.4183],\n",
            "        [0.4412],\n",
            "        [0.4300],\n",
            "        [0.4001],\n",
            "        [0.4298],\n",
            "        [0.3292],\n",
            "        [0.4347],\n",
            "        [0.4239],\n",
            "        [0.4192],\n",
            "        [0.3775],\n",
            "        [0.3866],\n",
            "        [0.4133],\n",
            "        [0.4375],\n",
            "        [0.4099],\n",
            "        [0.4131],\n",
            "        [0.4603],\n",
            "        [0.4068],\n",
            "        [0.3679],\n",
            "        [0.4463],\n",
            "        [0.4364],\n",
            "        [0.3453],\n",
            "        [0.4227],\n",
            "        [0.3705],\n",
            "        [0.4908],\n",
            "        [0.4365],\n",
            "        [0.4248],\n",
            "        [0.4286],\n",
            "        [0.4313],\n",
            "        [0.4133],\n",
            "        [0.4394],\n",
            "        [0.3933],\n",
            "        [0.3924],\n",
            "        [0.4040],\n",
            "        [0.4354],\n",
            "        [0.4231],\n",
            "        [0.4224],\n",
            "        [0.4333],\n",
            "        [0.3995],\n",
            "        [0.4116],\n",
            "        [0.4197]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:22 loss:0.6796574340803945\n",
            "tensor([[0.4303],\n",
            "        [0.4167],\n",
            "        [0.4084],\n",
            "        [0.4252],\n",
            "        [0.4043],\n",
            "        [0.3987],\n",
            "        [0.4039],\n",
            "        [0.5159],\n",
            "        [0.4698],\n",
            "        [0.3865],\n",
            "        [0.4167],\n",
            "        [0.4038],\n",
            "        [0.4365],\n",
            "        [0.4500],\n",
            "        [0.4322],\n",
            "        [0.4407],\n",
            "        [0.3995],\n",
            "        [0.4158],\n",
            "        [0.3687],\n",
            "        [0.3901],\n",
            "        [0.5226],\n",
            "        [0.4280],\n",
            "        [0.3972],\n",
            "        [0.5289],\n",
            "        [0.4039],\n",
            "        [0.3860],\n",
            "        [0.4107],\n",
            "        [0.4905],\n",
            "        [0.4375],\n",
            "        [0.4609],\n",
            "        [0.3863],\n",
            "        [0.4107],\n",
            "        [0.4234],\n",
            "        [0.3948],\n",
            "        [0.4349],\n",
            "        [0.4014],\n",
            "        [0.4315],\n",
            "        [0.4019],\n",
            "        [0.4289],\n",
            "        [0.4713],\n",
            "        [0.4127],\n",
            "        [0.4374],\n",
            "        [0.4101],\n",
            "        [0.3844],\n",
            "        [0.4338],\n",
            "        [0.4396],\n",
            "        [0.4648],\n",
            "        [0.4051],\n",
            "        [0.4006],\n",
            "        [0.4011],\n",
            "        [0.4881],\n",
            "        [0.4383],\n",
            "        [0.4040],\n",
            "        [0.4265],\n",
            "        [0.3923],\n",
            "        [0.4103],\n",
            "        [0.4232],\n",
            "        [0.4020],\n",
            "        [0.4178],\n",
            "        [0.4083],\n",
            "        [0.3959],\n",
            "        [0.3836],\n",
            "        [0.4164],\n",
            "        [0.4936],\n",
            "        [0.4940],\n",
            "        [0.3871],\n",
            "        [0.4339],\n",
            "        [0.4483],\n",
            "        [0.4286],\n",
            "        [0.6123],\n",
            "        [0.4447],\n",
            "        [0.4299],\n",
            "        [0.4590],\n",
            "        [0.4655],\n",
            "        [0.5040],\n",
            "        [0.4354],\n",
            "        [0.3907],\n",
            "        [0.3794],\n",
            "        [0.4053],\n",
            "        [0.4117],\n",
            "        [0.4102],\n",
            "        [0.3902],\n",
            "        [0.4162],\n",
            "        [0.4448],\n",
            "        [0.4141],\n",
            "        [0.4994],\n",
            "        [0.3976],\n",
            "        [0.3966],\n",
            "        [0.3998],\n",
            "        [0.3864],\n",
            "        [0.4019],\n",
            "        [0.4183],\n",
            "        [0.4119],\n",
            "        [0.4456],\n",
            "        [0.3894],\n",
            "        [0.3941],\n",
            "        [0.4485],\n",
            "        [0.4400],\n",
            "        [0.3934],\n",
            "        [0.5325],\n",
            "        [0.3933],\n",
            "        [0.4708],\n",
            "        [0.4130],\n",
            "        [0.3984],\n",
            "        [0.4593],\n",
            "        [0.4080],\n",
            "        [0.3977],\n",
            "        [0.3808],\n",
            "        [0.3963],\n",
            "        [0.4568],\n",
            "        [0.3707],\n",
            "        [0.4118],\n",
            "        [0.4134],\n",
            "        [0.4413],\n",
            "        [0.3813],\n",
            "        [0.4123],\n",
            "        [0.4255],\n",
            "        [0.4483],\n",
            "        [0.4865],\n",
            "        [0.4196],\n",
            "        [0.4324],\n",
            "        [0.4199],\n",
            "        [0.3939],\n",
            "        [0.4656],\n",
            "        [0.5169],\n",
            "        [0.4376],\n",
            "        [0.3921],\n",
            "        [0.4873],\n",
            "        [0.4061],\n",
            "        [0.4386],\n",
            "        [0.4099],\n",
            "        [0.3846],\n",
            "        [0.5793],\n",
            "        [0.4359],\n",
            "        [0.3942],\n",
            "        [0.4188],\n",
            "        [0.4025],\n",
            "        [0.4340],\n",
            "        [0.4190],\n",
            "        [0.4080],\n",
            "        [0.4218],\n",
            "        [0.4687],\n",
            "        [0.3939],\n",
            "        [0.4116],\n",
            "        [0.4361],\n",
            "        [0.3764],\n",
            "        [0.4224],\n",
            "        [0.4273],\n",
            "        [0.3774],\n",
            "        [0.4466],\n",
            "        [0.3854],\n",
            "        [0.4013],\n",
            "        [0.4367],\n",
            "        [0.4319],\n",
            "        [0.4432],\n",
            "        [0.3740],\n",
            "        [0.3599],\n",
            "        [0.4420],\n",
            "        [0.4195],\n",
            "        [0.4133],\n",
            "        [0.4829],\n",
            "        [0.4086],\n",
            "        [0.4252],\n",
            "        [0.4577],\n",
            "        [0.4356],\n",
            "        [0.4676],\n",
            "        [0.4119],\n",
            "        [0.4138],\n",
            "        [0.4205],\n",
            "        [0.5387],\n",
            "        [0.5422],\n",
            "        [0.4268],\n",
            "        [0.4105],\n",
            "        [0.4148],\n",
            "        [0.4105],\n",
            "        [0.4229],\n",
            "        [0.3986],\n",
            "        [0.4840],\n",
            "        [0.4512],\n",
            "        [0.4886],\n",
            "        [0.4248],\n",
            "        [0.4245],\n",
            "        [0.3852],\n",
            "        [0.3944],\n",
            "        [0.4484],\n",
            "        [0.4311],\n",
            "        [0.4159],\n",
            "        [0.4229],\n",
            "        [0.4501],\n",
            "        [0.4274],\n",
            "        [0.5055],\n",
            "        [0.4033],\n",
            "        [0.4113],\n",
            "        [0.4528],\n",
            "        [0.4952],\n",
            "        [0.4293],\n",
            "        [0.3800],\n",
            "        [0.4214],\n",
            "        [0.4749],\n",
            "        [0.4017],\n",
            "        [0.4969],\n",
            "        [0.4134],\n",
            "        [0.5080],\n",
            "        [0.4690],\n",
            "        [0.4051],\n",
            "        [0.4501],\n",
            "        [0.4620],\n",
            "        [0.3929],\n",
            "        [0.4020],\n",
            "        [0.4054],\n",
            "        [0.4309],\n",
            "        [0.3494],\n",
            "        [0.4381],\n",
            "        [0.4329],\n",
            "        [0.3787],\n",
            "        [0.4234],\n",
            "        [0.4467],\n",
            "        [0.4044],\n",
            "        [0.3989],\n",
            "        [0.4468],\n",
            "        [0.4291],\n",
            "        [0.4367],\n",
            "        [0.3906],\n",
            "        [0.4229],\n",
            "        [0.4287],\n",
            "        [0.4348],\n",
            "        [0.4259],\n",
            "        [0.3953],\n",
            "        [0.4450],\n",
            "        [0.4619],\n",
            "        [0.4096],\n",
            "        [0.5568],\n",
            "        [0.4521],\n",
            "        [0.3785],\n",
            "        [0.4486],\n",
            "        [0.3971],\n",
            "        [0.4054],\n",
            "        [0.4675],\n",
            "        [0.4122],\n",
            "        [0.4038],\n",
            "        [0.4392],\n",
            "        [0.4765],\n",
            "        [0.3806],\n",
            "        [0.4209],\n",
            "        [0.4332],\n",
            "        [0.4008],\n",
            "        [0.4097],\n",
            "        [0.4368],\n",
            "        [0.4102],\n",
            "        [0.4559],\n",
            "        [0.4035],\n",
            "        [0.4150],\n",
            "        [0.4011],\n",
            "        [0.4057],\n",
            "        [0.3962],\n",
            "        [0.4173],\n",
            "        [0.4014],\n",
            "        [0.4392],\n",
            "        [0.3972],\n",
            "        [0.4093],\n",
            "        [0.4207],\n",
            "        [0.4116],\n",
            "        [0.4039],\n",
            "        [0.4151],\n",
            "        [0.4161],\n",
            "        [0.5439],\n",
            "        [0.4221],\n",
            "        [0.4213],\n",
            "        [0.4638],\n",
            "        [0.4023],\n",
            "        [0.3890],\n",
            "        [0.4308],\n",
            "        [0.4024],\n",
            "        [0.5429],\n",
            "        [0.4194],\n",
            "        [0.4334],\n",
            "        [0.3971],\n",
            "        [0.3786],\n",
            "        [0.4555],\n",
            "        [0.4141],\n",
            "        [0.4215],\n",
            "        [0.4393],\n",
            "        [0.4355],\n",
            "        [0.4582],\n",
            "        [0.4173],\n",
            "        [0.4100],\n",
            "        [0.3956],\n",
            "        [0.4120],\n",
            "        [0.4034],\n",
            "        [0.3829],\n",
            "        [0.4167],\n",
            "        [0.4005],\n",
            "        [0.5423],\n",
            "        [0.4777],\n",
            "        [0.4282],\n",
            "        [0.3688],\n",
            "        [0.4279],\n",
            "        [0.5207],\n",
            "        [0.4494],\n",
            "        [0.4207],\n",
            "        [0.4341],\n",
            "        [0.3820],\n",
            "        [0.4019],\n",
            "        [0.4384],\n",
            "        [0.4514],\n",
            "        [0.4330],\n",
            "        [0.4212],\n",
            "        [0.4177],\n",
            "        [0.4442],\n",
            "        [0.4280],\n",
            "        [0.3998],\n",
            "        [0.4042],\n",
            "        [0.4143],\n",
            "        [0.4065],\n",
            "        [0.4682],\n",
            "        [0.4123],\n",
            "        [0.3971],\n",
            "        [0.4612],\n",
            "        [0.3806],\n",
            "        [0.4455],\n",
            "        [0.4483],\n",
            "        [0.4385],\n",
            "        [0.3963],\n",
            "        [0.4253],\n",
            "        [0.4140],\n",
            "        [0.4244],\n",
            "        [0.4144],\n",
            "        [0.3971],\n",
            "        [0.3941],\n",
            "        [0.4551],\n",
            "        [0.4145],\n",
            "        [0.4174],\n",
            "        [0.4303],\n",
            "        [0.4167],\n",
            "        [0.3978],\n",
            "        [0.4724],\n",
            "        [0.3975],\n",
            "        [0.4160],\n",
            "        [0.4549],\n",
            "        [0.4110],\n",
            "        [0.4407],\n",
            "        [0.4165],\n",
            "        [0.3915],\n",
            "        [0.3895],\n",
            "        [0.4136],\n",
            "        [0.4309],\n",
            "        [0.4220],\n",
            "        [0.4841],\n",
            "        [0.4402],\n",
            "        [0.3783],\n",
            "        [0.4520],\n",
            "        [0.4194],\n",
            "        [0.4428],\n",
            "        [0.4343],\n",
            "        [0.4521],\n",
            "        [0.4442],\n",
            "        [0.4404],\n",
            "        [0.3931],\n",
            "        [0.3956],\n",
            "        [0.4095],\n",
            "        [0.3946],\n",
            "        [0.4004],\n",
            "        [0.4374],\n",
            "        [0.4077],\n",
            "        [0.4287],\n",
            "        [0.3985],\n",
            "        [0.4180],\n",
            "        [0.4285],\n",
            "        [0.4610],\n",
            "        [0.4019],\n",
            "        [0.4817],\n",
            "        [0.4175],\n",
            "        [0.4572],\n",
            "        [0.4517],\n",
            "        [0.4290],\n",
            "        [0.3785],\n",
            "        [0.4305],\n",
            "        [0.4002],\n",
            "        [0.4418],\n",
            "        [0.4285],\n",
            "        [0.4334],\n",
            "        [0.4742],\n",
            "        [0.3770],\n",
            "        [0.4117],\n",
            "        [0.5253],\n",
            "        [0.4230],\n",
            "        [0.4265],\n",
            "        [0.4931],\n",
            "        [0.4392],\n",
            "        [0.4377],\n",
            "        [0.4232],\n",
            "        [0.4437],\n",
            "        [0.5378],\n",
            "        [0.4227],\n",
            "        [0.4587],\n",
            "        [0.4246],\n",
            "        [0.4231],\n",
            "        [0.4581],\n",
            "        [0.4327],\n",
            "        [0.4199],\n",
            "        [0.4446],\n",
            "        [0.4129],\n",
            "        [0.4383],\n",
            "        [0.4183],\n",
            "        [0.3993],\n",
            "        [0.3968],\n",
            "        [0.4083],\n",
            "        [0.4314],\n",
            "        [0.3877],\n",
            "        [0.3868],\n",
            "        [0.4268],\n",
            "        [0.3866],\n",
            "        [0.4111],\n",
            "        [0.4692],\n",
            "        [0.4258],\n",
            "        [0.4042],\n",
            "        [0.4322],\n",
            "        [0.4123],\n",
            "        [0.4052],\n",
            "        [0.4672],\n",
            "        [0.4110],\n",
            "        [0.3808],\n",
            "        [0.3969],\n",
            "        [0.4138],\n",
            "        [0.3837],\n",
            "        [0.3991],\n",
            "        [0.4469],\n",
            "        [0.4233],\n",
            "        [0.3918],\n",
            "        [0.3873],\n",
            "        [0.4322],\n",
            "        [0.4459],\n",
            "        [0.4739],\n",
            "        [0.3935],\n",
            "        [0.4841],\n",
            "        [0.4050],\n",
            "        [0.3996],\n",
            "        [0.4561],\n",
            "        [0.4924],\n",
            "        [0.4588],\n",
            "        [0.4031],\n",
            "        [0.4423],\n",
            "        [0.4189],\n",
            "        [0.3824],\n",
            "        [0.4435],\n",
            "        [0.4122],\n",
            "        [0.4274],\n",
            "        [0.4170],\n",
            "        [0.3783],\n",
            "        [0.3895],\n",
            "        [0.3831],\n",
            "        [0.4759],\n",
            "        [0.4172],\n",
            "        [0.3735],\n",
            "        [0.4413]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:23 loss:0.6806650310784387\n",
            "tensor([[0.4286],\n",
            "        [0.3977],\n",
            "        [0.3796],\n",
            "        [0.4658],\n",
            "        [0.3824],\n",
            "        [0.3631],\n",
            "        [0.3808],\n",
            "        [0.5953],\n",
            "        [0.4750],\n",
            "        [0.3460],\n",
            "        [0.3991],\n",
            "        [0.4081],\n",
            "        [0.4286],\n",
            "        [0.4481],\n",
            "        [0.4365],\n",
            "        [0.4550],\n",
            "        [0.3934],\n",
            "        [0.4107],\n",
            "        [0.3430],\n",
            "        [0.3651],\n",
            "        [0.5421],\n",
            "        [0.4170],\n",
            "        [0.3893],\n",
            "        [0.5971],\n",
            "        [0.3859],\n",
            "        [0.3562],\n",
            "        [0.3834],\n",
            "        [0.5628],\n",
            "        [0.4542],\n",
            "        [0.5195],\n",
            "        [0.3637],\n",
            "        [0.4145],\n",
            "        [0.4138],\n",
            "        [0.3691],\n",
            "        [0.4747],\n",
            "        [0.3944],\n",
            "        [0.4130],\n",
            "        [0.4079],\n",
            "        [0.4230],\n",
            "        [0.5104],\n",
            "        [0.4069],\n",
            "        [0.4730],\n",
            "        [0.4263],\n",
            "        [0.4014],\n",
            "        [0.4273],\n",
            "        [0.4534],\n",
            "        [0.4892],\n",
            "        [0.3939],\n",
            "        [0.3640],\n",
            "        [0.3761],\n",
            "        [0.4770],\n",
            "        [0.4392],\n",
            "        [0.3853],\n",
            "        [0.4392],\n",
            "        [0.3673],\n",
            "        [0.3931],\n",
            "        [0.4094],\n",
            "        [0.3999],\n",
            "        [0.4479],\n",
            "        [0.4068],\n",
            "        [0.4061],\n",
            "        [0.4018],\n",
            "        [0.3912],\n",
            "        [0.5318],\n",
            "        [0.5051],\n",
            "        [0.3809],\n",
            "        [0.4560],\n",
            "        [0.4501],\n",
            "        [0.3900],\n",
            "        [0.7173],\n",
            "        [0.4880],\n",
            "        [0.4438],\n",
            "        [0.4572],\n",
            "        [0.4735],\n",
            "        [0.5066],\n",
            "        [0.4090],\n",
            "        [0.3591],\n",
            "        [0.3462],\n",
            "        [0.4084],\n",
            "        [0.4197],\n",
            "        [0.4482],\n",
            "        [0.3577],\n",
            "        [0.3915],\n",
            "        [0.5175],\n",
            "        [0.3950],\n",
            "        [0.5062],\n",
            "        [0.3846],\n",
            "        [0.4241],\n",
            "        [0.4051],\n",
            "        [0.3731],\n",
            "        [0.4454],\n",
            "        [0.4235],\n",
            "        [0.3961],\n",
            "        [0.4414],\n",
            "        [0.3783],\n",
            "        [0.3761],\n",
            "        [0.4976],\n",
            "        [0.4461],\n",
            "        [0.3854],\n",
            "        [0.5441],\n",
            "        [0.4226],\n",
            "        [0.4651],\n",
            "        [0.4055],\n",
            "        [0.4080],\n",
            "        [0.4715],\n",
            "        [0.4303],\n",
            "        [0.3828],\n",
            "        [0.3518],\n",
            "        [0.3918],\n",
            "        [0.4793],\n",
            "        [0.3431],\n",
            "        [0.3996],\n",
            "        [0.4725],\n",
            "        [0.4525],\n",
            "        [0.3566],\n",
            "        [0.3893],\n",
            "        [0.4395],\n",
            "        [0.5100],\n",
            "        [0.5563],\n",
            "        [0.4028],\n",
            "        [0.5042],\n",
            "        [0.4112],\n",
            "        [0.3823],\n",
            "        [0.5178],\n",
            "        [0.6274],\n",
            "        [0.4361],\n",
            "        [0.3807],\n",
            "        [0.5165],\n",
            "        [0.4329],\n",
            "        [0.4291],\n",
            "        [0.3927],\n",
            "        [0.3808],\n",
            "        [0.6801],\n",
            "        [0.4343],\n",
            "        [0.3791],\n",
            "        [0.4080],\n",
            "        [0.3868],\n",
            "        [0.4521],\n",
            "        [0.4051],\n",
            "        [0.4029],\n",
            "        [0.4490],\n",
            "        [0.4909],\n",
            "        [0.3731],\n",
            "        [0.4075],\n",
            "        [0.4699],\n",
            "        [0.3922],\n",
            "        [0.4429],\n",
            "        [0.4412],\n",
            "        [0.3504],\n",
            "        [0.4344],\n",
            "        [0.3669],\n",
            "        [0.3814],\n",
            "        [0.4313],\n",
            "        [0.4251],\n",
            "        [0.4420],\n",
            "        [0.3376],\n",
            "        [0.3312],\n",
            "        [0.4407],\n",
            "        [0.4373],\n",
            "        [0.4021],\n",
            "        [0.5328],\n",
            "        [0.4078],\n",
            "        [0.4222],\n",
            "        [0.4785],\n",
            "        [0.4008],\n",
            "        [0.4949],\n",
            "        [0.4132],\n",
            "        [0.4049],\n",
            "        [0.4068],\n",
            "        [0.6927],\n",
            "        [0.5824],\n",
            "        [0.4422],\n",
            "        [0.4127],\n",
            "        [0.4028],\n",
            "        [0.3950],\n",
            "        [0.4299],\n",
            "        [0.3723],\n",
            "        [0.4954],\n",
            "        [0.4787],\n",
            "        [0.5192],\n",
            "        [0.4069],\n",
            "        [0.4498],\n",
            "        [0.3617],\n",
            "        [0.3924],\n",
            "        [0.4640],\n",
            "        [0.4648],\n",
            "        [0.4015],\n",
            "        [0.4246],\n",
            "        [0.4621],\n",
            "        [0.4192],\n",
            "        [0.5381],\n",
            "        [0.3901],\n",
            "        [0.3802],\n",
            "        [0.4396],\n",
            "        [0.5317],\n",
            "        [0.4076],\n",
            "        [0.4077],\n",
            "        [0.4682],\n",
            "        [0.5080],\n",
            "        [0.3737],\n",
            "        [0.5561],\n",
            "        [0.3954],\n",
            "        [0.5990],\n",
            "        [0.4583],\n",
            "        [0.4475],\n",
            "        [0.4792],\n",
            "        [0.4977],\n",
            "        [0.4202],\n",
            "        [0.3777],\n",
            "        [0.4008],\n",
            "        [0.4323],\n",
            "        [0.3249],\n",
            "        [0.4526],\n",
            "        [0.4629],\n",
            "        [0.3517],\n",
            "        [0.4270],\n",
            "        [0.4965],\n",
            "        [0.3883],\n",
            "        [0.3853],\n",
            "        [0.4531],\n",
            "        [0.4164],\n",
            "        [0.4621],\n",
            "        [0.3706],\n",
            "        [0.4300],\n",
            "        [0.4197],\n",
            "        [0.4521],\n",
            "        [0.4290],\n",
            "        [0.3696],\n",
            "        [0.5243],\n",
            "        [0.5007],\n",
            "        [0.3839],\n",
            "        [0.6872],\n",
            "        [0.4718],\n",
            "        [0.3681],\n",
            "        [0.4809],\n",
            "        [0.3841],\n",
            "        [0.3992],\n",
            "        [0.5488],\n",
            "        [0.4142],\n",
            "        [0.4097],\n",
            "        [0.4381],\n",
            "        [0.4922],\n",
            "        [0.3475],\n",
            "        [0.4317],\n",
            "        [0.4889],\n",
            "        [0.3804],\n",
            "        [0.3938],\n",
            "        [0.4722],\n",
            "        [0.4243],\n",
            "        [0.4545],\n",
            "        [0.3967],\n",
            "        [0.4096],\n",
            "        [0.4204],\n",
            "        [0.3768],\n",
            "        [0.3771],\n",
            "        [0.4364],\n",
            "        [0.4111],\n",
            "        [0.4788],\n",
            "        [0.3735],\n",
            "        [0.4236],\n",
            "        [0.4243],\n",
            "        [0.4760],\n",
            "        [0.4163],\n",
            "        [0.3810],\n",
            "        [0.4274],\n",
            "        [0.5812],\n",
            "        [0.4268],\n",
            "        [0.4156],\n",
            "        [0.4869],\n",
            "        [0.3876],\n",
            "        [0.3966],\n",
            "        [0.4586],\n",
            "        [0.3825],\n",
            "        [0.5611],\n",
            "        [0.4220],\n",
            "        [0.4533],\n",
            "        [0.3780],\n",
            "        [0.3762],\n",
            "        [0.4943],\n",
            "        [0.4577],\n",
            "        [0.4213],\n",
            "        [0.4359],\n",
            "        [0.4377],\n",
            "        [0.4463],\n",
            "        [0.4201],\n",
            "        [0.3998],\n",
            "        [0.3910],\n",
            "        [0.4117],\n",
            "        [0.3742],\n",
            "        [0.3560],\n",
            "        [0.4475],\n",
            "        [0.4292],\n",
            "        [0.5625],\n",
            "        [0.4986],\n",
            "        [0.4399],\n",
            "        [0.3310],\n",
            "        [0.4204],\n",
            "        [0.5900],\n",
            "        [0.4738],\n",
            "        [0.4016],\n",
            "        [0.4609],\n",
            "        [0.3441],\n",
            "        [0.4099],\n",
            "        [0.4808],\n",
            "        [0.4468],\n",
            "        [0.4326],\n",
            "        [0.3723],\n",
            "        [0.4192],\n",
            "        [0.4492],\n",
            "        [0.4128],\n",
            "        [0.3675],\n",
            "        [0.3798],\n",
            "        [0.4165],\n",
            "        [0.4047],\n",
            "        [0.4860],\n",
            "        [0.4057],\n",
            "        [0.3714],\n",
            "        [0.5498],\n",
            "        [0.3628],\n",
            "        [0.4873],\n",
            "        [0.4619],\n",
            "        [0.4188],\n",
            "        [0.3969],\n",
            "        [0.4349],\n",
            "        [0.4081],\n",
            "        [0.4576],\n",
            "        [0.4088],\n",
            "        [0.3838],\n",
            "        [0.3600],\n",
            "        [0.4708],\n",
            "        [0.4219],\n",
            "        [0.4127],\n",
            "        [0.4511],\n",
            "        [0.4168],\n",
            "        [0.3712],\n",
            "        [0.4459],\n",
            "        [0.4021],\n",
            "        [0.4117],\n",
            "        [0.4785],\n",
            "        [0.3912],\n",
            "        [0.4306],\n",
            "        [0.4150],\n",
            "        [0.3637],\n",
            "        [0.3926],\n",
            "        [0.4073],\n",
            "        [0.4933],\n",
            "        [0.4739],\n",
            "        [0.5181],\n",
            "        [0.4652],\n",
            "        [0.3566],\n",
            "        [0.4618],\n",
            "        [0.4047],\n",
            "        [0.4513],\n",
            "        [0.4294],\n",
            "        [0.5030],\n",
            "        [0.4501],\n",
            "        [0.4322],\n",
            "        [0.3967],\n",
            "        [0.3928],\n",
            "        [0.3949],\n",
            "        [0.3983],\n",
            "        [0.3884],\n",
            "        [0.4404],\n",
            "        [0.3960],\n",
            "        [0.4389],\n",
            "        [0.4094],\n",
            "        [0.4228],\n",
            "        [0.4552],\n",
            "        [0.4825],\n",
            "        [0.3868],\n",
            "        [0.5358],\n",
            "        [0.4546],\n",
            "        [0.4571],\n",
            "        [0.4562],\n",
            "        [0.4215],\n",
            "        [0.3595],\n",
            "        [0.4470],\n",
            "        [0.3789],\n",
            "        [0.4536],\n",
            "        [0.4320],\n",
            "        [0.4456],\n",
            "        [0.4693],\n",
            "        [0.3474],\n",
            "        [0.4233],\n",
            "        [0.5907],\n",
            "        [0.4125],\n",
            "        [0.4133],\n",
            "        [0.5063],\n",
            "        [0.4474],\n",
            "        [0.4415],\n",
            "        [0.4375],\n",
            "        [0.4570],\n",
            "        [0.6408],\n",
            "        [0.4103],\n",
            "        [0.4584],\n",
            "        [0.4454],\n",
            "        [0.4266],\n",
            "        [0.4485],\n",
            "        [0.4252],\n",
            "        [0.4469],\n",
            "        [0.4461],\n",
            "        [0.4440],\n",
            "        [0.4382],\n",
            "        [0.4226],\n",
            "        [0.3685],\n",
            "        [0.3850],\n",
            "        [0.4012],\n",
            "        [0.4409],\n",
            "        [0.3683],\n",
            "        [0.3778],\n",
            "        [0.4345],\n",
            "        [0.3681],\n",
            "        [0.4110],\n",
            "        [0.5597],\n",
            "        [0.3958],\n",
            "        [0.3968],\n",
            "        [0.4242],\n",
            "        [0.4014],\n",
            "        [0.4143],\n",
            "        [0.4909],\n",
            "        [0.4669],\n",
            "        [0.3474],\n",
            "        [0.3823],\n",
            "        [0.4130],\n",
            "        [0.3896],\n",
            "        [0.4125],\n",
            "        [0.4689],\n",
            "        [0.4154],\n",
            "        [0.3827],\n",
            "        [0.3767],\n",
            "        [0.4102],\n",
            "        [0.4705],\n",
            "        [0.5496],\n",
            "        [0.3590],\n",
            "        [0.5163],\n",
            "        [0.4524],\n",
            "        [0.3887],\n",
            "        [0.5207],\n",
            "        [0.4859],\n",
            "        [0.4710],\n",
            "        [0.3939],\n",
            "        [0.4520],\n",
            "        [0.4119],\n",
            "        [0.3648],\n",
            "        [0.4470],\n",
            "        [0.4308],\n",
            "        [0.4517],\n",
            "        [0.4250],\n",
            "        [0.3440],\n",
            "        [0.3681],\n",
            "        [0.3603],\n",
            "        [0.5007],\n",
            "        [0.4306],\n",
            "        [0.3516],\n",
            "        [0.4603]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:24 loss:0.6854673083722005\n",
            "tensor([[0.4398],\n",
            "        [0.3952],\n",
            "        [0.3810],\n",
            "        [0.4578],\n",
            "        [0.3903],\n",
            "        [0.3544],\n",
            "        [0.3874],\n",
            "        [0.6138],\n",
            "        [0.4752],\n",
            "        [0.3474],\n",
            "        [0.3971],\n",
            "        [0.4089],\n",
            "        [0.4309],\n",
            "        [0.4538],\n",
            "        [0.4491],\n",
            "        [0.4482],\n",
            "        [0.3929],\n",
            "        [0.4107],\n",
            "        [0.3512],\n",
            "        [0.3710],\n",
            "        [0.5422],\n",
            "        [0.4238],\n",
            "        [0.4028],\n",
            "        [0.5886],\n",
            "        [0.3913],\n",
            "        [0.3653],\n",
            "        [0.3825],\n",
            "        [0.5758],\n",
            "        [0.4431],\n",
            "        [0.5364],\n",
            "        [0.3725],\n",
            "        [0.4120],\n",
            "        [0.4212],\n",
            "        [0.3726],\n",
            "        [0.4900],\n",
            "        [0.4041],\n",
            "        [0.4185],\n",
            "        [0.4124],\n",
            "        [0.4175],\n",
            "        [0.5182],\n",
            "        [0.4139],\n",
            "        [0.4855],\n",
            "        [0.4271],\n",
            "        [0.4114],\n",
            "        [0.4276],\n",
            "        [0.4593],\n",
            "        [0.4927],\n",
            "        [0.3909],\n",
            "        [0.3592],\n",
            "        [0.3728],\n",
            "        [0.4772],\n",
            "        [0.4515],\n",
            "        [0.3928],\n",
            "        [0.4453],\n",
            "        [0.3733],\n",
            "        [0.3833],\n",
            "        [0.4145],\n",
            "        [0.4147],\n",
            "        [0.4436],\n",
            "        [0.4224],\n",
            "        [0.4222],\n",
            "        [0.4029],\n",
            "        [0.3835],\n",
            "        [0.5322],\n",
            "        [0.5075],\n",
            "        [0.3758],\n",
            "        [0.4608],\n",
            "        [0.4538],\n",
            "        [0.3729],\n",
            "        [0.7357],\n",
            "        [0.4941],\n",
            "        [0.4610],\n",
            "        [0.4568],\n",
            "        [0.4890],\n",
            "        [0.5057],\n",
            "        [0.3958],\n",
            "        [0.3596],\n",
            "        [0.3492],\n",
            "        [0.4225],\n",
            "        [0.4322],\n",
            "        [0.4451],\n",
            "        [0.3596],\n",
            "        [0.3922],\n",
            "        [0.5325],\n",
            "        [0.3983],\n",
            "        [0.4999],\n",
            "        [0.3920],\n",
            "        [0.4281],\n",
            "        [0.4168],\n",
            "        [0.3751],\n",
            "        [0.4459],\n",
            "        [0.4362],\n",
            "        [0.3903],\n",
            "        [0.4423],\n",
            "        [0.3885],\n",
            "        [0.3768],\n",
            "        [0.5021],\n",
            "        [0.4440],\n",
            "        [0.3920],\n",
            "        [0.5407],\n",
            "        [0.4242],\n",
            "        [0.4632],\n",
            "        [0.4052],\n",
            "        [0.3984],\n",
            "        [0.4783],\n",
            "        [0.4329],\n",
            "        [0.3809],\n",
            "        [0.3591],\n",
            "        [0.3903],\n",
            "        [0.4895],\n",
            "        [0.3495],\n",
            "        [0.4154],\n",
            "        [0.4772],\n",
            "        [0.4446],\n",
            "        [0.3583],\n",
            "        [0.3873],\n",
            "        [0.4412],\n",
            "        [0.5221],\n",
            "        [0.5462],\n",
            "        [0.4003],\n",
            "        [0.5226],\n",
            "        [0.4186],\n",
            "        [0.3828],\n",
            "        [0.5234],\n",
            "        [0.6237],\n",
            "        [0.4443],\n",
            "        [0.3901],\n",
            "        [0.5148],\n",
            "        [0.4405],\n",
            "        [0.4325],\n",
            "        [0.3984],\n",
            "        [0.3677],\n",
            "        [0.6864],\n",
            "        [0.4371],\n",
            "        [0.3809],\n",
            "        [0.4230],\n",
            "        [0.3942],\n",
            "        [0.4583],\n",
            "        [0.4001],\n",
            "        [0.4112],\n",
            "        [0.4548],\n",
            "        [0.5017],\n",
            "        [0.3819],\n",
            "        [0.4132],\n",
            "        [0.4617],\n",
            "        [0.3927],\n",
            "        [0.4543],\n",
            "        [0.4443],\n",
            "        [0.3624],\n",
            "        [0.4333],\n",
            "        [0.3722],\n",
            "        [0.3870],\n",
            "        [0.4405],\n",
            "        [0.4343],\n",
            "        [0.4420],\n",
            "        [0.3398],\n",
            "        [0.3358],\n",
            "        [0.4439],\n",
            "        [0.4295],\n",
            "        [0.3984],\n",
            "        [0.5376],\n",
            "        [0.4228],\n",
            "        [0.4308],\n",
            "        [0.4809],\n",
            "        [0.3837],\n",
            "        [0.4843],\n",
            "        [0.4150],\n",
            "        [0.4058],\n",
            "        [0.4085],\n",
            "        [0.7136],\n",
            "        [0.5893],\n",
            "        [0.4308],\n",
            "        [0.4130],\n",
            "        [0.3966],\n",
            "        [0.4036],\n",
            "        [0.4336],\n",
            "        [0.3755],\n",
            "        [0.5061],\n",
            "        [0.4853],\n",
            "        [0.5299],\n",
            "        [0.4056],\n",
            "        [0.4464],\n",
            "        [0.3638],\n",
            "        [0.3982],\n",
            "        [0.4578],\n",
            "        [0.4775],\n",
            "        [0.4053],\n",
            "        [0.4353],\n",
            "        [0.4745],\n",
            "        [0.4293],\n",
            "        [0.5310],\n",
            "        [0.4004],\n",
            "        [0.3794],\n",
            "        [0.4336],\n",
            "        [0.5353],\n",
            "        [0.3956],\n",
            "        [0.4195],\n",
            "        [0.4651],\n",
            "        [0.5196],\n",
            "        [0.3699],\n",
            "        [0.5690],\n",
            "        [0.4010],\n",
            "        [0.5792],\n",
            "        [0.4565],\n",
            "        [0.4668],\n",
            "        [0.4893],\n",
            "        [0.5118],\n",
            "        [0.4173],\n",
            "        [0.3854],\n",
            "        [0.4146],\n",
            "        [0.4407],\n",
            "        [0.3103],\n",
            "        [0.4667],\n",
            "        [0.4647],\n",
            "        [0.3622],\n",
            "        [0.4328],\n",
            "        [0.4927],\n",
            "        [0.3984],\n",
            "        [0.3893],\n",
            "        [0.4559],\n",
            "        [0.4270],\n",
            "        [0.4725],\n",
            "        [0.3681],\n",
            "        [0.4356],\n",
            "        [0.4231],\n",
            "        [0.4553],\n",
            "        [0.4381],\n",
            "        [0.3692],\n",
            "        [0.5187],\n",
            "        [0.5011],\n",
            "        [0.3630],\n",
            "        [0.7040],\n",
            "        [0.4798],\n",
            "        [0.3835],\n",
            "        [0.4863],\n",
            "        [0.3929],\n",
            "        [0.4046],\n",
            "        [0.5667],\n",
            "        [0.4171],\n",
            "        [0.4171],\n",
            "        [0.4419],\n",
            "        [0.4920],\n",
            "        [0.3538],\n",
            "        [0.4274],\n",
            "        [0.5016],\n",
            "        [0.3791],\n",
            "        [0.3979],\n",
            "        [0.4749],\n",
            "        [0.4406],\n",
            "        [0.4551],\n",
            "        [0.3974],\n",
            "        [0.4170],\n",
            "        [0.4324],\n",
            "        [0.3735],\n",
            "        [0.3808],\n",
            "        [0.4382],\n",
            "        [0.4276],\n",
            "        [0.4917],\n",
            "        [0.3835],\n",
            "        [0.4327],\n",
            "        [0.4257],\n",
            "        [0.4936],\n",
            "        [0.4343],\n",
            "        [0.3734],\n",
            "        [0.4303],\n",
            "        [0.5836],\n",
            "        [0.4407],\n",
            "        [0.4180],\n",
            "        [0.4987],\n",
            "        [0.3907],\n",
            "        [0.4074],\n",
            "        [0.4547],\n",
            "        [0.3715],\n",
            "        [0.5555],\n",
            "        [0.4314],\n",
            "        [0.4664],\n",
            "        [0.3878],\n",
            "        [0.3863],\n",
            "        [0.5153],\n",
            "        [0.4620],\n",
            "        [0.4236],\n",
            "        [0.4424],\n",
            "        [0.4441],\n",
            "        [0.4454],\n",
            "        [0.4216],\n",
            "        [0.4053],\n",
            "        [0.4034],\n",
            "        [0.4225],\n",
            "        [0.3636],\n",
            "        [0.3596],\n",
            "        [0.4557],\n",
            "        [0.4417],\n",
            "        [0.5673],\n",
            "        [0.5057],\n",
            "        [0.4459],\n",
            "        [0.3396],\n",
            "        [0.4151],\n",
            "        [0.5978],\n",
            "        [0.4753],\n",
            "        [0.4043],\n",
            "        [0.4639],\n",
            "        [0.3501],\n",
            "        [0.4228],\n",
            "        [0.4865],\n",
            "        [0.4499],\n",
            "        [0.4244],\n",
            "        [0.3558],\n",
            "        [0.4184],\n",
            "        [0.4569],\n",
            "        [0.4080],\n",
            "        [0.3701],\n",
            "        [0.3761],\n",
            "        [0.4256],\n",
            "        [0.4075],\n",
            "        [0.4934],\n",
            "        [0.4097],\n",
            "        [0.3803],\n",
            "        [0.5468],\n",
            "        [0.3615],\n",
            "        [0.4798],\n",
            "        [0.4721],\n",
            "        [0.4210],\n",
            "        [0.3926],\n",
            "        [0.4486],\n",
            "        [0.3989],\n",
            "        [0.4573],\n",
            "        [0.4148],\n",
            "        [0.3972],\n",
            "        [0.3563],\n",
            "        [0.4751],\n",
            "        [0.4230],\n",
            "        [0.4148],\n",
            "        [0.4671],\n",
            "        [0.4336],\n",
            "        [0.3763],\n",
            "        [0.4317],\n",
            "        [0.4116],\n",
            "        [0.4217],\n",
            "        [0.4877],\n",
            "        [0.3952],\n",
            "        [0.4321],\n",
            "        [0.4152],\n",
            "        [0.3584],\n",
            "        [0.4002],\n",
            "        [0.3925],\n",
            "        [0.5080],\n",
            "        [0.4793],\n",
            "        [0.5111],\n",
            "        [0.4747],\n",
            "        [0.3602],\n",
            "        [0.4585],\n",
            "        [0.4127],\n",
            "        [0.4637],\n",
            "        [0.4379],\n",
            "        [0.5132],\n",
            "        [0.4565],\n",
            "        [0.4304],\n",
            "        [0.3906],\n",
            "        [0.4018],\n",
            "        [0.4011],\n",
            "        [0.4052],\n",
            "        [0.3963],\n",
            "        [0.4308],\n",
            "        [0.3793],\n",
            "        [0.4339],\n",
            "        [0.4009],\n",
            "        [0.4277],\n",
            "        [0.4707],\n",
            "        [0.4847],\n",
            "        [0.3862],\n",
            "        [0.5391],\n",
            "        [0.4658],\n",
            "        [0.4626],\n",
            "        [0.4627],\n",
            "        [0.4240],\n",
            "        [0.3537],\n",
            "        [0.4427],\n",
            "        [0.3769],\n",
            "        [0.4645],\n",
            "        [0.4200],\n",
            "        [0.4563],\n",
            "        [0.4589],\n",
            "        [0.3496],\n",
            "        [0.4260],\n",
            "        [0.6031],\n",
            "        [0.4184],\n",
            "        [0.4189],\n",
            "        [0.5041],\n",
            "        [0.4554],\n",
            "        [0.4479],\n",
            "        [0.4390],\n",
            "        [0.4680],\n",
            "        [0.6555],\n",
            "        [0.4100],\n",
            "        [0.4529],\n",
            "        [0.4542],\n",
            "        [0.4278],\n",
            "        [0.4505],\n",
            "        [0.4291],\n",
            "        [0.4552],\n",
            "        [0.4512],\n",
            "        [0.4350],\n",
            "        [0.4436],\n",
            "        [0.4132],\n",
            "        [0.3738],\n",
            "        [0.3946],\n",
            "        [0.4111],\n",
            "        [0.4340],\n",
            "        [0.3803],\n",
            "        [0.3654],\n",
            "        [0.4411],\n",
            "        [0.3792],\n",
            "        [0.4213],\n",
            "        [0.5711],\n",
            "        [0.3942],\n",
            "        [0.3998],\n",
            "        [0.4290],\n",
            "        [0.3971],\n",
            "        [0.4280],\n",
            "        [0.4959],\n",
            "        [0.4709],\n",
            "        [0.3529],\n",
            "        [0.3943],\n",
            "        [0.4251],\n",
            "        [0.3779],\n",
            "        [0.4158],\n",
            "        [0.4739],\n",
            "        [0.4222],\n",
            "        [0.3924],\n",
            "        [0.3891],\n",
            "        [0.4045],\n",
            "        [0.4689],\n",
            "        [0.5704],\n",
            "        [0.3558],\n",
            "        [0.5274],\n",
            "        [0.4661],\n",
            "        [0.3972],\n",
            "        [0.5397],\n",
            "        [0.4808],\n",
            "        [0.4636],\n",
            "        [0.4073],\n",
            "        [0.4610],\n",
            "        [0.4179],\n",
            "        [0.3731],\n",
            "        [0.4553],\n",
            "        [0.4421],\n",
            "        [0.4465],\n",
            "        [0.4176],\n",
            "        [0.3476],\n",
            "        [0.3722],\n",
            "        [0.3680],\n",
            "        [0.5054],\n",
            "        [0.4287],\n",
            "        [0.3580],\n",
            "        [0.4594]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "epoch:25 loss:0.686193812624093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_pred=model.forward(X_test_tensors)\n",
        "  y_pred=(y_pred>=0.5).float()\n",
        "  accuracy=(y_pred==y_test_tensors).float().mean()\n",
        "  print(f\"accuracy:{accuracy.item()}\")\n",
        "\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "l6ftgRT91HSk",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9793f8df-5842-404b-876d-9812291459e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:0.6229034066200256\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch"
      ],
      "metadata": {
        "id": "QqUJxV2Klu63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y=make_classification(n_samples=10,n_features=2,n_classes=2,n_informative=2,n_redundant=0,random_state=42)"
      ],
      "metadata": {
        "id": "-3w51be0lu18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU5LgDXD3msY",
        "outputId": "c7dfc930-1f84-46b9-db22-62cd35d42967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.06833894, -0.97007347],\n",
              "       [-1.14021544, -0.83879234],\n",
              "       [-2.8953973 ,  1.97686236],\n",
              "       [-0.72063436, -0.96059253],\n",
              "       [-1.96287438, -0.99225135],\n",
              "       [-0.9382051 , -0.54304815],\n",
              "       [ 1.72725924, -1.18582677],\n",
              "       [ 1.77736657,  1.51157598],\n",
              "       [ 1.89969252,  0.83444483],\n",
              "       [-0.58723065, -1.97171753]])"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62lwpYay3nvg",
        "outputId": "65b1e033-b528-49ef-8b68-3d55d4be3f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptEbr9dR3tOd",
        "outputId": "ca3f66d3-dbc6-49c0-a469-0c1cf669ab4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLt4QyS_3u91",
        "outputId": "45f95380-5ae6-4201-be91-9faf43c10028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=torch.tensor(X,dtype=torch.float64)\n",
        "y=torch.tensor(y,dtype=torch.float64)"
      ],
      "metadata": {
        "id": "0MWB3vUL3wXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader"
      ],
      "metadata": {
        "id": "BXNs-tmT31Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CD(Dataset):\n",
        "\n",
        "  def  __init__(self,features,labels):\n",
        "    self.features=features\n",
        "    self.labels=labels\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "\n",
        "    return self.features.shape[0]\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.features[index],self.labels[index]"
      ],
      "metadata": {
        "id": "Y91wxzyc35R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=CD(X,y)"
      ],
      "metadata": {
        "id": "Aqe5nuHn4aUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQYmbzY74dWZ",
        "outputId": "cda42c12-e982-4773-d800-5428cb6cb7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1.0683, -0.9701], dtype=torch.float64),\n",
              " tensor(1., dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYB5I6qD4fGJ",
        "outputId": "801dfc4a-a875-4bc7-bdf8-3449fc26a53a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader=DataLoader(dataset,batch_size=2,shuffle=True)"
      ],
      "metadata": {
        "id": "spWeyb334gzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_features,batch_labels in dataloader:\n",
        "  print(batch_features)\n",
        "  print(batch_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RqFdrjq4nrS",
        "outputId": "6abfec72-3d4c-47fb-9988-045d56ec0db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.9629, -0.9923],\n",
            "        [-0.9382, -0.5430]], dtype=torch.float64)\n",
            "tensor([0., 1.], dtype=torch.float64)\n",
            "tensor([[ 1.8997,  0.8344],\n",
            "        [-0.7206, -0.9606]], dtype=torch.float64)\n",
            "tensor([1., 0.], dtype=torch.float64)\n",
            "tensor([[ 1.0683, -0.9701],\n",
            "        [-0.5872, -1.9717]], dtype=torch.float64)\n",
            "tensor([1., 0.], dtype=torch.float64)\n",
            "tensor([[ 1.7273, -1.1858],\n",
            "        [-2.8954,  1.9769]], dtype=torch.float64)\n",
            "tensor([1., 0.], dtype=torch.float64)\n",
            "tensor([[ 1.7774,  1.5116],\n",
            "        [-1.1402, -0.8388]], dtype=torch.float64)\n",
            "tensor([1., 0.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LFLIYJxa4x4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "df=pd.read_csv('fmnist_small.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "lO6nqb5oBPGT",
        "outputId": "164cd5d3-968f-46cd-e808-009fb5c3401c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
              "0      9       0       0       0       0       0       0       0       0   \n",
              "1      7       0       0       0       0       0       0       0       0   \n",
              "2      0       0       0       0       0       0       1       0       0   \n",
              "3      8       0       0       0       0       0       0       0       0   \n",
              "4      8       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
              "0       0  ...         0         7         0        50       205       196   \n",
              "1       0  ...         0         0         0         0         0         0   \n",
              "2       0  ...       142       142       142        21         0         3   \n",
              "3       0  ...         0         0         0         0         0         0   \n",
              "4       0  ...       213       203       174       151       188        10   \n",
              "\n",
              "   pixel781  pixel782  pixel783  pixel784  \n",
              "0       213       165         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67594a68-46fa-47ed-bd29-4aa89eff4e74\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>205</td>\n",
              "      <td>196</td>\n",
              "      <td>213</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>142</td>\n",
              "      <td>142</td>\n",
              "      <td>142</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>213</td>\n",
              "      <td>203</td>\n",
              "      <td>174</td>\n",
              "      <td>151</td>\n",
              "      <td>188</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67594a68-46fa-47ed-bd29-4aa89eff4e74')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-67594a68-46fa-47ed-bd29-4aa89eff4e74 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-67594a68-46fa-47ed-bd29-4aa89eff4e74');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-631c31b4-5279-4306-ab42-42f51927fb20\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-631c31b4-5279-4306-ab42-42f51927fb20')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-631c31b4-5279-4306-ab42-42f51927fb20 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.iloc[:,1: ].values\n",
        "y=df.iloc[:,0 ].values"
      ],
      "metadata": {
        "id": "avksJbHDBYwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "Fee8kMdIBhzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "U53IKE34JBV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc26cfe2-949f-472a-c960-37a420f2a119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4800, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train/255.0\n",
        "X_test=X_test/255.0"
      ],
      "metadata": {
        "id": "jDsyAwc-GxGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "LG_UQhjDG3yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eac3990-21cb-48d0-f659-a362fa429dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       ...,\n",
              "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
              "        0.       ],\n",
              "       [0.       , 0.       , 0.       , ..., 0.0627451, 0.       ,\n",
              "        0.       ]])"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    self.features=torch.tensor(features,dtype=torch.float64).reshape(-1,1,28,28) # Change data type to torch.float64\n",
        "    self.labels=torch.tensor(features,dtype=torch.long)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.features[index],self.labels[index]"
      ],
      "metadata": {
        "id": "VMVmmC1FG_Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=CustomDataset(X_train,y_train)\n",
        "test_dataset=CustomDataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "8fwpQe8OHJVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "mAYcpDcTHN4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_features, 32, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),  # BatchNorm2d should match the number of output channels of the previous layer\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),  # BatchNorm2d should match the number of output channels of the previous layer (64 in this case)\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(7 * 7 * 64, 128), # Changed input features to 7 * 7 * 64 to match flattened output of features\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L3LoXHyeIcXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "learning_rate=0.1"
      ],
      "metadata": {
        "id": "BSD8AgcdHw47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode=MyNN(1)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(mode.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "sGEBHrzfJJWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-275-56fa7afd1a4b\n",
        "for epoch in range(epochs): # Changed epochs to epoch for correct iteration\n",
        "  total_loss=0\n",
        "  for batch_features,batch_labels in train_loader:\n",
        "\n",
        "    outputs=model.forward(batch_features) # Changed out to outputs and used model\n",
        "    loss=criterion(outputs,batch_labels) # Changed outputs to match previous line\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss+=loss.item()\n",
        "\n",
        "  print(f\"epoch:{epoch+1} loss:{total_loss/len(train_loader)}\") # Changed epoch to match loop variable"
      ],
      "metadata": {
        "id": "73h1sYgRJVaS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "0401a5bb-373c-4deb-af9e-9c3032520e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (896x28 and 30x1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-290-bf318b099332>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Changed out to outputs and used model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Changed outputs to match previous line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-224-5d3bacc5550a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;31m# Add return statement here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (896x28 and 30x1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vice =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device:{device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHoFmyRmuNu0",
        "outputId": "cc85cb68-8aed-4f8d-b26b-1eb67a278fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "# %%\n",
        "torch.manual_seed(42)\n",
        "df=pd.read_csv('fmnist_small.csv')\n",
        "df.head()\n",
        "# %%\n",
        "X=df.iloc[:,1: ].values\n",
        "y=df.iloc[:,0 ].values\n",
        "# %%\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "# %%\n",
        "X_train.shape\n",
        "# %%\n",
        "# Convert X_train and X_test to float32\n",
        "X_train=X_train/255.0\n",
        "X_train = X_train.astype(np.float32)  # Convert to float32\n",
        "X_test=X_test/255.0\n",
        "X_test = X_test.astype(np.float32)  # Convert to float32\n",
        "# %%\n",
        "X_train\n",
        "# %%\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    self.features=torch.tensor(features,dtype=torch.float32) # Data type should match\n",
        "    self.labels=torch.tensor(labels,dtype=torch.long) # Corrected to use labels\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.features[index],self.labels[index]\n",
        "# %%\n",
        "train_dataset=CustomDataset(X_train,y_train)\n",
        "test_dataset=CustomDataset(X_test,y_test)\n",
        "# %%\n",
        "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False)\n",
        "# %%\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(num_features, 128),  # First hidden layer: 128 neurons\n",
        "            nn.BatchNorm1d(128),  # Batch normalization\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64), # Third hidden layer: 64 neurons\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.3),\n",
        "            nn.Linear(64, 10)  # Output layer: 10 neurons (for 10 classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# %%\n",
        "epochs=100\n",
        "learning_rate=0.1\n",
        "# %%\n",
        "model=MyNN(X_train.shape[1]) # Changed 'mode' to 'model' to match usage\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(model.parameters(),lr=learning_rate,weight_decay=1e-4)\n",
        "# %%\n",
        "for epoch in range(epochs): # Changed 'epochs' to 'epoch' for iteration\n",
        "  for batch_features,batch_labels in train_loader:\n",
        "\n",
        "    outputs=model.forward(batch_features) # Changed 'out' to 'outputs' to match usage\n",
        "    loss=criterion(outputs,batch_labels) # Use 'outputs' instead of undefined 'out'\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "wydfJeUFKku6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jz9UI7IKsC2",
        "outputId": "0d48083a-19a8-4f54-8264-0616235bc0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyNN(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (5): ReLU()\n",
              "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=3136, out_features=128, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Dropout(p=0.4, inplace=False)\n",
              "    (7): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0 #  keeps track of total data points\n",
        "correct = 0 #  keeps track of correct predictions\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_features, batch_labels in test_loader:\n",
        "    outputs = mode(batch_features)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    total += batch_labels.size(0) # Add the batch size instead of the data\n",
        "    correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "5Jq2FL4wKxPA",
        "outputId": "f59dc8d1-f25e-472f-81bb-c1716b4221c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "running_mean should contain 64 elements not 32",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-279-96e28c63bbf5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-272-36879feb5bba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 64 elements not 32"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations for image datasets\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize images\n",
        "])\n",
        "\n",
        "# Download and prepare dataset (e.g., MNIST)\n",
        "dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Data loaders initialized successfully!\")\n"
      ],
      "metadata": {
        "id": "YI0prqNVMROh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,output_dim,num_hidden_layers,neurons_per_layer):\n",
        "\n",
        "    super().__init__()\n",
        "    layers=[]\n",
        "    for i in range(num_hidden_layers):\n",
        "      layers.append(nn.Linear(input_dim,neurons_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "\n",
        "      layers.append(nn.Dropout(p=0.3))\n",
        "      input_dim=neurons_per_layer\n",
        "\n",
        "    layers.append(nn.Linear(neurons_per_layer,output_dim))\n",
        "\n",
        "    self.model=nn.Sequential(*layers)\n",
        "    self.input_dim=input_dim\n",
        "    self.output_dim=output_dim\n",
        "    self.num_hidden_layers=num_hidden_layers\n",
        "    self.neurons_per_layer=neurons_per_layer\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "Uz5ridSH7mSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ----------------------- Define Neural Network -----------------------\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_hidden_layers, neurons_per_layer):\n",
        "        super(MyNN, self).__init__()\n",
        "\n",
        "        layers = [nn.Linear(input_dim, neurons_per_layer), nn.ReLU()]\n",
        "\n",
        "        for _ in range(num_hidden_layers - 1):\n",
        "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        layers.append(nn.Linear(neurons_per_layer, output_dim))  # Output layer\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# ----------------------- Prepare Data -----------------------\n",
        "# Transform images to tensors & normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load dataset (e.g., MNIST)\n",
        "dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "\n",
        "# Split into training & validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Define DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ----------------------- Objective Function for Optuna -----------------------\n",
        "def objective(trial):\n",
        "    # Hyperparameter tuning\n",
        "    num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 1, 5)\n",
        "    neurons_per_layer = trial.suggest_int(\"neurons_per_layer\", 16, 128, step=8)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
        "\n",
        "    # Model definition\n",
        "    input_dim = 784  # Flattened 28x28 input\n",
        "    output_dim = 10  # MNIST has 10 classes\n",
        "    model = MyNN(input_dim, output_dim, num_hidden_layers, neurons_per_layer)\n",
        "\n",
        "    # Loss function & optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 5  # Reduce epochs for fast tuning\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            batch_size = batch_features.shape[0]\n",
        "            batch_features = batch_features.view(batch_size, -1)  # Flatten input\n",
        "\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for val_features, val_labels in val_loader:\n",
        "            batch_size = val_features.shape[0]\n",
        "            val_features = val_features.view(batch_size, -1)  # Flatten validation input\n",
        "\n",
        "            val_outputs = model(val_features)\n",
        "            val_loss += criterion(val_outputs, val_labels).item()\n",
        "\n",
        "    return val_loss / len(val_loader)  # Return avg validation loss\n",
        "\n",
        "# ----------------------- Run Optuna Study -----------------------\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best hyperparameters:\", study.best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "FvklJCoO9bkV",
        "outputId": "c76d26e0-fbaa-4320-9b16-e8c3f56a7063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'optuna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-257-4f53b074b56b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "G229eYR--XQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b2247a-46c3-42d1-e4fa-2a56aecb5c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNqZ7lQO_Fmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}