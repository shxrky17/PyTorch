{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_regression_pred_to_anchors_or_proposals(box_transform_pred, anchors_or_proposals):\n",
    "    # Reshape the box transformation predictions\n",
    "    box_transform_pred = box_transform_pred.reshape(box_transform_pred.size(0), -1, 4)\n",
    "\n",
    "    # Compute width (w) and height (h) of the anchors/proposals\n",
    "    w = anchors_or_proposals[:, 2] - anchors_or_proposals[:, 0]  # x2 - x1\n",
    "    h = anchors_or_proposals[:, 3] - anchors_or_proposals[:, 1]  # y2 - y1\n",
    "\n",
    "    # Compute center coordinates of the anchors/proposals\n",
    "    center_x = anchors_or_proposals[:, 0] + 0.5 * w\n",
    "    center_y = anchors_or_proposals[:, 1] + 0.5 * h\n",
    "\n",
    "    # Extract predicted transformation values (dx, dy, dw, dh)\n",
    "    dx = box_transform_pred[..., 0]\n",
    "    dy = box_transform_pred[..., 1]\n",
    "    dw = box_transform_pred[..., 2]\n",
    "    dh = box_transform_pred[..., 3]\n",
    "\n",
    "    # Apply transformations\n",
    "    pred_center_x=dx*w[:,None]+center_x[:,None]\n",
    "    pred_center_y=dy*h[:,None]+center_y[:,None]\n",
    "    pred_w=torch.exp(dw)*w[:,None]\n",
    "    pred_h=torch.exp(dh)*h[:,None]\n",
    "\n",
    "    pred_box_x1=pred_center_x-0.5*pred_w\n",
    "    pred_box_y1=pred_center_x-0.5*pred_h\n",
    "    pred_box_x2=pred_center_x+0.5*pred_w\n",
    "    pred_box_y2=pred_center_x+0.5*pred_h\n",
    "\n",
    "    pred_boxes=torch.stack((pred_box_x1,pred_box_y1,pred_box_x2,pred_box_y2),dim=2)\n",
    "\n",
    "    return pred_boxes    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxes1,boxes2):\n",
    "    area1=(boxes1[:,2]-boxes1[:,0])*(boxes1[:,3]-boxes1[:,1])\n",
    "    area2=(boxes2[:,2]-boxes2[:,0])*(boxes2[:,3]-boxes2[:,1])\n",
    "\n",
    "    x_left=torch.max(boxes1[:,None,0],boxes2[:,0])\n",
    "    y_top=torch.max(boxes1[:,None,1],boxes2[:,1])\n",
    "    x_right=torch.min(boxes1[:,None,2],boxes2[:,2])\n",
    "    y_bottom=torch.min(boxes1[:,None,3],boxes2[:,3])\n",
    "\n",
    "    intersectioon_area=(x_right-x_left).clamp(min=0)*(y_bottom-y_top).clamp(min=0)\n",
    "    union=area1[:,None]+area2-intersectioon_area\n",
    "    return intersectioon_area/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_boxes_to_image_boundry(boxes,image_shape):\n",
    "    boxes_x1=boxes[...,0]\n",
    "    boxes_y1=boxes[...,1]\n",
    "    boxes_x2=boxes[...,2]\n",
    "    boxes_y2=boxes[...,3]\n",
    "    height,width=image_shape[-2,:]\n",
    "    boxes_x1=boxes_x1.clamp(min=0,max=width)\n",
    "    boxes_y1=boxes_x1.clamp(min=0,max=width)\n",
    "    boxes_x2=boxes_x1.clamp(min=0,max=width)\n",
    "    boxes_y2=boxes_x1.clamp(min=0,max=width)\n",
    "    boxes=torch.cat((\n",
    "        boxes_x1[...,None],\n",
    "        boxes_y1[...,None],\n",
    "        boxes_x2[...,None],\n",
    "        boxes_y2[...,None]\n",
    "    ),dim=-1)\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_to_transformation_targtes(ground_truth_boxes,anchors_or_proposals):\n",
    "\n",
    "    widths=anchors_or_proposals[:,2]-anchors_or_proposals[:,0]\n",
    "    heights=anchors_or_proposals[:,3]-anchors_or_proposals[:,1]\n",
    "    centre_x=anchors_or_proposals[:,0]+0.5*widths\n",
    "    center_y=anchors_or_proposals[:1]+0.5*heights\n",
    "\n",
    "    gt_widths=ground_truth_boxes[:,2]-ground_truth_boxes[:,0]\n",
    "    gt_heights=ground_truth_boxes[:,3]-ground_truth_boxes[:,1]\n",
    "    gt_centre_x=anchors_or_proposals[:,0]+0.5*gt_widths\n",
    "    gt_center_y=anchors_or_proposals[:,1]+0.5*gt_heights\n",
    "\n",
    "    target_dx=(gt_centre_x-centre_x)/widths\n",
    "    target_dy=(gt_center_y-center_y)/heights\n",
    "    target_dw=torch.log(gt_widths/widths)\n",
    "    target_dh=torch.log(gt_heights/heights)\n",
    "\n",
    "    regression_targets=torch.stack((\n",
    "        target_dx,target_dy,target_dw,target_dh\n",
    "    ),dim=1)\n",
    "\n",
    "    return regression_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_positive_negatives(labels,positive_count,total_count):\n",
    "    positve=torch.where(labels>=1)[0]\n",
    "    negative=torch.where(labels==0)[0]\n",
    "    num_pos=positive_count\n",
    "    num_pos=min(positve.numel(),num_pos)\n",
    "    num_neg=total_count-num_pos\n",
    "    num_neg=min(negative.numel(),num_neg)\n",
    "\n",
    "    perm_positive_idxs=torch.randperm(positve.numel(),device=positve.device)[:num_pos]\n",
    "    perm_negative_idxs=torch.randperm(negative.numel(),device=negative.device)[:num_neg]\n",
    "\n",
    "    pos_idxs=positve[perm_positive_idxs]\n",
    "    neg_idxs=negative[perm_negative_idxs]\n",
    "\n",
    "    sampled_pos_idx_masks=torch.zeros_like(labels,dtype=torch.bool)\n",
    "    sampled_neg_idx_masks=torch.zeros_like(labels,dtype=torch.bool)\n",
    "\n",
    "    sampled_pos_idx_masks[pos_idxs]=True\n",
    "    sampled_neg_idx_masks[neg_idxs]=True\n",
    "    return sampled_neg_idx_masks,sampled_pos_idx_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self,in_channels=512):\n",
    "        super(RegionProposalNetwork,self).__init__()\n",
    "        self.scales=[128,256,512]\n",
    "        self.aspect_ratios=[0.5,1,2]\n",
    "        self.num_anchors=len(self.scales)*len(self.aspect_ratios)\n",
    "\n",
    "        self.rpn_conv=nn.Conv2d(in_channels,in_channels,kernel_size=3,stride=1,padding=1)\n",
    "        self.cls_layer=nn.Conv2d(in_channels,self.num_anchors,kernel_size=1,stride=1)\n",
    "        self.bbox_reg_layer=nn.Conv2d(in_channels,self.num_anchors*4,kernel_size=1,stride=1)\n",
    "\n",
    "    def generate_anchors(self,image,feat):\n",
    "        grid_h,grid_w=feat.shape[-2:]\n",
    "        image_h,image_w=image.shape[-2:]\n",
    "\n",
    "        stride_h=torch.tensor(image_h//grid_h,dtype=torch.int64,device=feat.device)\n",
    "        stride_w=torch.tensor(image_w//grid_w,dtype=torch.int64,device=feat.device)\n",
    "        scales=torch.as_tensor(self.scales,dtype=feat.dtype,device=feat.device)\n",
    "        aspect_ratios=torch.as_tensor(self.aspect_ratios,dtype=feat.dtype,device=feat.device)\n",
    "\n",
    "\n",
    "        h_ratios=torch.sqrt(aspect_ratios)\n",
    "        w_ratios=1/h_ratios\n",
    "\n",
    "        ws=(w_ratios[:,None]*scales[None,:]).view(-1)\n",
    "        hs=(h_ratios[:,None]*scales[None,:]).view(-1)\n",
    "\n",
    "        base_anchors=torch.stack([-ws,-hs,ws,hs],dim=1)/2\n",
    "        base_anchors=base_anchors.round()\n",
    "\n",
    "        shifts_x=torch.arange(0,grid_w,dtype=torch.int32,device=feat.device)*stride_w\n",
    "        shifts_y=torch.arange(0,grid_h,dtype=torch.int32,device=feat.device)*stride_h\n",
    "\n",
    "        shifts_x,shifts_y=torch.meshgrid(shifts_y,shifts_x,indexing='ij')\n",
    "\n",
    "        shifts_x=shifts_x.reshape(-1)\n",
    "        shifts_y=shifts_y.reshape(-1)\n",
    "        shifts=torch.stack((shifts_x,shifts_y,shifts_x,shifts_y),dim=1)\n",
    "\n",
    "        anchors=(shifts.view(-1,-1,4)+base_anchors.view(-1,-1,4))\n",
    "        anchors=anchors.reshape(-1,4)\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def filter_proposals(self,proposals,cls_scores,image_shape):\n",
    "        cls_scores=cls_scores.reshape(-1)\n",
    "        cls_scores=torch.sigmoid(cls_scores)\n",
    "        _,top_n_idx=cls_scores.topk(10000)\n",
    "        cls_scores=cls_scores[top_n_idx]\n",
    "        proposals=proposals[top_n_idx]\n",
    "\n",
    "        proposals=clamp_boxes_to_image_boundry(proposals,image_shape)\n",
    "        keep_mask=torch.zeros_like(cls_scores,dtype=torch.bool)\n",
    "        keep_indices=torch.ops.torchvision.nms(proposals,cls_scores,0.7)\n",
    "\n",
    "        post_nms_keep_indices=keep_indices[cls_scores[keep_indices].sort(descending=True)[1]]\n",
    "\n",
    "        proposals=proposals[post_nms_keep_indices[:2000]]\n",
    "        cls_scores=cls_scores[post_nms_keep_indices[:2000]]\n",
    "        return proposals,cls_scores\n",
    "\n",
    "    def assign_target_to_anchors(self,anchors,gt_boxes):\n",
    "        iou_matrix=get_iou(gt_boxes,anchors)\n",
    "\n",
    "        best_match_iou,best_match_gt_index=iou_matrix.max(dim=0)\n",
    "        best_match_gt_idx_pre_threshhold=best_match_gt_index.clone()\n",
    "\n",
    "        below_low_threshold=best_match_iou<0.3\n",
    "        between_threshold=(best_match_iou>=0.3)&(best_match_iou<0.7)\n",
    "        best_match_gt_index[below_low_threshold]=-1\n",
    "        best_match_gt_index[between_threshold]=-2\n",
    "\n",
    "        best_anchor_iou_for_gt,_=iou_matrix.max(dim=1)\n",
    "        gt_pred_pair_with_highest_iou=torch.where(iou_matrix==best_anchor_iou_for_gt[:,None])\n",
    "\n",
    "        pred_inds_to_update=gt_pred_pair_with_highest_iou[1]\n",
    "        best_match_gt_index[pred_inds_to_update]=best_match_gt_idx_pre_threshhold[pred_inds_to_update]\n",
    "\n",
    "        matched_gt_boxes=gt_boxes[best_match_gt_index.clamp(min=0)]\n",
    "\n",
    "        labels=best_match_gt_index>=0\n",
    "        labels=labels.to(dtype=torch.float32)\n",
    "\n",
    "        background_anchors=best_match_gt_index==-1\n",
    "        labels[background_anchors]=0.0\n",
    "\n",
    "        ignored_anchors=best_match_gt_index==-2\n",
    "        labels[ignored_anchors]=-1.0 \n",
    "\n",
    "        return labels,matched_gt_boxes      \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self,image,feat,target):\n",
    "        rpn_feat=nn.ReLU()(self.rpn_conv(feat))\n",
    "        cls_scores=self.cls_layer(rpn_feat)\n",
    "        box_transform_pred=self.bbox_reg_layer(rpn_feat)\n",
    "        anchors=self.generate_anchors(image,feat)\n",
    "        number_of_anchors_per_location=cls_scores.size(1)\n",
    "        cls_scores=cls_scores.permute(0,2,3,1)\n",
    "        cls_scores=cls_scores.reshape(-1,1)\n",
    "\n",
    "\n",
    "        box_transform_pred=box_transform_pred.view(\n",
    "            box_transform_pred.size(0),\n",
    "            number_of_anchors_per_location,\n",
    "            4,\n",
    "            rpn_feat.shape[-2],\n",
    "            rpn_feat.shape[-1],\n",
    "        )\n",
    "        box_transform_pred=box_transform_pred.permute(0,3,4,1,2)\n",
    "        box_transform_pred=box_transform_pred.reshape(-1,4)\n",
    "\n",
    "\n",
    "        proposals=apply_regression_pred_to_anchors_or_proposals(\n",
    "            box_transform_pred.detach().reshape(-1,1,4),\n",
    "            anchors\n",
    "        )\n",
    "        proposals=proposals.reshape(proposals.size(0),4)\n",
    "        proposals,scores=self.filter_proposals(proposals,cls_scores.detach(),image.shape)\n",
    "\n",
    "\n",
    "        rpn_output={\n",
    "            'proposals':proposals,\n",
    "            'scores':scores\n",
    "        }\n",
    "\n",
    "        if not self.training or target is None:\n",
    "            return rpn_output\n",
    "        else:\n",
    "\n",
    "            labels_for_anchors,matched_gt_boxes_for_anchors=self.assign_target_to_anchors(\n",
    "                anchors,\n",
    "                target['bboxes'][0]\n",
    "            )\n",
    "\n",
    "            regression_targets=boxes_to_transformation_targtes(\n",
    "                matched_gt_boxes_for_anchors,\n",
    "                anchors\n",
    "            )\n",
    "\n",
    "            sampled_neg_idx_mask,sampled_pos_idx_mask=sample_positive_negatives(\n",
    "                labels_for_anchors,\n",
    "                positive_count=128,\n",
    "                total_count=256\n",
    "            )\n",
    "\n",
    "            sampled_idxs=torch.where(sampled_neg_idx_mask|sampled_pos_idx_mask)[0]\n",
    "\n",
    "            localization_loss=(\n",
    "                torch.nn.functional.smooth_l1_loss(\n",
    "                    box_transform_pred[sampled_pos_idx_mask],\n",
    "                    regression_targets[sampled_pos_idx_mask],\n",
    "                    beta=1/9,\n",
    "                    reduction='sum'\n",
    "                )/(sampled_idxs.numel())\n",
    "            )\n",
    "\n",
    "            cls_loss=torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                cls_scores[sampled_idxs].flatten(),\n",
    "                labels_for_anchors[sampled_idxs].flatten()\n",
    "            )\n",
    "\n",
    "            rpn_output['rpn_classification_loss']=cls_loss\n",
    "            rpn_output['rpn_localization_loss']=localization_loss\n",
    "            return rpn_output\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=512):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.scales = [128, 256, 512]\n",
    "        self.aspect_ratios = [0.5, 1, 2]\n",
    "        self.num_anchors = len(self.scales) * len(self.aspect_ratios)\n",
    "\n",
    "        self.rpn_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.cls_layer = nn.Conv2d(in_channels, self.num_anchors, kernel_size=1, stride=1)\n",
    "        self.bbox_reg_layer = nn.Conv2d(in_channels, self.num_anchors * 4, kernel_size=1, stride=1)\n",
    "\n",
    "    def generate_anchors(self, image, feat):\n",
    "        grid_h, grid_w = feat.shape[-2:]\n",
    "        image_h, image_w = image.shape[-2:]\n",
    "\n",
    "        stride_h = torch.tensor(image_h // grid_h, dtype=torch.int64, device=feat.device)\n",
    "        stride_w = torch.tensor(image_w // grid_w, dtype=torch.int64, device=feat.device)\n",
    "        scales = torch.as_tensor(self.scales, dtype=feat.dtype, device=feat.device)\n",
    "        aspect_ratios = torch.as_tensor(self.aspect_ratios, dtype=feat.dtype, device=feat.device)\n",
    "\n",
    "        h_ratios = torch.sqrt(aspect_ratios)\n",
    "        w_ratios = 1 / h_ratios\n",
    "\n",
    "        ws = (w_ratios[:, None] * scales[None, :]).view(-1)\n",
    "        hs = (h_ratios[:, None] * scales[None, :]).view(-1)\n",
    "\n",
    "        base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "        base_anchors = base_anchors.round()\n",
    "\n",
    "        shifts_x = torch.arange(0, grid_w, dtype=torch.int32, device=feat.device) * stride_w\n",
    "        shifts_y = torch.arange(0, grid_h, dtype=torch.int32, device=feat.device) * stride_h\n",
    "        shifts_x, shifts_y = torch.meshgrid(shifts_y, shifts_x, indexing='ij')\n",
    "\n",
    "        shifts_x = shifts_x.reshape(-1)\n",
    "        shifts_y = shifts_y.reshape(-1)\n",
    "        shifts = torch.stack((shifts_x, shifts_y, shifts_x, shifts_y), dim=1)\n",
    "\n",
    "        anchors = (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4)\n",
    "        return anchors\n",
    "\n",
    "    def filter_proposals(self, proposals, cls_scores, image_shape):\n",
    "        cls_scores = cls_scores.reshape(-1).sigmoid()\n",
    "        _, top_n_idx = cls_scores.topk(10000)\n",
    "        cls_scores = cls_scores[top_n_idx]\n",
    "        proposals = proposals[top_n_idx]\n",
    "\n",
    "        proposals[:, 0::2] = proposals[:, 0::2].clamp(min=0, max=image_shape[1])\n",
    "        proposals[:, 1::2] = proposals[:, 1::2].clamp(min=0, max=image_shape[0])\n",
    "        \n",
    "        keep_indices = torch.ops.torchvision.nms(proposals, cls_scores, 0.7)\n",
    "        proposals = proposals[keep_indices[:2000]]\n",
    "        cls_scores = cls_scores[keep_indices[:2000]]\n",
    "        return proposals, cls_scores\n",
    "\n",
    "    def forward(self, image, feat, target=None):\n",
    "        rpn_feat = F.relu(self.rpn_conv(feat))\n",
    "        cls_scores = self.cls_layer(rpn_feat)\n",
    "        box_transform_pred = self.bbox_reg_layer(rpn_feat)\n",
    "        anchors = self.generate_anchors(image, feat)\n",
    "\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1).reshape(-1)\n",
    "        box_transform_pred = box_transform_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n",
    "\n",
    "        proposals = apply_regression_pred_to_anchors_or_proposals(box_transform_pred.detach(), anchors)\n",
    "        proposals, scores = self.filter_proposals(proposals, cls_scores.detach(), image.shape)\n",
    "\n",
    "        rpn_output = {'proposals': proposals, 'scores': scores}\n",
    "        \n",
    "        if not self.training or target is None:\n",
    "            return rpn_output\n",
    "        \n",
    "        labels, matched_gt_boxes = self.assign_target_to_anchors(anchors, target['bboxes'][0])\n",
    "        regression_targets = boxes_to_transformation_targtes(matched_gt_boxes, anchors)\n",
    "        sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negatives(labels, 128, 256)\n",
    "        sampled_idxs = torch.where(sampled_neg_idx_mask | sampled_pos_idx_mask)[0]\n",
    "\n",
    "        localization_loss = F.smooth_l1_loss(\n",
    "            box_transform_pred[sampled_pos_idx_mask],\n",
    "            regression_targets[sampled_pos_idx_mask],\n",
    "            beta=1/9,\n",
    "            reduction='sum'\n",
    "        ) / sampled_idxs.numel()\n",
    "        \n",
    "        cls_loss = F.binary_cross_entropy_with_logits(\n",
    "            cls_scores[sampled_idxs], labels[sampled_idxs]\n",
    "        )\n",
    "        \n",
    "        rpn_output.update({\n",
    "            'rpn_classification_loss': cls_loss,\n",
    "            'rpn_localization_loss': localization_loss\n",
    "        })\n",
    "        \n",
    "        return rpn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "class ROIHead(nn.Module):\n",
    "    def __init__(self, num_classes=21, in_channels=512):\n",
    "        super(ROIHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pool_size = 7\n",
    "        self.fc_inner_dim = 1024\n",
    "\n",
    "        self.fc6 = nn.Linear(in_channels * self.pool_size * self.pool_size, self.fc_inner_dim)\n",
    "        self.fc7 = nn.Linear(self.fc_inner_dim, self.fc_inner_dim)\n",
    "        self.cls_layer = nn.Linear(self.fc_inner_dim, self.num_classes)\n",
    "        self.bbox_reg_layer = nn.Linear(self.fc_inner_dim, self.num_classes * 4)\n",
    "\n",
    "    def assign_target_to_proposals(self, proposals, gt_boxes, gt_labels):\n",
    "        iou_matrix = get_iou(gt_boxes, proposals)\n",
    "        best_match_iou, best_match_gt_idx = iou_matrix.max(dim=0)\n",
    "        below_low_threshold = best_match_iou < 0.5\n",
    "\n",
    "        best_match_gt_idx[below_low_threshold] = -1\n",
    "        matched_gt_boxes_for_proposals = gt_boxes[best_match_gt_idx.clamp(min=0)]\n",
    "\n",
    "        labels = gt_labels[best_match_gt_idx.clamp(min=0)]\n",
    "        labels = labels.to(dtype=torch.int64)\n",
    "\n",
    "        background_proposals = best_match_gt_idx == -1\n",
    "        labels[background_proposals] = torch.tensor(0, dtype=torch.int64, device=labels.device)\n",
    "\n",
    "        return labels, matched_gt_boxes_for_proposals\n",
    "\n",
    "    def filter_predictions(self, pred_boxes, pred_labels, pred_scores):\n",
    "        keep = torch.where(pred_scores > 0.05)[0]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_labels[keep], pred_scores[keep]\n",
    "\n",
    "        min_size = 1\n",
    "        ws, hs = pred_boxes[:, 2] - pred_boxes[:, 0], pred_boxes[:, 3] - pred_boxes[:, 1]\n",
    "        keep = torch.where((ws >= min_size) & (hs >= min_size))[0]\n",
    "\n",
    "        pred_boxes, pred_labels, pred_scores = pred_boxes[keep], pred_labels[keep], pred_scores[keep]\n",
    "\n",
    "        keep_mask = torch.zeros_like(pred_scores, dtype=torch.bool)\n",
    "        for class_id in torch.unique(pred_labels):\n",
    "            curr_indices = torch.where(pred_labels == class_id)[0]\n",
    "            curr_keep_indices = torch.ops.torchvision.nms(\n",
    "                pred_boxes[curr_indices], pred_scores[curr_indices], 0.5\n",
    "            )\n",
    "            keep_mask[curr_indices[curr_keep_indices]] = True\n",
    "\n",
    "        keep_indices = torch.where(keep_mask)[0]\n",
    "        post_nms_keep_indices = keep_indices[pred_scores[keep_indices].sort(descending=True)[1]]\n",
    "\n",
    "        keep = post_nms_keep_indices[:100]\n",
    "        pred_boxes, pred_scores, pred_labels = pred_boxes[keep], pred_scores[keep], pred_labels[keep]\n",
    "\n",
    "        return pred_boxes, pred_scores, pred_labels\n",
    "\n",
    "    def forward(self, feat, proposals, image_shape, target):\n",
    "        if self.training and target is not None:\n",
    "            gt_boxes = target['bboxes'][0]\n",
    "            gt_labels = target['labels'][0]\n",
    "\n",
    "            labels, matched_gt_boxes_for_proposals = self.assign_target_to_proposals(\n",
    "                proposals, gt_boxes, gt_labels\n",
    "            )\n",
    "\n",
    "            sampled_neg_idx_mask, sampled_pos_idx_mask = sample_positive_negatives(\n",
    "                labels, positive_count=32, total_count=128\n",
    "            )\n",
    "\n",
    "            sampled_idxs = torch.where(sampled_neg_idx_mask | sampled_pos_idx_mask)[0]\n",
    "            proposals = proposals[sampled_idxs]\n",
    "            matched_gt_boxes_for_proposals = matched_gt_boxes_for_proposals[sampled_idxs]\n",
    "            regression_targets = boxes_to_transformation_targtes(\n",
    "                matched_gt_boxes_for_proposals, proposals\n",
    "            )\n",
    "\n",
    "            spatial_scale = 0.0625\n",
    "\n",
    "            proposal_roi_feats = torchvision.ops.roi_pool(\n",
    "                feat, [proposals], output_size=self.pool_size, spatial_scale=spatial_scale\n",
    "            )\n",
    "\n",
    "            proposal_roi_feats = proposal_roi_feats.flatten(start_dim=1)\n",
    "            box_fc_6 = torch.nn.functional.relu(self.fc6(proposal_roi_feats))\n",
    "            box_fc_7 = torch.nn.functional.relu(self.fc7(box_fc_6))\n",
    "            cls_scores = self.cls_layer(box_fc_7)\n",
    "            box_transform_pred = self.bbox_reg_layer(box_fc_7)\n",
    "\n",
    "            num_boxes, num_classes = cls_scores.shape\n",
    "            box_transform_pred = box_transform_pred.reshape(num_boxes, num_classes, 4)\n",
    "\n",
    "            frcnn_output = {}\n",
    "            if self.training and target is not None:\n",
    "                classification_loss = torch.nn.functional.cross_entropy(cls_scores, labels)\n",
    "\n",
    "                fg_proposal_idx = torch.where(labels > 0)[0]\n",
    "                fg_class_labels = labels[fg_proposal_idx]\n",
    "                localization_loss = torch.nn.functional.smooth_l1_loss(\n",
    "                    box_transform_pred[fg_proposal_idx, fg_class_labels],\n",
    "                    regression_targets[fg_proposal_idx],\n",
    "                    beta=1/9,\n",
    "                    reduction='sum'\n",
    "                )\n",
    "                localization_loss = localization_loss / labels.numel()\n",
    "                frcnn_output['frcnn_classification_loss'] = classification_loss\n",
    "                frcnn_output['frcnn_localization_loss'] = localization_loss\n",
    "                return frcnn_output\n",
    "\n",
    "            else:\n",
    "                pred_boxes = apply_regression_pred_to_anchors_or_proposals(\n",
    "                    box_transform_pred, proposals\n",
    "                )\n",
    "\n",
    "                pred_scores = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
    "\n",
    "                pred_boxes = clamp_boxes_to_image_boundry(pred_boxes, image_shape)\n",
    "\n",
    "                pred_labels = torch.arange(self.num_classes, device=cls_scores.device)\n",
    "                pred_labels = pred_labels.view(1, -1).expand_as(pred_scores)\n",
    "\n",
    "                pred_boxes = pred_boxes[:, 1:]\n",
    "                pred_scores = pred_scores[:, 1:]\n",
    "                pred_labels = pred_labels[:, 1:]\n",
    "\n",
    "                pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "                pred_scores = pred_scores.reshape(-1)\n",
    "                pred_labels = pred_labels.reshape(-1)\n",
    "\n",
    "                pred_boxes, pred_labels, pred_scores = self.filter_predictions(\n",
    "                    pred_boxes, pred_labels, pred_scores\n",
    "                )\n",
    "\n",
    "                frcnn_output['boxes'] = pred_boxes\n",
    "                frcnn_output['scores'] = pred_scores\n",
    "                frcnn_output['labels'] = pred_labels\n",
    "\n",
    "                return frcnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "def transform_boxes_to_orihgnal_size(boxes, new_size, original_size):\n",
    "    ratios = [\n",
    "        torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
    "        / torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
    "        for s, s_orig in zip(new_size, original_size)\n",
    "    ]\n",
    "    ratio_height, ratio_width = ratios\n",
    "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
    "    xmin = xmin * ratio_width\n",
    "    xmax = xmax * ratio_width\n",
    "    ymin = ymin * ratio_height\n",
    "    ymax = ymax * ratio_height\n",
    "    return torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        self.backbone = vgg16.features[:-1]  # Fixed typo \"featues\" -> \"features\"\n",
    "        self.rpn = RegionProposalNetwork(in_channels=512)\n",
    "        self.roi_head = ROIHead(num_classes=num_classes, in_channels=512)\n",
    "\n",
    "        for layer in self.backbone[:10]:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.image_mean = [0.485, 0.456, 0.406]\n",
    "        self.image_std = [0.229, 0.224, 0.225]\n",
    "        self.min_size = 600\n",
    "        self.max_size = 1000\n",
    "\n",
    "    def normalize_resize_image_and_noxes(self, image, bboxes):\n",
    "        mean = torch.as_tensor(self.image_mean, dtype=image.dtype, device=image.device)\n",
    "        std = torch.as_tensor(self.image_std, dtype=image.dtype, device=image.device)\n",
    "\n",
    "        image = (image - mean[:, None, None]) / std[:, None, None]\n",
    "\n",
    "        h, w = image.shape[-2:]\n",
    "        im_shape = torch.tensor([h, w], dtype=torch.float32, device=image.device)\n",
    "        min_size, max_size = im_shape.min(), im_shape.max()\n",
    "        \n",
    "        scale = torch.min(\n",
    "            torch.tensor(self.min_size, dtype=torch.float32, device=image.device) / min_size,\n",
    "            torch.tensor(self.max_size, dtype=torch.float32, device=image.device) / max_size\n",
    "        )\n",
    "        scale_factor = scale.item()\n",
    "\n",
    "        image = torch.nn.functional.interpolate(\n",
    "            image, size=None, scale_factor=scale_factor, mode='bilinear', recompute_scale_factor=True, align_corners=False\n",
    "        )\n",
    "\n",
    "        if bboxes is not None:\n",
    "            ratios = [\n",
    "                torch.tensor(s, dtype=torch.float32, device=bboxes.device)\n",
    "                / torch.tensor(s_orig, dtype=torch.float32, device=bboxes.device)\n",
    "                for s, s_orig in zip(image.shape[-2:], (h, w))\n",
    "            ]\n",
    "            ratio_height, ratio_width = ratios\n",
    "            xmin, ymin, xmax, ymax = bboxes.unbind(2)  # Fixed incorrect \"boxes.unbind(2)\"\n",
    "            xmin = xmin * ratio_width\n",
    "            xmax = xmax * ratio_width\n",
    "            ymin = ymin * ratio_height\n",
    "            ymax = ymax * ratio_height\n",
    "            bboxes = torch.stack((xmin, ymin, xmax, ymax), dim=2)\n",
    "        \n",
    "        return image, bboxes if bboxes is not None else None\n",
    "\n",
    "    def forward(self, image, target=None):\n",
    "        old_shape = image.shape[-2:]\n",
    "        if self.training:\n",
    "            image, bboxes = self.normalize_resize_image_and_noxes(image, target['bboxes'])\n",
    "            target['bboxes'] = bboxes\n",
    "        else:\n",
    "            image, _ = self.normalize_resize_image_and_noxes(image, None)\n",
    "\n",
    "        feat = self.backbone(image)\n",
    "        rpn_output = self.rpn(image, feat, target)\n",
    "        proposals = rpn_output['proposals']\n",
    "\n",
    "        frcnn_output = self.roi_head(feat, proposals, image.shape[-2:], target)\n",
    "\n",
    "        if not self.training:\n",
    "            frcnn_output['boxes'] = transform_boxes_to_orihgnal_size(\n",
    "                frcnn_output['boxes'], image.shape[-2:], old_shape\n",
    "            )\n",
    "\n",
    "        return rpn_output, frcnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def load_images_and_anns(im_dir, ann_dir, label2idx):\n",
    "    r\"\"\"\n",
    "    Method to get the xml files and for each file\n",
    "    get all the objects and their ground truth detection\n",
    "    information for the dataset\n",
    "    :param im_dir: Path of the images\n",
    "    :param ann_dir: Path of annotation xmlfiles\n",
    "    :param label2idx: Class Name to index mapping for dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    im_infos = []\n",
    "    for ann_file in tqdm(glob.glob(os.path.join(ann_dir, '*.xml'))):\n",
    "        im_info = {}\n",
    "        im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n",
    "        im_info['filename'] = os.path.join(im_dir, '{}.jpg'.format(im_info['img_id']))\n",
    "        ann_info = ET.parse(ann_file)\n",
    "        root = ann_info.getroot()\n",
    "        size = root.find('size')\n",
    "        width = int(size.find('width').text)\n",
    "        height = int(size.find('height').text)\n",
    "        im_info['width'] = width\n",
    "        im_info['height'] = height\n",
    "        detections = []\n",
    "        \n",
    "        for obj in ann_info.findall('object'):\n",
    "            det = {}\n",
    "            label = label2idx[obj.find('name').text]\n",
    "            bbox_info = obj.find('bndbox')\n",
    "            bbox = [\n",
    "                int(float(bbox_info.find('xmin').text))-1,\n",
    "                int(float(bbox_info.find('ymin').text))-1,\n",
    "                int(float(bbox_info.find('xmax').text))-1,\n",
    "                int(float(bbox_info.find('ymax').text))-1\n",
    "            ]\n",
    "            det['label'] = label\n",
    "            det['bbox'] = bbox\n",
    "            detections.append(det)\n",
    "        im_info['detections'] = detections\n",
    "        im_infos.append(im_info)\n",
    "    print('Total {} images found'.format(len(im_infos)))\n",
    "    return im_infos\n",
    "\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, split, im_dir, ann_dir):\n",
    "        self.split = split\n",
    "        self.im_dir = im_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        classes = [\n",
    "            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'\n",
    "        ]\n",
    "        classes = sorted(classes)\n",
    "        classes = ['background'] + classes\n",
    "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
    "        print(self.idx2label)\n",
    "        self.images_info = load_images_and_anns(im_dir, ann_dir, self.label2idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im_info = self.images_info[index]\n",
    "        im = Image.open(im_info['filename'])\n",
    "        to_flip = False\n",
    "        if self.split == 'train' and random.random() < 0.5:\n",
    "            to_flip = True\n",
    "            im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
    "        targets = {}\n",
    "        targets['bboxes'] = torch.as_tensor([detection['bbox'] for detection in im_info['detections']])\n",
    "        targets['labels'] = torch.as_tensor([detection['label'] for detection in im_info['detections']])\n",
    "        if to_flip:\n",
    "            for idx, box in enumerate(targets['bboxes']):\n",
    "                x1, y1, x2, y2 = box\n",
    "                w = x2-x1\n",
    "                im_w = im_tensor.shape[-1]\n",
    "                x1 = im_w - x1 - w\n",
    "                x2 = x1 + w\n",
    "                targets['bboxes'][idx] = torch.as_tensor([x1, y1, x2, y2])\n",
    "        return im_tensor, targets, im_info['filename']\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yash)",
   "language": "python",
   "name": "yash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
